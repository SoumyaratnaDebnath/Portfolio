[
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMay 3, 2023\n\n\nModified Harris Hawk Optimization Algorithm for Multi-level Image Thresholding\n\n\nSoumyaratna Debnath, Abhirup Deb, Sourav De\n\n\n\n\nAug 25, 2022\n\n\nPicrypt - Inscribe Images with Encrypted Texts\n\n\nSoumyaratna Debnath, Sourav De, Siddhartha B.\n\n\n\n\nMar 15, 2021\n\n\nMultilevel Image Segmentation Using Modified Red Deer Algorithm\n\n\nSandip Dey, Sourav De, Abhirup Deb, Soumyaratna Debnath\n\n\n\n\nDec 21, 2020\n\n\nA New Modified Red Deer Algorithm for Multi-level Image Thresholding\n\n\nSourav De, Sandip Dey, Soumyaratna Debnath, Abhirup Deb\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/modifiedRDA-2/index.html",
    "href": "publications/modifiedRDA-2/index.html",
    "title": "Multilevel Image Segmentation Using Modified Red Deer Algorithm",
    "section": "",
    "text": "DOI : 10.1109/Confluence51648.2021.9377112\n\n\n\n\nPublished in - 2021 11th International Conference on Cloud Computing, Data Science & Engineering (Confluence)\n\nIn this paper, an evolution strategy based, Modified Red Deer Algorithm (MRDA) has been proposed to find optimum thresholds of publicly available gray scale images. In the recent year, the activity of red deers has been thoroughly observed in the course of their breading season by a group of researchers. Later, this inspired them to introduce a renowned meta-heuristic, popularly known as Red Deer Algorithm (RDA). The RDA is capable to handle several combinatorial optimization problems of various real-world applications. In this paper, a number of adaptive strategy has been suggested to alter the intrinsic operators and parameters used in RDA to improve its performance. The efficiency of MRDA has been judged with RDA and classical Particle Swarm Optimization (PSO) using two publicly accessible real world benchmark images. The performance of each of the competitive method has been analysed with regards to a variety of measure quantitatively and visually. Finally, as a statistical comparison, Kruskal-Wallis test has been carried out between the proposed method and others. The obtained results prove that MRDA is the best performing method than others in all aspects and provides immensely competitive results"
  },
  {
    "objectID": "publications/modifiedHHO/index.html",
    "href": "publications/modifiedHHO/index.html",
    "title": "Modified Harris Hawk Optimization Algorithm for Multi-level Image Thresholding",
    "section": "",
    "text": "ISBN : 9781032393025\n\n\n\n\n\n\nPublished as a chapter in - Hybrid Computational Intelligent Systems Modeling, Simulation and Optimization\n\nHybrid Computational Intelligent Systems – Modeling, Simulation and Optimization unearths the latest advances in evolving hybrid intelligent modeling and simulation of human-centric data-intensive applications optimized for real-time use, thereby enabling researchers to come up with novel breakthroughs in this ever-growing field.\n\nSalient features include the fundamentals of modeling and simulation with recourse to knowledge-based simulation, interaction paradigms, and human factors, along with the enhancement of the existing state of art in a high-performance computing setup. In addition, this book presents optimization strategies to evolve robust and failsafe intelligent system modeling and simulation.\n\nThe volume also highlights novel applications for different engineering problems including signal and data processing, speech, image, sensor data processing, innovative intelligent systems, and swarm intelligent manufacturing systems."
  },
  {
    "objectID": "projects/013 October Virtual Mouse/index.html",
    "href": "projects/013 October Virtual Mouse/index.html",
    "title": "October Virtual Mouse",
    "section": "",
    "text": "GitHub"
  },
  {
    "objectID": "projects/013 October Virtual Mouse/index.html#introduction",
    "href": "projects/013 October Virtual Mouse/index.html#introduction",
    "title": "October Virtual Mouse",
    "section": "Introduction",
    "text": "Introduction\nOctober is an innovative virtual assistant designed specifically for desktop computers. The primary goal of October is to provide users with the ability to control their mouse pointer using only voice commands. This can be particularly beneficial for individuals with mobility impairments, offering them an easier and more efficient way to navigate and interact with their computer systems."
  },
  {
    "objectID": "projects/013 October Virtual Mouse/index.html#features",
    "href": "projects/013 October Virtual Mouse/index.html#features",
    "title": "October Virtual Mouse",
    "section": "Features",
    "text": "Features\n\nVoice-Controlled Mouse Pointer\nOctober enables users to move their mouse pointer and perform various actions such as clicking, double-clicking, and dragging through voice commands. This feature significantly enhances accessibility and can improve the overall user experience for those who find traditional input devices challenging to use.\n\n\nSpeech Recognition\nOctober leverages the Google Speech Recognition API to accurately interpret and process voice commands. This integration ensures that the virtual assistant can understand a wide range of voice inputs with high accuracy, making it a reliable tool for everyday use.\n\n\nMultiprogramming Capabilities\nThe application is built using advanced multiprogramming concepts in Python, which allows it to handle multiple tasks simultaneously. This ensures that October can respond to voice commands in real-time while managing other background processes efficiently."
  },
  {
    "objectID": "projects/013 October Virtual Mouse/index.html#technical-implementation",
    "href": "projects/013 October Virtual Mouse/index.html#technical-implementation",
    "title": "October Virtual Mouse",
    "section": "Technical Implementation",
    "text": "Technical Implementation\n\nPython\nPython is the primary programming language used for developing October. Its simplicity and extensive library support make it an ideal choice for creating a robust and scalable virtual assistant.\n\n\nGoogle Speech Recognition API\nThe Google Speech Recognition API is integrated into October to provide high-quality voice recognition services. This API converts spoken language into text, which October then interprets to execute the appropriate mouse actions.\n\n\nMultiprogramming\nOctober’s architecture is designed using multiprogramming principles, enabling it to perform multiple operations concurrently. This design ensures that voice commands are processed quickly and that the virtual assistant remains responsive during extended use."
  },
  {
    "objectID": "projects/013 October Virtual Mouse/index.html#how-it-works",
    "href": "projects/013 October Virtual Mouse/index.html#how-it-works",
    "title": "October Virtual Mouse",
    "section": "How It Works",
    "text": "How It Works\n\nInitialization: Upon launching October, the application initializes the speech recognition engine and prepares to receive voice commands.\nListening for Commands: October continuously listens for voice inputs from the user. The audio data is captured and sent to the Google Speech Recognition API for processing.\nProcessing Voice Commands: The speech recognition API converts the audio input into text. October then parses this text to determine the appropriate action to perform with the mouse pointer.\nExecuting Mouse Actions: Based on the parsed commands, October moves the mouse pointer, clicks, double-clicks, or performs other mouse-related actions as instructed by the user."
  },
  {
    "objectID": "projects/013 October Virtual Mouse/index.html#benefits",
    "href": "projects/013 October Virtual Mouse/index.html#benefits",
    "title": "October Virtual Mouse",
    "section": "Benefits",
    "text": "Benefits\n\nAccessibility: October provides an alternative input method for users with physical disabilities, allowing them to interact with their computers more easily.\nEfficiency: Voice commands can be quicker and more intuitive for certain tasks, improving overall productivity.\nUser-Friendly: The application is designed to be easy to use, requiring minimal setup and offering a straightforward interface for voice command input."
  },
  {
    "objectID": "projects/013 October Virtual Mouse/index.html#conclusion",
    "href": "projects/013 October Virtual Mouse/index.html#conclusion",
    "title": "October Virtual Mouse",
    "section": "Conclusion",
    "text": "Conclusion\nOctober is a powerful virtual assistant that enhances desktop computer accessibility through voice-controlled mouse pointer manipulation. By utilizing Python, the Google Speech Recognition API, and multiprogramming concepts, October delivers a responsive and reliable tool for users seeking an alternative to traditional input devices. Whether for accessibility purposes or simply to streamline workflow, October represents a significant advancement in voice-controlled technology for desktop computing."
  },
  {
    "objectID": "projects/011 Histogram Matching from Scratch/index.html",
    "href": "projects/011 Histogram Matching from Scratch/index.html",
    "title": "Histogram Matching from Scratch",
    "section": "",
    "text": "GitHub \n\nImporting the necessary libraries\n\n# Importing the libraries\nimport numpy as np\nimport os \nimport cv2 # used only for loading the image\nimport matplotlib.pyplot as plt # used only for displaying the image\n\n\n\nDefining the required functions\n\n# this function is responsible for calculating the histogram of an image\ndef calculate_histogram(image, num_bins=256):\n    histogram = np.zeros(num_bins, dtype=np.int32) # initialize the histogram\n    for pixel_value in image.ravel(): # ravel() returns a contiguous flattened array\n        histogram[pixel_value] += 1 # increment the count of the pixel value\n    return histogram # return the histogram\n\n# this function is responsible for calculating the normalized histogram of an image\ndef calculate_normalized_histogram(image, num_bins=256):\n    histogram = calculate_histogram(image, num_bins) # calculate the histogram\n    sum_of_histogram = np.sum(histogram) # sum of all the pixel values\n    histogram = histogram / sum_of_histogram # normalize the histogram\n    return histogram # return the normalized histogram\n\n# this function is responsible for calculating the cumulative histogram of an image\ndef calculate_cumulative_histogram(histogram):\n    sum_of_histogram = np.sum(histogram) # sum of all the pixel values\n    histogram = histogram / sum_of_histogram # normalize the histogram\n    cumulative_histogram = np.zeros(histogram.shape, dtype=np.float32) # initialize the cumulative histogram\n    cumulative_histogram[0] = histogram[0] \n    for i in range(1, histogram.shape[0]): # calculate the cumulative histogram\n        cumulative_histogram[i] = cumulative_histogram[i - 1] + histogram[i]\n    return cumulative_histogram # return the cumulative histogram\n\n\n# opeining the images in grayscale and storing them in a list\nimage_folder_path = os.path.join(os.getcwd(), 'Dataset', 'histogram_matching')\nimage_set = []\nfor image_name in os.listdir(image_folder_path): # iterate over all the images in the folder\n    img = cv2.imread(os.path.join(image_folder_path, image_name), cv2.IMREAD_GRAYSCALE) # read the image in grayscale\n    image_set.append(img) # append the image to the list\n\n\n# this function is responsible for displaying the images and their histograms\ndef visualize_histograms(image_set, figsize=(15, 9), image_titles=None):\n    plt.figure(figsize=figsize) # set the figure size\n\n    for i, image in enumerate(image_set): # iterate over all the images\n        histogram = calculate_histogram(image) # calculate the histogram of the image\n        normalized_histogram = calculate_normalized_histogram(image) # calculate the normalized histogram of the image\n        cumulative_histogram = calculate_cumulative_histogram(histogram) # calculate the cumulative histogram of the image\n        \n        plt.subplot(3, len(image_set), i + 1) # display the image\n        plt.imshow(image, cmap='gray')\n        plt.title(image_titles[i], fontsize=10)\n        plt.axis('off')\n\n        plt.subplot(3, len(image_set), i + 1 + len(image_set)) # display the histogram\n        plt.bar(range(256), normalized_histogram, width=1.0)\n        plt.title('Normalized Histogram ('+image_titles[i]+')', fontsize=8)\n        plt.xlabel('Pixel Value', fontsize=8)\n        plt.ylabel('Frequency', fontsize=8)\n\n        plt.subplot(3, len(image_set), i + 1 + 2 * len(image_set)) # display the cumulative histogram\n        plt.plot(cumulative_histogram)\n        plt.title('Cumulative Histogram ('+image_titles[i]+')', fontsize=8)\n        plt.xlabel('Pixel Value', fontsize=8)\n        plt.ylabel('Frequency', fontsize=8)\n\n    plt.tight_layout()\n    plt.show()\n\n# displaying the images and their histograms\nvisualize_histograms(image_set, image_titles=['Image 0', 'Image 1', 'Image 2', 'Image 3'])\n\n\n\n\n\n\nImplementing the algorithm\n\n# this function is responsible for matching the histogram of an image to the histogram of a reference image\ndef match_histograms(image, reference_image):\n    mapping = get_mapping(image, reference_image) # get the mapping\n    matched_image = np.zeros(image.shape, dtype=np.uint8) # initialize the matched image\n    for i in range(image.shape[0]): # match the image\n        for j in range(image.shape[1]):\n            matched_image[i, j] = mapping[image[i, j]]\n    return matched_image # return the matched image\n\n# this function is responsible for matching the histogram of an image to the histogram of a reference image\ndef get_mapping(image, reference_image, gray_levels=256):\n    histogram = calculate_histogram(image) # calculate the histogram of the image\n    cumulative_histogram = calculate_cumulative_histogram(histogram) # calculate the cumulative histogram of the image\n    reference_histogram = calculate_histogram(reference_image) # calculate the histogram of the reference image\n    reference_cumulative_histogram = calculate_cumulative_histogram(reference_histogram) # calculate the cumulative histogram of the reference image\n\n    mapping = np.zeros(gray_levels) # initialize the mapping\n    for pixel_value in range(gray_levels):\n        old_value = cumulative_histogram[pixel_value] # get the cumulative histogram of the image\n        temp = reference_cumulative_histogram - old_value # get the difference between the cumulative histogram of the reference image and the cumulative histogram of the image\n        new_value = np.argmin(np.abs(temp)) # get the index of the minimum value in the difference\n        mapping[pixel_value] = new_value # map the pixel value to the new value\n    return mapping # return the mapping\n\n\n# performing histogram matching and displaying the images and their histograms\ndef histogram_matching_and_visualization(image, reference_image, visualize=True): \n    matched_image = match_histograms(image, reference_image) # match the histogram of the image to the histogram of the reference image\n    image_set = [image, reference_image, matched_image] \n    image_titles = ['Source Image', 'Target Image', 'Matched Image']\n    if visualize:\n        visualize_histograms(image_set, image_titles=image_titles) # display the images and their histograms   \n    return matched_image # return the matched image  \n\n\nmatching = histogram_matching_and_visualization(image_set[0], image_set[1])  \n\n\n\n\n\nmatching = histogram_matching_and_visualization(image_set[3], image_set[2])\n\n\n\n\n\nmatching = histogram_matching_and_visualization(image_set[1], image_set[3])   \n\n\n\n\n\nmatching = histogram_matching_and_visualization(image_set[2], image_set[0])\n\n\n\n\n\n\nAnalysis of the obtained resutls\n\nimport numpy as np\nfrom scipy.stats import entropy\nfrom skimage.metrics import structural_similarity as ssim\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\n\ndef calculate_image_statistics(original_image, target_image, matched_image):\n    # Calculate Mean and Standard Deviation\n    mean_original = np.mean(original_image)\n    std_original = np.std(original_image)\n\n    mean_target = np.mean(target_image)\n    std_target = np.std(target_image)\n\n    mean_matched = np.mean(matched_image)\n    std_matched = np.std(matched_image)\n\n    # Calculate SSIM\n    ssim_score = ssim(original_image, matched_image)\n\n    # Create a dictionary to store the statistics\n    statistics = {\n        \"Original Mean\": mean_original,\n        \"Original Standard Deviation\": std_original,\n        \"Target Mean\": mean_target,\n        \"Target Standard Deviation\": std_target,\n        \"Matched Mean\": mean_matched,\n        \"Matched Standard Deviation\": std_matched,\n        \"SSIM Score (Source vs Matched)\": ssim_score,\n    }\n\n    return statistics\n\n\nmatching = histogram_matching_and_visualization(image_set[0], image_set[1], visualize=True)\nstatistics = calculate_image_statistics(image_set[0], image_set[1], matching)\nfor key, value in statistics.items():\n    print(f\"{key}: {value}\")\n\n\n\n\nOriginal Mean: 80.04150772094727\nOriginal Standard Deviation: 68.48801554947227\nTarget Mean: 126.48823547363281\nTarget Standard Deviation: 69.12917387622817\nMatched Mean: 130.7914924621582\nMatched Standard Deviation: 62.277325624710976\nSSIM Score (Source vs Matched): 0.6362629163593398\n\n\nMean and Standard Deviation\nThe original image has a mean of approximately 80.04 and a standard deviation of approximately 68.49. The target image has a mean of approximately 126.49 and a standard deviation of approximately 69.13. After the histogram matching process, the matched image has a mean of approximately 130.79 and a standard deviation of approximately 62.27. Interpretation: The means have shifted towards each other after histogram matching, but the standard deviations have not changed significantly.\nSSIM Score\nThe Structural Similarity Index (SSIM) score between the original and matched images is approximately 0.636. Interpretation: An SSIM score of 1 indicates a perfect match. A score of 0.636 suggests that the matched image is reasonably similar to the original but not a perfect match.\n\nmatching = histogram_matching_and_visualization(image_set[1], image_set[3], visualize=True)\nstatistics = calculate_image_statistics(image_set[1], image_set[3], matching)\nfor key, value in statistics.items():\n    print(f\"{key}: {value}\")\n\n\n\n\nOriginal Mean: 126.48823547363281\nOriginal Standard Deviation: 69.12917387622817\nTarget Mean: 102.53508758544922\nTarget Standard Deviation: 90.99119020648051\nMatched Mean: 102.81129837036133\nMatched Standard Deviation: 90.91224212665476\nSSIM Score (Source vs Matched): 0.6144812508253021\n\n\nMean and Standard Deviation\nThe original image has a mean of approximately 126.49 and a standard deviation of approximately 69.13. The target image has a mean of approximately 102.54 and a standard deviation of approximately 90.99. After the histogram matching process, the matched image has a mean of approximately 102.81 and a standard deviation of approximately 90.91. Interpretation: Both the means and standard deviations have shifted towards the target image each other after histogram matching.\nSSIM Score\nThe Structural Similarity Index (SSIM) score between the original and matched images is approximately 0.615. Interpretation: The matched image is reasonably similar to the original, but it may not be a perfect match.\n\n\nReferences\n\nHistogram matching Wikipedia\nStructural similarity index measure Wikipedia\nHistogram Matching, Google Earth Engine\nScikit-Image Histogram Matching"
  },
  {
    "objectID": "projects/009 KNN Viusalizer/index.html",
    "href": "projects/009 KNN Viusalizer/index.html",
    "title": "K-Nearest Neighbors Visualized",
    "section": "",
    "text": "Run Application \nThe K-Nearest Neighbors (KNN) algorithm is a robust and intuitive method primarily used for classification tasks. Here’s how it operates:\n\nDistance Calculation: The algorithm computes the distance between a new instance and all existing data points.\nSorting by Distance: It then ranks these points based on proximity.\nNeighbor Selection: The K closest data points are identified as the nearest neighbors.\nMajority Voting: The algorithm tallies the categories of these K neighbors.\nClassification: The new data point is assigned to the category most frequent among its neighbors.\n\nI’ve implemented an interactive visualization of the K-Nearest Neighbors algorithm using Python and Streamlit. In this application, you can dynamically adjust the number of neighbors (K) and the total data points to see how the classification changes with these parameters. This hands-on visualization enhances understanding and allows for real-time experimentation with the KNN algorithm."
  },
  {
    "objectID": "projects/007 VGG Compared/index.html",
    "href": "projects/007 VGG Compared/index.html",
    "title": "A Comparative Study Between Different VGG Models",
    "section": "",
    "text": "GitHub \n\nCode\n\n# Importing the required libraries\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\nfrom keras.callbacks import TensorBoard\nimport time\nimport csv\nimport os\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport shutil\n\n2023-04-19 14:37:07.771981: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2023-04-19 14:37:07.807696: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2023-04-19 14:37:07.808495: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-04-19 14:37:08.701505: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n\n\n\nDriver Code\n\nprint('Are you sure want to override previous report ', end='')\ncommand = input()\n\nif command == 'Yes':\n    print('Overriding previous report')\n    # Open the results file in write mode and adding the head\n    with open('results.csv', mode='w', newline='') as results_file:\n        results_writer = csv.writer(results_file)\n        results_writer.writerow(['Model Name', 'Training Time', 'Train Loss', 'Train Acc', 'Test Acc', 'Num Params'])\n    \n    folders_to_remove = [\"saved_models\", \"log_images\", \"log_stats\"]\n    for folder in folders_to_remove:\n        if os.path.exists(folder):\n            shutil.rmtree(folder)\n            print(f\"Directory {folder} removed.\")\n        else:\n            print(f\"Directory {folder} does not exist.\")\n\nAre you sure want to override previous report  Yes\nOverriding previous report\nDirectory saved_models does not exist.\nDirectory log_images does not exist.\nDirectory log_stats does not exist.\n\n\n\n# Define directories for training and testing data\ntrain_dir = 'dataset/train/'\ntest_dir = 'dataset/test/'\n\n# Define the image size to be used for resizing the images\nimg_size = (128, 128)\n\n# Define the input image size (including the number of color channels)\ninput_img_size = (128, 128, 3)\n\n# Define the batch size for training the model\nbatch_size = 20\n\n# Define the number of epochs for training the model\nnum_epochs = 20\n\n\n# Create an ImageDataGenerator object for data augmentation and normalization of training data\ntrain_datagen = ImageDataGenerator(rescale=1./255)\n\n# Create a generator for loading training data from the directory\ntrain_generator = train_datagen.flow_from_directory(train_dir, \n                                                    target_size=img_size, # Resizes the images to a target size\n                                                    batch_size=batch_size, # Defines the batch size\n                                                    class_mode='binary') # Defines the type of labels to use\n\n# Create an ImageDataGenerator object for normalization of testing data\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\n# Create a generator for loading testing data from the directory\ntest_generator = test_datagen.flow_from_directory(test_dir, \n                                                  target_size=img_size, # Resizes the images to a target size\n                                                  batch_size=batch_size, # Defines the batch size\n                                                  class_mode='binary') # Defines the type of labels to use\n\n# Data generators for prediction\nprediction_datagen = ImageDataGenerator(rescale=1./255)\npreprocess_input = tf.keras.applications.vgg16.preprocess_input\nprediction_datagen_vgg = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)\nprediction_generator = test_datagen.flow_from_directory(test_dir, target_size=img_size, batch_size=1, class_mode='binary', shuffle=False) \nprediction_generator_vgg = prediction_datagen_vgg.flow_from_directory(test_dir, target_size=img_size, batch_size=1, class_mode='binary', shuffle=False) \n\nFound 160 images belonging to 2 classes.\nFound 40 images belonging to 2 classes.\nFound 40 images belonging to 2 classes.\nFound 40 images belonging to 2 classes.\n\n\n\n# Function for plotting the predictions and writing to TensorBoard\ndef plot_predictions(title, model, log_dir, test_generator):\n    # Create a summary writer for TensorBoard\n    writer = tf.summary.create_file_writer(log_dir)\n\n    # Get the predicted classes for the test set\n    y_pred = model.predict(test_generator)\n    y_pred_classes = tf.round(y_pred).numpy().astype(int).flatten()\n\n    # Get the true classes for the test set\n    y_true = test_generator.classes.astype(int)\n\n    # Get the class labels for the dataset\n    class_labels = list(test_generator.class_indices.keys())\n\n    # Get all the images and their corresponding labels from the test set generator\n    images = []\n    labels = []\n    for i in range(len(test_generator)):\n        batch = test_generator[i]\n        images.extend(batch[0])\n        labels.extend(batch[1].astype(int))\n\n    for i in range(len(test_generator)):\n        # Write the image to TensorBoard\n        with tf.summary.create_file_writer(log_dir).as_default():\n            tf.summary.image(\"{}   Image {}   Predicted: {}   True: {}\".format(title, i+1, class_labels[y_pred_classes[i]], class_labels[labels[i]]), np.expand_dims(images[i], 0), step=0)\n\n\n\nVGG 1 Block\n\n# Define a function that creates a VGG block with one convolutional layer\ndef vgg_1_block():\n    # Create a Sequential model object with a name\n    model = Sequential(name = 'vgg_block_1')\n    \n    # Add a convolutional layer with 64 filters, a 3x3 kernel size, 'relu' activation, and 'same' padding\n    model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=input_img_size))\n    \n    # Add a max pooling layer with a 2x2 pool size\n    model.add(MaxPooling2D((2, 2)))\n    \n    # Add a flatten layer to convert the 2D feature maps to a 1D feature vector\n    model.add(Flatten())\n    \n    # Add a fully connected layer with 128 units and 'relu' activation\n    model.add(Dense(128, activation='relu'))\n    \n    # Add an output layer with 1 unit and 'sigmoid' activation (for binary classification)\n    model.add(Dense(1, activation='sigmoid'))\n    \n    # Return the model\n    return model\n\n# Create a VGG block with one convolutional layer\nmodel1 = vgg_1_block()\n\n# Print a summary of the model's architecture\nmodel1.summary()\n\n# Define a log directory for TensorBoard\nlog_dir = 'log_stats/vgg_1_block'\n\n# Define the TensorBoard callback with update_freq='batch'\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq='batch')\n\nModel: \"vgg_block_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 128, 128, 64)      1792      \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 64, 64, 64)       0         \n )                                                               \n                                                                 \n flatten (Flatten)           (None, 262144)            0         \n                                                                 \n dense (Dense)               (None, 128)               33554560  \n                                                                 \n dense_1 (Dense)             (None, 1)                 129       \n                                                                 \n=================================================================\nTotal params: 33,556,481\nTrainable params: 33,556,481\nNon-trainable params: 0\n_________________________________________________________________\n\n\n2023-04-19 14:38:27.877910: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\nSkipping registering GPU devices...\n\n\n\n# Compile the model with 'adam' optimizer, 'binary_crossentropy' loss function, and 'accuracy' metric\nmodel1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Start timing the training\nstart_time = time.time()\n\n# Train the model using the training generator for a specified number of epochs, and save the history\nhistory = model1.fit(train_generator, steps_per_epoch=len(train_generator), epochs=num_epochs, callbacks=[tensorboard_callback])\n\n# Stop timing the training\nend_time = time.time()\n\n# Calculate the training time by subtracting the start time from the end time\ntraining_time = end_time - start_time\n\n# Evaluate the model on the training set and get the training loss and accuracy\ntrain_loss, train_acc = model1.evaluate(train_generator)\n\n# Evaluate the model on the test set and get the test loss and accuracy\ntest_loss, test_acc = model1.evaluate(test_generator)\n\n# Count the number of parameters in the model\nnum_params = model1.count_params()\n\n# Open the results file in append mode and writing the results\nwith open('results.csv', mode='a', newline='') as results_file:\n    results_writer = csv.writer(results_file)\n    results_writer.writerow(['VGG 1 Block', training_time, train_loss, train_acc, test_acc, num_params])\n    \nmodel1.save('saved_models/vgg_1_block.h5')\n\nEpoch 1/20\n8/8 [==============================] - 4s 443ms/step - loss: 16.0090 - accuracy: 0.4938\nEpoch 2/20\n8/8 [==============================] - 4s 441ms/step - loss: 1.3641 - accuracy: 0.5063\nEpoch 3/20\n8/8 [==============================] - 4s 440ms/step - loss: 0.8025 - accuracy: 0.6000\nEpoch 4/20\n8/8 [==============================] - 4s 439ms/step - loss: 0.5900 - accuracy: 0.6875\nEpoch 5/20\n8/8 [==============================] - 4s 439ms/step - loss: 0.4382 - accuracy: 0.7750\nEpoch 6/20\n8/8 [==============================] - 4s 441ms/step - loss: 0.3841 - accuracy: 0.8687\nEpoch 7/20\n8/8 [==============================] - 4s 440ms/step - loss: 0.2853 - accuracy: 0.8875\nEpoch 8/20\n8/8 [==============================] - 4s 440ms/step - loss: 0.2322 - accuracy: 0.9438\nEpoch 9/20\n8/8 [==============================] - 4s 442ms/step - loss: 0.1883 - accuracy: 0.9563\nEpoch 10/20\n8/8 [==============================] - 4s 440ms/step - loss: 0.1287 - accuracy: 0.9812\nEpoch 11/20\n8/8 [==============================] - 4s 440ms/step - loss: 0.1186 - accuracy: 0.9875\nEpoch 12/20\n8/8 [==============================] - 4s 442ms/step - loss: 0.0991 - accuracy: 0.9625\nEpoch 13/20\n8/8 [==============================] - 4s 440ms/step - loss: 0.0737 - accuracy: 0.9812\nEpoch 14/20\n8/8 [==============================] - 4s 442ms/step - loss: 0.0589 - accuracy: 1.0000\nEpoch 15/20\n8/8 [==============================] - 4s 448ms/step - loss: 0.0466 - accuracy: 1.0000\nEpoch 16/20\n8/8 [==============================] - 4s 443ms/step - loss: 0.0412 - accuracy: 0.9937\nEpoch 17/20\n8/8 [==============================] - 4s 441ms/step - loss: 0.0302 - accuracy: 1.0000\nEpoch 18/20\n8/8 [==============================] - 4s 442ms/step - loss: 0.0279 - accuracy: 1.0000\nEpoch 19/20\n8/8 [==============================] - 4s 443ms/step - loss: 0.0257 - accuracy: 1.0000\nEpoch 20/20\n8/8 [==============================] - 4s 443ms/step - loss: 0.0187 - accuracy: 1.0000\n1/8 [==&gt;...........................] - ETA: 1s - loss: 0.0182 - accuracy: 1.00008/8 [==============================] - 1s 54ms/step - loss: 0.0206 - accuracy: 1.0000\n2/2 [==============================] - 0s 58ms/step - loss: 0.5984 - accuracy: 0.8250\n\n\n2023-04-19 14:38:31.364769: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-19 14:39:43.633980: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-19 14:39:44.234190: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n\nplot_predictions('Vgg1', model1, 'log_images/vgg_1_block', prediction_generator)\n\n 4/40 [==&gt;...........................] - ETA: 0s40/40 [==============================] - 1s 20ms/step\n\n\n2023-04-19 14:39:44.990430: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n\n\nVGG 3 Block\n\n# Define a function to create a VGG block with three convolutional layers\ndef vgg_3_block():\n    # Create a Sequential model object with the name 'vgg_block_3'\n    model = Sequential(name='vgg_block_3')\n    \n    # Add a convolutional layer with 64 filters, a kernel size of 3x3, 'same' padding, and ReLU activation,\n    # and specify the input shape as the desired image size and 3 color channels\n    model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=input_img_size))\n    \n    # Add a max pooling layer with a pool size of 2x2\n    model.add(MaxPooling2D((2, 2)))\n    \n    # Add another convolutional layer with 128 filters and 'same' padding\n    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n    \n    # Add another max pooling layer\n    model.add(MaxPooling2D((2, 2)))\n    \n    # Add a third convolutional layer with 256 filters and 'same' padding\n    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n    \n    # Add a third max pooling layer\n    model.add(MaxPooling2D((2, 2)))\n    \n    # Flatten the output of the convolutional layers\n    model.add(Flatten())\n    \n    # Add a fully connected layer with 128 units and ReLU activation\n    model.add(Dense(128, activation='relu'))\n    \n    # Add a final output layer with a single unit and sigmoid activation (for binary classification)\n    model.add(Dense(1, activation='sigmoid'))\n    \n    # Return the completed model object\n    return model\n\n# Create an instance of the VGG block using the vgg_3_block function\nmodel2 = vgg_3_block()\n\n# Print a summary of the model's architecture\nmodel2.summary()\n\n# Define a log directory for TensorBoard\nlog_dir = 'log_stats/vgg_3_block'\n\n# Define the TensorBoard callback with update_freq='batch'\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq='batch')\n\nModel: \"vgg_block_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_1 (Conv2D)           (None, 128, 128, 64)      1792      \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 64, 64, 64)       0         \n 2D)                                                             \n                                                                 \n conv2d_2 (Conv2D)           (None, 64, 64, 128)       73856     \n                                                                 \n max_pooling2d_2 (MaxPooling  (None, 32, 32, 128)      0         \n 2D)                                                             \n                                                                 \n conv2d_3 (Conv2D)           (None, 32, 32, 256)       295168    \n                                                                 \n max_pooling2d_3 (MaxPooling  (None, 16, 16, 256)      0         \n 2D)                                                             \n                                                                 \n flatten_1 (Flatten)         (None, 65536)             0         \n                                                                 \n dense_2 (Dense)             (None, 128)               8388736   \n                                                                 \n dense_3 (Dense)             (None, 1)                 129       \n                                                                 \n=================================================================\nTotal params: 8,759,681\nTrainable params: 8,759,681\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n# Compile the model with 'adam' optimizer, 'binary_crossentropy' loss function, and 'accuracy' metric\nmodel2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Start timing the training\nstart_time = time.time()\n\n# Train the model using the training generator for a specified number of epochs, and save the history\nhistory = model2.fit(train_generator, steps_per_epoch=len(train_generator), epochs=num_epochs, callbacks=[tensorboard_callback])\n\n# Stop timing the training\nend_time = time.time()\n\n# Calculate the training time by subtracting the start time from the end time\ntraining_time = end_time - start_time\n\n# Evaluate the model on the training set and get the training loss and accuracy\ntrain_loss, train_acc = model2.evaluate(train_generator)\n\n# Evaluate the model on the test set and get the test loss and accuracy\ntest_loss, test_acc = model2.evaluate(test_generator)\n\n# Count the number of parameters in the model\nnum_params = model2.count_params()\n\n# Open the results file in append mode and writing the results\nwith open('results.csv', mode='a', newline='') as results_file:\n    results_writer = csv.writer(results_file)\n    results_writer.writerow(['VGG 3 Block', training_time, train_loss, train_acc, test_acc, num_params])\n    \nmodel2.save('saved_models/vgg_3_block.h5')\n\nEpoch 1/20\n8/8 [==============================] - 4s 452ms/step - loss: 1.2802 - accuracy: 0.5250\nEpoch 2/20\n8/8 [==============================] - 4s 445ms/step - loss: 0.6984 - accuracy: 0.5437\nEpoch 3/20\n8/8 [==============================] - 4s 446ms/step - loss: 0.7064 - accuracy: 0.5000\nEpoch 4/20\n8/8 [==============================] - 4s 446ms/step - loss: 0.6933 - accuracy: 0.6125\nEpoch 5/20\n8/8 [==============================] - 4s 452ms/step - loss: 0.6818 - accuracy: 0.5437\nEpoch 6/20\n8/8 [==============================] - 4s 452ms/step - loss: 0.6226 - accuracy: 0.6562\nEpoch 7/20\n8/8 [==============================] - 4s 449ms/step - loss: 0.5584 - accuracy: 0.7250\nEpoch 8/20\n8/8 [==============================] - 4s 446ms/step - loss: 0.4817 - accuracy: 0.7625\nEpoch 9/20\n8/8 [==============================] - 4s 446ms/step - loss: 0.4709 - accuracy: 0.8188\nEpoch 10/20\n8/8 [==============================] - 4s 451ms/step - loss: 0.3938 - accuracy: 0.8000\nEpoch 11/20\n8/8 [==============================] - 4s 445ms/step - loss: 0.3187 - accuracy: 0.8438\nEpoch 12/20\n8/8 [==============================] - 4s 448ms/step - loss: 0.2332 - accuracy: 0.9062\nEpoch 13/20\n8/8 [==============================] - 4s 449ms/step - loss: 0.1594 - accuracy: 0.9438\nEpoch 14/20\n8/8 [==============================] - 4s 456ms/step - loss: 0.0972 - accuracy: 0.9688\nEpoch 15/20\n8/8 [==============================] - 4s 469ms/step - loss: 0.0486 - accuracy: 0.9812\nEpoch 16/20\n8/8 [==============================] - 4s 445ms/step - loss: 0.0302 - accuracy: 1.0000\nEpoch 17/20\n8/8 [==============================] - 4s 453ms/step - loss: 0.0142 - accuracy: 1.0000\nEpoch 18/20\n8/8 [==============================] - 4s 448ms/step - loss: 0.0089 - accuracy: 1.0000\nEpoch 19/20\n8/8 [==============================] - 4s 449ms/step - loss: 0.0033 - accuracy: 1.0000\nEpoch 20/20\n8/8 [==============================] - 4s 446ms/step - loss: 0.0023 - accuracy: 1.0000\n8/8 [==============================] - 1s 78ms/step - loss: 0.0012 - accuracy: 1.0000\n1/2 [==============&gt;...............] - ETA: 0s - loss: 0.9936 - accuracy: 0.85002/2 [==============================] - 0s 79ms/step - loss: 1.0520 - accuracy: 0.8250\n\n\n2023-04-19 14:39:46.379523: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-19 14:41:00.131167: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-19 14:41:00.947230: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n\nplot_predictions('Vgg3', model2, 'log_images/vgg_3_block', prediction_generator)\n\n 9/40 [=====&gt;........................] - ETA: 0s40/40 [==============================] - 1s 14ms/step\n\n\n2023-04-19 14:41:01.335855: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n\n\nVGG 3 Block with Data Argumentation\n\n# Define an ImageDataGenerator for data augmentation during training\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,                   # rescale pixel values to [0,1]\n    rotation_range=45,                # random rotation between 0-45 degrees\n    # width_shift_range=0.2,            # random shift horizontally up to 20% of the image width\n    # height_shift_range=0.2,           # random shift vertically up to 20% of the image height\n    shear_range=0.2,                  # random shear up to 20%\n    zoom_range=0.2,                   # random zoom up to 20%\n    # horizontal_flip=True,             # randomly flip images horizontally \n    fill_mode='nearest'               # fill any missing pixels with the nearest available pixel\n)\n\n# Create a flow of augmented training data from the training directory\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,                        # path to training data directory\n    target_size=img_size,             # size of input images\n    batch_size=batch_size,            # number of images per batch\n    class_mode='binary'               # type of classification problem (binary or categorical)\n)\n\n# Define an ImageDataGenerator for rescaling pixel values in the test set\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\n# Create a flow of test data from the test directory\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,                         # path to test data directory\n    target_size=img_size,             # size of input images\n    batch_size=batch_size,            # number of images per batch\n    class_mode='binary'               # type of classification problem (binary or categorical)\n)\n\nFound 160 images belonging to 2 classes.\nFound 40 images belonging to 2 classes.\n\n\n\n# Create an instance of the VGG block using the vgg_3_block function\nmodel3 = vgg_3_block()\n\n# Print a summary of the model's architecture\nmodel3.summary()\n\n# Define a log directory for TensorBoard\nlog_dir = 'log_stats/vgg_3_block_with_args'\n\n# Define the TensorBoard callback with update_freq='batch'\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq='batch')\n\nModel: \"vgg_block_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_4 (Conv2D)           (None, 128, 128, 64)      1792      \n                                                                 \n max_pooling2d_4 (MaxPooling  (None, 64, 64, 64)       0         \n 2D)                                                             \n                                                                 \n conv2d_5 (Conv2D)           (None, 64, 64, 128)       73856     \n                                                                 \n max_pooling2d_5 (MaxPooling  (None, 32, 32, 128)      0         \n 2D)                                                             \n                                                                 \n conv2d_6 (Conv2D)           (None, 32, 32, 256)       295168    \n                                                                 \n max_pooling2d_6 (MaxPooling  (None, 16, 16, 256)      0         \n 2D)                                                             \n                                                                 \n flatten_2 (Flatten)         (None, 65536)             0         \n                                                                 \n dense_4 (Dense)             (None, 128)               8388736   \n                                                                 \n dense_5 (Dense)             (None, 1)                 129       \n                                                                 \n=================================================================\nTotal params: 8,759,681\nTrainable params: 8,759,681\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n# Compile the model with 'adam' optimizer, 'binary_crossentropy' loss function, and 'accuracy' metric\nmodel3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Start timing the training\nstart_time = time.time()\n\n# Train the model using the training generator for a specified number of epochs, and save the history\nhistory = model3.fit(train_generator, steps_per_epoch=len(train_generator), epochs=num_epochs, callbacks=[tensorboard_callback])\n\n# Stop timing the training\nend_time = time.time()\n\n# Calculate the training time by subtracting the start time from the end time\ntraining_time = end_time - start_time\n\n# Evaluate the model on the training set and get the training loss and accuracy\ntrain_loss, train_acc = model3.evaluate(train_generator)\n\n# Evaluate the model on the test set and get the test loss and accuracy\ntest_loss, test_acc = model3.evaluate(test_generator)\n\n# Count the number of parameters in the model\nnum_params = model3.count_params()\n\n# Open the results file in append mode and writing the results\nwith open('results.csv', mode='a', newline='') as results_file:\n    results_writer = csv.writer(results_file)\n    results_writer.writerow(['VGG 3 Block with Argumentation', training_time, train_loss, train_acc, test_acc, num_params])\n    \nmodel3.save('saved_models/vgg_3_block_with_args.h5')\n\nEpoch 1/20\n8/8 [==============================] - 4s 463ms/step - loss: 1.2631 - accuracy: 0.4812\nEpoch 2/20\n8/8 [==============================] - 4s 456ms/step - loss: 0.6834 - accuracy: 0.5750\nEpoch 3/20\n8/8 [==============================] - 4s 453ms/step - loss: 0.6574 - accuracy: 0.5813\nEpoch 4/20\n8/8 [==============================] - 4s 453ms/step - loss: 0.6267 - accuracy: 0.7375\nEpoch 5/20\n8/8 [==============================] - 4s 452ms/step - loss: 0.5854 - accuracy: 0.7125\nEpoch 6/20\n8/8 [==============================] - 4s 449ms/step - loss: 0.5650 - accuracy: 0.7500\nEpoch 7/20\n8/8 [==============================] - 4s 450ms/step - loss: 0.5137 - accuracy: 0.7875\nEpoch 8/20\n8/8 [==============================] - 4s 452ms/step - loss: 0.4996 - accuracy: 0.7812\nEpoch 9/20\n8/8 [==============================] - 4s 452ms/step - loss: 0.4974 - accuracy: 0.7688\nEpoch 10/20\n8/8 [==============================] - 4s 449ms/step - loss: 0.4721 - accuracy: 0.7688\nEpoch 11/20\n8/8 [==============================] - 4s 451ms/step - loss: 0.4361 - accuracy: 0.8250\nEpoch 12/20\n8/8 [==============================] - 4s 455ms/step - loss: 0.4523 - accuracy: 0.8125\nEpoch 13/20\n8/8 [==============================] - 4s 448ms/step - loss: 0.4220 - accuracy: 0.8125\nEpoch 14/20\n8/8 [==============================] - 4s 447ms/step - loss: 0.4201 - accuracy: 0.8000\nEpoch 15/20\n8/8 [==============================] - 4s 450ms/step - loss: 0.4078 - accuracy: 0.8125\nEpoch 16/20\n8/8 [==============================] - 4s 452ms/step - loss: 0.4350 - accuracy: 0.8375\nEpoch 17/20\n8/8 [==============================] - 4s 453ms/step - loss: 0.4067 - accuracy: 0.8125\nEpoch 18/20\n8/8 [==============================] - 4s 453ms/step - loss: 0.3679 - accuracy: 0.8813\nEpoch 19/20\n8/8 [==============================] - 4s 452ms/step - loss: 0.3514 - accuracy: 0.8313\nEpoch 20/20\n8/8 [==============================] - 4s 448ms/step - loss: 0.3851 - accuracy: 0.8188\n8/8 [==============================] - 1s 109ms/step - loss: 0.3559 - accuracy: 0.8438\n1/2 [==============&gt;...............] - ETA: 0s - loss: 0.5282 - accuracy: 0.80002/2 [==============================] - 0s 80ms/step - loss: 0.5902 - accuracy: 0.7500\n\n\n2023-04-19 14:41:02.555902: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-19 14:42:17.701998: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-19 14:42:18.761244: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n\nplot_predictions('Vgg3_args', model3, 'log_images/vgg_3_block_with_args', prediction_generator)\n\n 9/40 [=====&gt;........................] - ETA: 0s40/40 [==============================] - 1s 14ms/step\n\n\n2023-04-19 14:42:19.125891: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n\n\nVGG 16 Transfer Learning\n\n# Define the preprocessing function for VGG16 model\npreprocess_input = tf.keras.applications.vgg16.preprocess_input\n\n# Create a train data generator with the preprocessing function\ntrain_datagen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)\n\n# Define the train generator by reading the images from the train directory\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,                        # path to training data directory\n    target_size=img_size,             # size of input images\n    batch_size=batch_size,            # number of images per batch\n    class_mode='binary'               # type of classification problem (binary or categorical)\n)\n\n# Create a test data generator with the same preprocessing function as train generator\ntest_datagen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)\n\n# Define the test generator by reading the images from the test directory\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,                         # path to test data directory\n    target_size=img_size,             # size of input images\n    batch_size=batch_size,            # number of images per batch\n    class_mode='binary'               # type of classification problem (binary or categorical)\n)\n\nFound 160 images belonging to 2 classes.\nFound 40 images belonging to 2 classes.\n\n\n\ndef vgg_16_transfer_learning():\n    # load model\n    model = tf.keras.applications.vgg16.VGG16(include_top=False, input_shape=input_img_size)\n    # mark loaded layers as not trainable\n    for layer in model.layers:\n        layer.trainable = False\n    # add new classifier layers\n    flat1 = Flatten()(model.layers[-1].output)\n    class1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n    output = Dense(1, activation='sigmoid')(class1)\n    # define new model\n    model = keras.models.Model(inputs=model.inputs, outputs=output, name='vgg_16')\n    return model\n\nmodel4 = vgg_16_transfer_learning()\n\n# Print a summary of the model's architecture\nmodel4.summary()\n\n# Define a log directory for TensorBoard\nlog_dir = 'log_stats/vgg_16'\n\n# Define the TensorBoard callback with update_freq='batch'\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq='batch')\n\nModel: \"vgg_16\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 128, 128, 3)]     0         \n                                                                 \n block1_conv1 (Conv2D)       (None, 128, 128, 64)      1792      \n                                                                 \n block1_conv2 (Conv2D)       (None, 128, 128, 64)      36928     \n                                                                 \n block1_pool (MaxPooling2D)  (None, 64, 64, 64)        0         \n                                                                 \n block2_conv1 (Conv2D)       (None, 64, 64, 128)       73856     \n                                                                 \n block2_conv2 (Conv2D)       (None, 64, 64, 128)       147584    \n                                                                 \n block2_pool (MaxPooling2D)  (None, 32, 32, 128)       0         \n                                                                 \n block3_conv1 (Conv2D)       (None, 32, 32, 256)       295168    \n                                                                 \n block3_conv2 (Conv2D)       (None, 32, 32, 256)       590080    \n                                                                 \n block3_conv3 (Conv2D)       (None, 32, 32, 256)       590080    \n                                                                 \n block3_pool (MaxPooling2D)  (None, 16, 16, 256)       0         \n                                                                 \n block4_conv1 (Conv2D)       (None, 16, 16, 512)       1180160   \n                                                                 \n block4_conv2 (Conv2D)       (None, 16, 16, 512)       2359808   \n                                                                 \n block4_conv3 (Conv2D)       (None, 16, 16, 512)       2359808   \n                                                                 \n block4_pool (MaxPooling2D)  (None, 8, 8, 512)         0         \n                                                                 \n block5_conv1 (Conv2D)       (None, 8, 8, 512)         2359808   \n                                                                 \n block5_conv2 (Conv2D)       (None, 8, 8, 512)         2359808   \n                                                                 \n block5_conv3 (Conv2D)       (None, 8, 8, 512)         2359808   \n                                                                 \n block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n                                                                 \n flatten_3 (Flatten)         (None, 8192)              0         \n                                                                 \n dense_6 (Dense)             (None, 128)               1048704   \n                                                                 \n dense_7 (Dense)             (None, 1)                 129       \n                                                                 \n=================================================================\nTotal params: 15,763,521\nTrainable params: 1,048,833\nNon-trainable params: 14,714,688\n_________________________________________________________________\n\n\n\n# Compile the model with 'adam' optimizer, 'binary_crossentropy' loss function, and 'accuracy' metric\nmodel4.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Start timing the training\nstart_time = time.time()\n\n# Train the model using the training generator for a specified number of epochs, and save the history\nhistory = model4.fit(train_generator, steps_per_epoch=len(train_generator), epochs=num_epochs, callbacks=[tensorboard_callback])\n\n# Stop timing the training\nend_time = time.time()\n\n# Calculate the training time by subtracting the start time from the end time\ntraining_time = end_time - start_time\n\n# Evaluate the model on the training set and get the training loss and accuracy\ntrain_loss, train_acc = model4.evaluate(train_generator)\n\n# Evaluate the model on the test set and get the test loss and accuracy\ntest_loss, test_acc = model4.evaluate(test_generator)\n\n# Count the number of parameters in the model\nnum_params = model4.count_params()\n\n# Open the results file in append mode and writing the results\nwith open('results.csv', mode='a', newline='') as results_file:\n    results_writer = csv.writer(results_file)\n    results_writer.writerow(['VGG 16', training_time, train_loss, train_acc, test_acc, num_params])\n    \nmodel4.save('saved_models/vgg_16_transfer_learning.h5')\n\nEpoch 1/20\n8/8 [==============================] - 3s 349ms/step - loss: 5.2201 - accuracy: 0.7875\nEpoch 2/20\n8/8 [==============================] - 3s 337ms/step - loss: 1.4520 - accuracy: 0.9250\nEpoch 3/20\n8/8 [==============================] - 3s 342ms/step - loss: 0.2833 - accuracy: 0.9625\nEpoch 4/20\n8/8 [==============================] - 3s 340ms/step - loss: 3.8766e-07 - accuracy: 1.0000\nEpoch 5/20\n8/8 [==============================] - 3s 335ms/step - loss: 2.5094e-04 - accuracy: 1.0000\nEpoch 6/20\n8/8 [==============================] - 3s 336ms/step - loss: 9.4699e-06 - accuracy: 1.0000\nEpoch 7/20\n8/8 [==============================] - 3s 335ms/step - loss: 5.3511e-07 - accuracy: 1.0000\nEpoch 8/20\n8/8 [==============================] - 3s 339ms/step - loss: 3.5025e-07 - accuracy: 1.0000\nEpoch 9/20\n8/8 [==============================] - 3s 336ms/step - loss: 2.1423e-07 - accuracy: 1.0000\nEpoch 10/20\n8/8 [==============================] - 3s 337ms/step - loss: 1.9476e-07 - accuracy: 1.0000\nEpoch 11/20\n8/8 [==============================] - 3s 337ms/step - loss: 1.4968e-07 - accuracy: 1.0000\nEpoch 12/20\n8/8 [==============================] - 3s 337ms/step - loss: 1.2451e-07 - accuracy: 1.0000\nEpoch 13/20\n8/8 [==============================] - 3s 335ms/step - loss: 1.1644e-07 - accuracy: 1.0000\nEpoch 14/20\n8/8 [==============================] - 3s 336ms/step - loss: 1.0194e-07 - accuracy: 1.0000\nEpoch 15/20\n8/8 [==============================] - 3s 337ms/step - loss: 9.6853e-08 - accuracy: 1.0000\nEpoch 16/20\n8/8 [==============================] - 3s 339ms/step - loss: 8.6054e-08 - accuracy: 1.0000\nEpoch 17/20\n8/8 [==============================] - 3s 338ms/step - loss: 7.8499e-08 - accuracy: 1.0000\nEpoch 18/20\n8/8 [==============================] - 3s 337ms/step - loss: 7.4105e-08 - accuracy: 1.0000\nEpoch 19/20\n8/8 [==============================] - 3s 337ms/step - loss: 6.9560e-08 - accuracy: 1.0000\nEpoch 20/20\n8/8 [==============================] - 3s 337ms/step - loss: 6.3877e-08 - accuracy: 1.0000\n8/8 [==============================] - 3s 331ms/step - loss: 6.1388e-08 - accuracy: 1.0000\n2/2 [==============================] - 1s 323ms/step - loss: 3.0647 - accuracy: 0.8500\n\n\n2023-04-19 14:42:20.544130: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-19 14:43:16.201525: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-19 14:43:19.074663: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n\ntest_loss, test_acc = model4.evaluate(test_generator)\n\n2023-04-19 14:43:19.915752: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n2/2 [==============================] - 1s 332ms/step - loss: 3.0647 - accuracy: 0.8500\n\n\n\nplot_predictions('Vgg16', model4, 'log_images/vgg_16', prediction_generator_vgg)\n\n 4/40 [==&gt;...........................] - ETA: 0s40/40 [==============================] - 1s 21ms/step\n\n\n2023-04-19 14:43:20.693366: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n\n\nMLP with 18 million trainable parameters\n\n# Define the MLP model\ndef create_mlp_18():\n    model = Sequential(name = 'MLP')\n    model.add(Flatten(input_shape=input_img_size))\n    model.add(Dense(256, activation='relu'))\n    model.add(Dense(4096, activation='relu'))\n    model.add(Dense(1024, activation='relu'))\n    # model.add(Dropout(0.2))\n    model.add(Dense(1, activation='sigmoid'))\n    return model\n\nmodel5 = create_mlp_18()\n# Print the model summary to see the number of parameters\nmodel5.summary()\n\n# Define a log directory for TensorBoard\nlog_dir = 'log_stats/mlp_18'\n\n# Define the TensorBoard callback with update_freq='batch'\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq='batch')\n\nModel: \"MLP\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n flatten_4 (Flatten)         (None, 49152)             0         \n                                                                 \n dense_8 (Dense)             (None, 256)               12583168  \n                                                                 \n dense_9 (Dense)             (None, 4096)              1052672   \n                                                                 \n dense_10 (Dense)            (None, 1024)              4195328   \n                                                                 \n dense_11 (Dense)            (None, 1)                 1025      \n                                                                 \n=================================================================\nTotal params: 17,832,193\nTrainable params: 17,832,193\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n# Compile the model with 'adam' optimizer, 'binary_crossentropy' loss function, and 'accuracy' metric\nmodel5.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Start timing the training\nstart_time = time.time()\n\n# Train the model using the training generator for a specified number of epochs, and save the history\nhistory = model5.fit(train_generator, steps_per_epoch=len(train_generator), epochs=num_epochs, callbacks=[tensorboard_callback])\n\n# Stop timing the training\nend_time = time.time()\n\n# Calculate the training time by subtracting the start time from the end time\ntraining_time = end_time - start_time\n\n# Evaluate the model on the training set and get the training loss and accuracy\ntrain_loss, train_acc = model5.evaluate(train_generator)\n\n# Evaluate the model on the test set and get the test loss and accuracy\ntest_loss, test_acc = model5.evaluate(test_generator)\n\n# Count the number of parameters in the model\nnum_params = model5.count_params()\n\n# Open the results file in append mode and writing the results\nwith open('results.csv', mode='a', newline='') as results_file:\n    results_writer = csv.writer(results_file)\n    results_writer.writerow(['MLP18', training_time, train_loss, train_acc, test_acc, num_params])\n    \nmodel5.save('saved_models/mlp18.h5')\n\nEpoch 1/20\n8/8 [==============================] - 2s 181ms/step - loss: 828.1496 - accuracy: 0.5188\nEpoch 2/20\n8/8 [==============================] - 1s 181ms/step - loss: 16.2150 - accuracy: 0.5375\nEpoch 3/20\n8/8 [==============================] - 1s 180ms/step - loss: 15.6720 - accuracy: 0.7688\nEpoch 4/20\n8/8 [==============================] - 1s 177ms/step - loss: 12.2998 - accuracy: 0.7250\nEpoch 5/20\n8/8 [==============================] - 1s 179ms/step - loss: 5.2125 - accuracy: 0.7750\nEpoch 6/20\n8/8 [==============================] - 1s 180ms/step - loss: 0.6979 - accuracy: 0.7625\nEpoch 7/20\n8/8 [==============================] - 1s 180ms/step - loss: 0.4396 - accuracy: 0.8125\nEpoch 8/20\n8/8 [==============================] - 1s 177ms/step - loss: 0.2875 - accuracy: 0.8375\nEpoch 9/20\n8/8 [==============================] - 1s 179ms/step - loss: 0.2572 - accuracy: 0.8562\nEpoch 10/20\n8/8 [==============================] - 1s 176ms/step - loss: 0.2298 - accuracy: 0.8813\nEpoch 11/20\n8/8 [==============================] - 1s 176ms/step - loss: 0.2363 - accuracy: 0.8625\nEpoch 12/20\n8/8 [==============================] - 1s 176ms/step - loss: 0.2186 - accuracy: 0.8813\nEpoch 13/20\n8/8 [==============================] - 1s 178ms/step - loss: 0.2041 - accuracy: 0.8875\nEpoch 14/20\n8/8 [==============================] - 1s 182ms/step - loss: 0.1776 - accuracy: 0.8875\nEpoch 15/20\n8/8 [==============================] - 1s 180ms/step - loss: 0.1936 - accuracy: 0.8813\nEpoch 16/20\n8/8 [==============================] - 1s 181ms/step - loss: 0.1938 - accuracy: 0.8750\nEpoch 17/20\n8/8 [==============================] - 1s 178ms/step - loss: 0.2023 - accuracy: 0.8562\nEpoch 18/20\n8/8 [==============================] - 1s 178ms/step - loss: 0.2071 - accuracy: 0.8562\nEpoch 19/20\n8/8 [==============================] - 1s 179ms/step - loss: 0.2038 - accuracy: 0.8562\nEpoch 20/20\n8/8 [==============================] - 1s 177ms/step - loss: 0.2014 - accuracy: 0.8562\n1/8 [==&gt;...........................] - ETA: 1s - loss: 0.1485 - accuracy: 0.95008/8 [==============================] - 0s 34ms/step - loss: 0.1929 - accuracy: 0.8562\n2/2 [==============================] - 0s 28ms/step - loss: 1.7975 - accuracy: 0.6750\n\n\n2023-04-19 14:43:22.001196: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-19 14:43:51.992711: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-19 14:43:52.421763: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n\nplot_predictions('MLP18', model5, 'log_images/mlp18', prediction_generator)\n\n 6/40 [===&gt;..........................] - ETA: 0s40/40 [==============================] - 1s 11ms/step\n\n\n2023-04-19 14:43:53.022685: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n\n\nMLP with 135 Million Trainable Parameters\n\n# Define the MLP model\ndef create_mlp_135():\n    model = Sequential(name = 'MLP')\n    model.add(Flatten(input_shape=input_img_size))\n    model.add(Dense(2500, activation='relu'))\n    model.add(Dense(2500, activation='relu'))\n    model.add(Dense(2500, activation='relu'))\n    # model.add(Dropout(0.2))\n    model.add(Dense(1, activation='sigmoid'))\n    return model\n\nmodel6 = create_mlp_135()\n# Print the model summary to see the number of parameters\nmodel6.summary()\n\n# Define a log directory for TensorBoard\nlog_dir = 'log_stats/mlp_135'\n\n# Define the TensorBoard callback with update_freq='batch'\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq='batch')\n\n2023-04-19 14:55:06.216229: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 491520000 exceeds 10% of free system memory.\n2023-04-19 14:55:06.543298: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 491520000 exceeds 10% of free system memory.\n2023-04-19 14:55:06.622387: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 491520000 exceeds 10% of free system memory.\n\n\nModel: \"MLP\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n flatten_5 (Flatten)         (None, 49152)             0         \n                                                                 \n dense_12 (Dense)            (None, 2500)              122882500 \n                                                                 \n dense_13 (Dense)            (None, 2500)              6252500   \n                                                                 \n dense_14 (Dense)            (None, 2500)              6252500   \n                                                                 \n dense_15 (Dense)            (None, 1)                 2501      \n                                                                 \n=================================================================\nTotal params: 135,390,001\nTrainable params: 135,390,001\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n# Compile the model with 'adam' optimizer, 'binary_crossentropy' loss function, and 'accuracy' metric\nmodel6.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Start timing the training\nstart_time = time.time()\n\n# Train the model using the training generator for a specified number of epochs, and save the history\nhistory = model6.fit(train_generator, steps_per_epoch=len(train_generator), epochs=num_epochs, callbacks=[tensorboard_callback])\n\n# Stop timing the training\nend_time = time.time()\n\n# Calculate the training time by subtracting the start time from the end time\ntraining_time = end_time - start_time\n\n# Evaluate the model on the training set and get the training loss and accuracy\ntrain_loss, train_acc = model6.evaluate(train_generator)\n\n# Evaluate the model on the test set and get the test loss and accuracy\ntest_loss, test_acc = model6.evaluate(test_generator)\n\n# Count the number of parameters in the model\nnum_params = model6.count_params()\n\n# Open the results file in append mode and writing the results\nwith open('results.csv', mode='a', newline='') as results_file:\n    results_writer = csv.writer(results_file)\n    results_writer.writerow(['MLP135', training_time, train_loss, train_acc, test_acc, num_params])\n    \nmodel5.save('saved_models/mlp135.h5')\n\nEpoch 1/20\n8/8 [==============================] - 11s 1s/step - loss: 8302.7051 - accuracy: 0.4750\nEpoch 2/20\n8/8 [==============================] - 10s 1s/step - loss: 871.6079 - accuracy: 0.6062\nEpoch 3/20\n8/8 [==============================] - 10s 1s/step - loss: 273.8789 - accuracy: 0.7188\nEpoch 4/20\n8/8 [==============================] - 10s 1s/step - loss: 134.2486 - accuracy: 0.7250\nEpoch 5/20\n8/8 [==============================] - 10s 1s/step - loss: 77.9153 - accuracy: 0.8062\nEpoch 6/20\n8/8 [==============================] - 10s 1s/step - loss: 115.3639 - accuracy: 0.8125\nEpoch 7/20\n8/8 [==============================] - 10s 1s/step - loss: 86.5102 - accuracy: 0.8813\nEpoch 8/20\n8/8 [==============================] - 10s 1s/step - loss: 5.3630 - accuracy: 0.9563\nEpoch 9/20\n8/8 [==============================] - 10s 1s/step - loss: 40.6399 - accuracy: 0.9187\nEpoch 10/20\n8/8 [==============================] - 10s 1s/step - loss: 8.1418 - accuracy: 0.9375\nEpoch 11/20\n8/8 [==============================] - 10s 1s/step - loss: 22.5314 - accuracy: 0.9375\nEpoch 12/20\n8/8 [==============================] - 10s 1s/step - loss: 50.7123 - accuracy: 0.9000\nEpoch 13/20\n8/8 [==============================] - 10s 1s/step - loss: 57.6497 - accuracy: 0.8813\nEpoch 14/20\n8/8 [==============================] - 10s 1s/step - loss: 34.0063 - accuracy: 0.9187\nEpoch 15/20\n8/8 [==============================] - 10s 1s/step - loss: 21.8980 - accuracy: 0.9187\nEpoch 16/20\n8/8 [==============================] - 10s 1s/step - loss: 0.7129 - accuracy: 0.9812\nEpoch 17/20\n8/8 [==============================] - 10s 1s/step - loss: 128.1349 - accuracy: 0.8875\nEpoch 18/20\n8/8 [==============================] - 10s 1s/step - loss: 12.1085 - accuracy: 0.9625\nEpoch 19/20\n8/8 [==============================] - 10s 1s/step - loss: 7.0070 - accuracy: 0.9750\nEpoch 20/20\n8/8 [==============================] - 10s 1s/step - loss: 36.5162 - accuracy: 0.9625\n1/8 [==&gt;...........................] - ETA: 1s - loss: 3.6694e-20 - accuracy: 1.00008/8 [==============================] - 0s 38ms/step - loss: 39.5354 - accuracy: 0.9812\n2/2 [==============================] - 0s 49ms/step - loss: 298.6696 - accuracy: 0.7000\n\n\n2023-04-19 14:55:09.682721: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-19 14:55:09.831306: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 491520000 exceeds 10% of free system memory.\n2023-04-19 14:55:09.874048: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 491520000 exceeds 10% of free system memory.\n2023-04-19 14:58:32.310921: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-19 14:58:32.793472: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n\nplot_predictions('MLP135', model6, 'log_images/mlp135', prediction_generator)\n\n 2/40 [&gt;.............................] - ETA: 2s40/40 [==============================] - 2s 52ms/step\n\n\n2023-04-19 14:58:33.204956: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n\n\nComparisons\n\nimport pandas as pd\ndf = pd.read_csv('results.csv', index_col=0)\ndisplay(df)\n\n\n\n\n\n\n\n\nTraining Time\nTrain Loss\nTrain Acc\nTest Acc\nNum Params\n\n\nModel Name\n\n\n\n\n\n\n\n\n\nVGG 1 Block\n72.288201\n2.056565e-02\n1.00000\n0.825\n33556481\n\n\nVGG 3 Block\n73.744514\n1.194572e-03\n1.00000\n0.825\n8759681\n\n\nVGG 3 Block with Argumentation\n75.150163\n3.559158e-01\n0.84375\n0.750\n8759681\n\n\nVGG 16\n55.661988\n6.138818e-08\n1.00000\n0.850\n15763521\n\n\nMLP18\n29.990009\n1.929377e-01\n0.85625\n0.675\n17832193\n\n\nMLP135\n202.682255\n3.953538e+01\n0.98125\n0.700\n135390001\n\n\n\n\n\n\n\n\n\n\nAre the results as expected? Why or why not?\nVGG (1 Block)\nIn VGG (1 block) architecture, max-pooling and fully connected layers are placed after the single block of convolutional layers . Although this architecture has a limited number of parameters and trains quite quickly, it might not be able to recognise complicated aspects in the input.\nVGG (3 blocks)\nThis architecture made up of three blocks of convolutional layers, and each block is followed by fully connected and max-pooling layers. This architecture can capture more complicated aspects in the data because it has more parameters than VGG (1 block). However, because there are more parameters, it takes longer to train than VGG (1 block).\nVGG (3 blocks with data argumentation)\nBy adding different changes to the input images, such as rotation, scaling, and cropping, we inflate the size of the training set. By exposing the model to a larger range of input images when it is used with VGG (3 blocks), data augmentation can increase the model’s capacity to generalise to new data.\nVGG 16 Tranfer Learning\nWhen training a new model on a different dataset, transfer learning entails using a previously trained model as a starting point. Two pre-trained models that can be utilised for transfer learning are VGG16 and VGG19. These models have already learned to recognise a range of visual traits after being trained on massive datasets like ImageNet. For us, we used VGG16. We can reduce the amount of time and computational resources required to train a new model on a smaller dataset by starting with these pre-trained models.\nFurthermore, when the target dataset is modest, transfer learning using VGG16 outperforms training a model from scratch in terms of performance.\nMLP with 135 million parameters\nThe choice between using a MLP with 135 million parameters versus transfer learning with VGG16 depends on several factors, including the specific task, available resources, and the size of the dataset.\nIn general, using an MLP with 135 million parameters can potentially achieve higher accuracy compared to transfer learning with VGG16, especially for more complex tasks. However, training such a large MLP from scratch requires a significant amount of computational resources and time, making it impractical for many applications. Additionally, having such a large number of parameters increases the risk of overfitting, which can negatively impact model performance.\nOn the other hand, transfer learning with VGG16 can be a good choice for image classification tasks, especially if the dataset is small.\nMLP with 18 million parameters\nIn general, an MLP with 135 million parameters is likely to have a higher capacity to model complex relationships within the data compared to an MLP with 18 million parameters. However, having a larger number of parameters does not necessarily guarantee better performance, and can increase the risk of overfitting, especially if the dataset is small.\nOn the other hand, an MLP with 18 million parameters is likely to require fewer computational resources and training time compared to an MLP with 135 million parameters. This can be beneficial, especially if the available resources are limited.\nThe number of layers in both MLPs being 3, it’s important to note that the depth of the neural network is also an important factor to consider. Increasing the depth of the network can help to model more complex relationships within the data. However, it can also increase the risk of vanishing or exploding gradients, making it more difficult to train the network.\n\n\n\n\n\n\n\n\n\n\n\nModel Name\nTraining Time\nTrain Loss\nTrain Acc\nTest Acc\nNum Params\n\n\n\n\nVGG 1 Block\n72.28820062\n0.020565649\n1\n0.824999988\n33556481\n\n\nVGG 3 Block\n73.74451399\n0.001194572\n1\n0.824999988\n8759681\n\n\nVGG 3 Block with Argumentation\n75.15016341\n0.355915844\n0.84375\n0.75\n8759681\n\n\nVGG 16\n55.6619885\n6.14E-08\n1\n0.850000024\n15763521\n\n\nMLP18\n29.99000883\n0.192937687\n0.856249988\n0.675000012\n17832193\n\n\n\n\n\n\n\n\n\n\nDoes data augmentation help? Why or why not?\nData augmentation is a method of creating additional training data from the existing datase by using various transformations including rotation, scaling, flipping, cropping, and other picture modificationst.\nBy doing this, the model is exposed to a wider variety of training data and can improve its ability to identify and generalise to fresh, unexplored data.\nData augmentation can reduce overfitting, which occurs when the model memorises the training data rather than generalising to new data, by expanding the quantity and diversity of the training dataset. This may enable the model to perform more effectively on the test data.\nIn other words, data augmentation makes the model more reliable and capable of identifying objects or patterns.\nFor us, the argumented model is working worse than the original one. One reason could be, less number of epochs to train with, perhaps may with with higher epoch, we will get better results.\n\n\n\nDoes it matter how many epochs you fine tune the model? Why or why not?\nThe term “number of epochs” refers to how many times the complete dataset was run through the model during training while fine-tuning a pre-trained model.\nIf the number of epoch is low, the model will underfit.\nOn the other side, if the number of epochs is too high , then model might begin to overfit to the training data, which would mean that it would memorise the training data rather than generalising to new data.\nIt is crucial to select the right number of epochs for fine-tuning in accordance with the difficulty of the task, the size of the dataset, and the unique properties of the model and data. Normally, this value is calculated by keeping track of the model’s performance on a validation set during training and halting when the validation accuracy stops increasing or begins to drop.\nEven though we have not implemented it, but early stopping could be a good measure to utilize, when it comes to number of epochs.\nEarly stopping is a callback technique that can help prevent overfitting and save computational resources by stopping the training process before it completes all epochs. It can improve model performance, reduce training time, and help generalize the model better to unseen data.\n\n\n\nAre there any particular images that the model is confused about? Why or why not?\n\nThis was the image for which each model got confused."
  },
  {
    "objectID": "projects/005 Code Meets Art/index.html",
    "href": "projects/005 Code Meets Art/index.html",
    "title": "A Tribute to IITGN’s CSE Faculty",
    "section": "",
    "text": "GitHub \n\nSecured third place in the inaugural Code Meet Art competition hosted by the Coding Initiative at Indian Institute of Technology Gandhinagar ; Created a mosaic cover featuring the portraits of CSE Department faculty against the Lal-Minar with a subtle polygraphic touch using some simple blocks of codes. The art symbolizes our academic community’s unity and diversity."
  },
  {
    "objectID": "projects/003 Evolutionary Artistry/index.html",
    "href": "projects/003 Evolutionary Artistry/index.html",
    "title": "Evolving Artistry",
    "section": "",
    "text": "Run Application \n\n\n\nAbstract\n\nThe project integrates advanced principles from Genetic Algorithms to effectively approximate intricate images using a streamlined, single running line approach. This method is not only innovative but also showcases the potential of computational creativity in generating visually striking artworks. By harnessing the initial line as a seed for Conway’s Game of Life, the project transcends traditional artistic boundaries, transforming simple inputs into complex and captivating compositions. This synthesis of algorithmic techniques not only explores the realms of artificial intelligence and machine learning but also pushes the boundaries of generative art, offering a novel perspective on how computational methods can inspire and create.\n\n\n\nIntroduction\n\nIn the ever-evolving field of generative art, the combination of Genetic Algorithms and Conway’s Game of Life presents a unique approach to image creation and transformation. This project aims to approximate intricate images using a single running line derived through Genetic Algorithms, which is then utilized as a seed for Conway’s Game of Life. The result is a seamless blend of algorithmic processes that produce visually compelling and complex artworks, demonstrating the fusion of artificial intelligence, machine learning, and creative expression.\n\n\n\nMethodology\n\nGenetic Algorithm for Image Approximation\n\nGenetic Algorithms, inspired by the process of natural selection, are employed to approximate intricate images with a single continuous line. This approach involves the evolution of candidate solutions through processes of selection, crossover, and mutation to achieve an optimal representation of the target image. The single running line generated through this method captures the essence of the image while maintaining a high degree of detail and coherence.\n\n\n\nConway’s Game of Life\n\nConway’s Game of Life, a cellular automaton devised by mathematician John Conway, serves as the transformative engine in this project. The initial line generated by the Genetic Algorithm is used as the seed configuration for the Game of Life. Through the iterative application of simple rules governing cell birth and death, the initial seed evolves into intricate patterns and structures, resulting in unique and captivating compositions. This process highlights the emergent properties of cellular automata and their potential for generating complex visual forms.\n\n\n\n\nImplementation\n\nImage Approximation with Genetic Algorithms\n\nThe Genetic Algorithm begins with a population of random lines, which are evaluated based on their similarity to the target image. Over successive generations, lines that better approximate the target image are selected and combined to produce new offspring lines. Mutations introduce random variations, ensuring diversity and aiding the exploration of the solution space. This evolutionary process continues until a sufficiently accurate approximation is achieved.\n\n\n\nSeeding and Evolving in the Game of Life\n\nThe optimized running line from the Genetic Algorithm is mapped onto a grid to serve as the initial configuration for Conway’s Game of Life. The cellular automaton then processes this seed according to its predefined rules, with each iteration producing a new state of the grid. The dynamic evolution of the seed line through the Game of Life’s iterations generates complex and unexpected patterns, transforming the original approximation into a piece of generative art.\n\n\n\n\nResults and Discussion\n\nThe integration of Genetic Algorithms and Conway’s Game of Life in this project demonstrates the potential of combining computational techniques for artistic creation. The initial single line approximation effectively captures the structure of the target image, while the subsequent evolution through the Game of Life introduces a layer of complexity and aesthetic appeal. The resulting artworks showcase the emergent properties of cellular automata and the creative possibilities of algorithmic processes. This approach not only highlights the power of artificial intelligence and machine learning in art but also opens new avenues for exploration in generative design.\n\n\n\nConclusion\n\nThis project exemplifies the synergy between Genetic Algorithms and Conway’s Game of Life in generating unique and visually striking artworks. By approximating images with a single running line and using it as a seed for cellular automata, the project explores new frontiers in computational creativity. The fusion of these algorithmic techniques offers a novel perspective on the role of artificial intelligence in art, demonstrating how computational methods can inspire and create intricate and captivating compositions.\n\n\n\nFuture Work\n\nFuture developments of this project could involve refining the Genetic Algorithm to enhance the accuracy and detail of the initial line approximation. Additionally, exploring variations in the rules of Conway’s Game of Life or integrating other cellular automata could yield even more diverse and complex artworks. Further research into the intersection of machine learning and generative art will continue to uncover innovative methods for creating and transforming visual content."
  },
  {
    "objectID": "projects/001 Vision Trasformer from Scratch/index.html",
    "href": "projects/001 Vision Trasformer from Scratch/index.html",
    "title": "Vision Transformer Implementation from Scratch",
    "section": "",
    "text": "GitHub \n\nimport numpy as np\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\nfrom torch.nn import CrossEntropyLoss\nimport pdb\n\nnp.random.seed(0)\ntorch.manual_seed(0)\n_device_ = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n\n\n\n\n\n\nImage 1: Architecture of ViT\n\n\n\n\n\n\n\n\n\n\nImage 2: Architecture of ViT for Classification\n\n\n\n\n\n\nThe idea of patch embedding is to divide the images into patches and then later flatten them to a one dimensional vector before feeding it to the model. Traditionally tramsformers are made to work with sequence of words, thus patch embedding allow us to sequencify the images.\n\ndef patch_embedding(images, n_patch = 7): # n_patch = 7 means 7x7 grid of patches, so there will be 49 patches (note, image size is 28x28)\n    n, c, h, w = images.shape # for MNIST, n = n (number of images), c = 1 (channels), h = 28 (height), w = 28 (width)\n\n    assert h == w, 'Input image should be square'\n    \n    patches = torch.zeros(n, n_patch ** 2, h * w * c // n_patch ** 2)   # (N, 49, 16) holds all the patches for all the images\n                                                                        # 28 * 28 * 1 / 7 * 7 = 16 pixels per patch\n\n    patch_size = h // n_patch # 28 / 7 = 4\n\n    for idx, image in enumerate(images): # ennumerate over the images\n        for i in range(n_patch): # iterate over the patches\n            for j in range(n_patch): # iterate over the patches\n                patch = image[:, i * patch_size: (i + 1) * patch_size, j * patch_size: (j + 1) * patch_size] # get the patch, (1, 4, 4)\n                patches[idx, i * n_patch + j] = patch.flatten() # flatten the patch and assign it to the patches tensor\n\n    return patches # N x 49 x 16\n\n\n\n\n\n\nImage 3: Equation for posistional encodeing\n\n\n\n\n\n\n\ndef positional_encoding(sequence_length, d): # 50, 8\n    \n    result = torch.ones(sequence_length, d) # 50 x 8\n\n    for i in range(sequence_length):\n        for j in range(d):\n            # the equation comes form the paper, depicted in the Image 3\n            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** (j / d)))\n\n    return result # 50 x 8\n\n\nclass MultiHeadAttention(nn.Module):\n\n    def __init__(self, d, n_heads = 2): # we have 8 dimensional vectors and 2 heads\n        super().__init__()\n\n        self.d = d # d is the dimension\n        self.n_heads = n_heads # n_heads is the number of heads\n\n        assert d % n_heads == 0, 'd should be divisible by n_heads'\n\n        self.d_head = d // n_heads # 8 / 2 = 4\n\n        self.q = nn.ModuleList([nn.Linear(self.d_head, self.d_head) for _ in range(self.n_heads)]) # [(4, 4), (4, 4)] =&gt; multi-heads\n        self.k = nn.ModuleList([nn.Linear(self.d_head, self.d_head) for _ in range(self.n_heads)]) # [(4, 4), (4, 4)] =&gt; multi-heads\n        self.v = nn.ModuleList([nn.Linear(self.d_head, self.d_head) for _ in range(self.n_heads)]) # [(4, 4), (4, 4)] =&gt; multi-heads\n\n        self.softmax = nn.Softmax(dim = -1)\n\n    def forward(self, sequences):\n        # N, seq_len, token_dim =&gt; N, 50, 8\n        # Each patch will have its own k, q, v\n        # Now each patch has a dimension 8, and we apply them across two heads, thus we do it twice with 4 dimensions each\n        results = []\n        for sequence in sequences:\n            seq_result = []\n            for head in range(self.n_heads): # 0, 1, or 2 (assuming n_heads is 3 for this example)\n                q_mapping = self.q[head]\n                k_mapping = self.k[head]\n                v_mapping = self.v[head]\n\n                # We take the first d_head dimensions and the next d_head dimensions in the next iteration\n                seq = sequence[:, head * self.d_head : (head + 1) * self.d_head] # [0: 4], [4: 8]\n\n                # We multiply the sequence with the q, k, v matrices\n                q = q_mapping(seq)\n                k = k_mapping(seq)\n                v = v_mapping(seq)\n                \n                # Compute attention scores\n                attention = self.softmax(torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(self.d_head)) # (batch_size, seq_len, seq_len)\n\n                # Apply attention to the value matrix\n                attended_seq = torch.matmul(attention, v) # (batch_size, seq_len, d_head)\n\n                seq_result.append(attended_seq) # append the result of the current head\n\n            # Concatenate the results of the heads along the last dimension\n            results.append(torch.hstack(seq_result)) # (batch_size, seq_len, d_model)\n\n        # Concatenate the results of the sequences along the batch dimension\n        return torch.cat([torch.unsqueeze(r, dim=0) for r in results], dim=0) # (total_sequences, batch_size, seq_len, d_model)\n\n\nclass EncoderVIT(nn.Module):\n\n    def __init__(self, hidden_d, n_heads, mlp_ratio = 4):\n        super().__init__()\n\n        self.hidden_d = hidden_d\n        self.n_heads = n_heads\n        self.mlp_ratio = mlp_ratio\n\n        # here we are following the flow mentioned in image 1, norm -&gt; mha -&gt; norm -&gt; mlp\n        self.norm1 = nn.LayerNorm(hidden_d) \n        self.mha = MultiHeadAttention(hidden_d, n_heads)\n        self.norm2 = nn.LayerNorm(hidden_d)\n        self.mlp = nn.Sequential(\n            nn.Linear(hidden_d, hidden_d * mlp_ratio),\n            nn.GELU(),\n            nn.Linear(hidden_d * mlp_ratio, hidden_d)\n        )\n\n    def forward(self, x):\n        out1 = self.norm1(x) # 32 x 50 x 8\n        out1 = self.mha(out1) # 32 x 50 x 8\n        out1 = x + out1 # adding the x will do the residual connection as shown in image 1 # 32 x 50 x 8\n        out2 = self.norm2(out1)\n        out2 = self.mlp(out2)\n        out2 = out1 + out2\n\n        # # A compact way to fo the exact same thing is given below\n        # out = x + self.mha(self.norm1(x))\n        # out = out + self.mlp(self.norm2(out))\n\n        return out2\n\n\nclass VIT(nn.Module):\n    def __init__(self, channel = 1, height = 28, width = 28, n_patch = 7, n_blocks = 2, hidden_d = 8, n_heads = 2, out_dim = 10):\n        super().__init__()\n\n        self.channel = channel\n        self.height = height\n        self.width = width\n        self.n_patch = n_patch\n        self.hidden_d = hidden_d\n        self.n_blocks = n_blocks\n        self.n_heads = n_heads\n\n        assert height == width, 'Input image should be square'\n        assert height % n_patch == 0, 'n_patch should be a factor of height'\n\n        self.patch_size = (height // n_patch, width // n_patch) # (4, 4)\n\n        # linear mapping of the patches\n        self.input_d = int(channel * self.patch_size[0] * self.patch_size[1]) # 1 * 4 * 4 = 16\n        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d) # 16 -&gt; 8\n\n        # learnable class token\n        self.class_token = nn.Parameter(torch.randn(1, self.hidden_d)) # 1 x 8\n\n        # positional embedding\n        self.pos_embed = nn.Parameter(torch.zeros((self.n_patch ** 2 + 1, self.hidden_d))) # 50 x 8\n        self.pos_embed.requires_grad = False\n\n        # In image 1, we see the encoder block is repeated n times, here we are repeating it 2 times\n        # Encoder block\n        self.blocks = nn.ModuleList(\n            [\n                EncoderVIT(self.hidden_d, self.n_heads) for _ in range(self.n_blocks)\n            ]\n        )\n\n        # we just need a final MLP layer to get the output\n        self.mlp = nn.Sequential(\n            nn.Linear(self.hidden_d, out_dim),\n            nn.Softmax(dim = -1)\n        )\n \n    def forward(self, images):\n        n, c, h, w = images.shape\n        # we get the patches\n        patches = patch_embedding(images, self.n_patch) # N x 49 x 16\n\n        # in image 1, we see the patches get passed through a linear projection which outputs an eight dimnsional vector, the next line does the exact same thing\n        # performing a linear projection on the patches\n        tokens = self.linear_mapper(patches.to(device=_device_)) # N x 49 x 8\n\n        # in image 2, we see along with the patches there is a &lt;cls&gt; token which is passed with each image, thus for each image we need to append a classification\n        # token, the next line does the exact same thing\n        # adding the class token to the tokens\n        tokens = torch.stack([torch.vstack((self.class_token, token)) for token in tokens]) # N x 50 x 8\n\n        # in image 2, we see the positional encoding is added to the tokens, the next line does the exact same thing\n        # adding the positional embedding to the tokens\n        pos_embed = self.pos_embed.repeat(n, 1, 1) # N x 50 x 8\n\n        # in image 1, we see the tokens and positional encoding are added together, the next line does the exact same thing\n        out = tokens + pos_embed\n        # transformer block\n        for block in self.blocks:\n            out = block(out)\n\n        # apply the final MLP layer only in the classification token, for that we need to extract the classification token\n        cls_token = out[:, 0] # 1 x 8\n        \n        return self.mlp(cls_token) # 1 x 10\n\nNow since everything is set, we define the training loop\n\ndef train():\n\n    transform = ToTensor()\n    train_set = MNIST(root = './data', train = True, download = True, transform = transform)\n    test_set = MNIST(root = './data', train = False, download = True, transform = transform)\n\n    train_loader = DataLoader(train_set, batch_size = 32, shuffle = True)\n    test_loader = DataLoader(test_set, batch_size = 32, shuffle = False)\n\n    device = _device_\n    model = VIT(channel=1, height=28, width=28, n_patch=7, n_blocks=2, hidden_d=8, n_heads=2, out_dim=10).to(device)\n    n_epochs = 2\n    lr = 0.005\n\n    optimizer = optim.Adam(model.parameters(), lr = lr)\n    criterion = CrossEntropyLoss()\n\n    # training loop\n    for epoch in range(n_epochs):\n        train_loss = 0.0\n        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{n_epochs} - Training\"):\n            images, labels = batch\n            images, labels = images.to(device), labels.to(device)\n            y_hat = model(images)\n            loss = criterion(y_hat, labels)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.detach().cpu().item() / len(train_loader)\n        print(f\"Epoch {epoch + 1}/{n_epochs} - Training loss: {train_loss}\")\n\n    # testing loop\n    with torch.no_grad():\n        correct = 0\n        total = 0\n        for batch in tqdm(test_loader, desc=f\"Epoch {epoch + 1}/{n_epochs} - Testing\"):\n            images, labels = batch\n            images, labels = images.to(device), labels.to(device)\n            y_hat = model(images)\n            _, predicted = torch.max(y_hat, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n        print(f\"Epoch {epoch + 1}/{n_epochs} - Testing accuracy: {correct / total}\")\n\n\nif __name__ == '__main__':\n    train()\n\nEpoch 1/2 - Training: 100%|██████████| 1875/1875 [04:29&lt;00:00,  6.97it/s]\nEpoch 2/2 - Training: 100%|██████████| 1875/1875 [04:30&lt;00:00,  6.94it/s]\nEpoch 2/2 - Testing: 100%|██████████| 313/313 [00:21&lt;00:00, 14.52it/s]\n\n\nEpoch 1/2 - Training loss: 2.1409480080922405\nEpoch 2/2 - Training loss: 2.093113217480978\nEpoch 2/2 - Testing accuracy: 0.3824"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Soumyaratna Debnath",
    "section": "",
    "text": "Bio\n\n\n\n\n\n\n\n\n\nI am an MTech student in Computer Science at IIT Gandhinagar, where I specialize in Computer Vision, focusing on rendering-based optimization and 3D shape analysis. Currently, I am a researcher in the CVIG Lab at IIT Gandhinagar, delving into cutting-edge projects in the filed of Computer Vision. In addition to my academic journey, I served as a Project Fellow at IIT Gandhinagar for 19 Months, where I contributed to the Institute Management System using Angular and ASP.NET Core. With a passion for technology and innovation, I am committed to making a significant impact in the ever-evolving field of computer science.\n\nYou can take a look at My Resume"
  },
  {
    "objectID": "education.html",
    "href": "education.html",
    "title": "Education",
    "section": "",
    "text": "Master of Technology in Computer Science and Engineering\n  2022-2025\n  \n  Indian Institute of Technology Gandhinagar\n  CPI 9.25\n  \n  I am presently overseeing the following projects as integral components of my thesis work. \n  \n  \n    Primate Pose Estimation and Tracking for in-depth behavioral studies on primates using synthetic data.\n    Smart Farming with Aerial Imagery to enhance cotton crop yield using images taken from drones.\n  \n  \n  My projects endeavors heavily on the utilization of Computer Vision and Machine Learning techniques, exemplifying the intersection of technology and scientific exploration.\n  \n\n\n\n\n\n\nBachelor of Technology in Computer Science and Engineering \n2018-2022\n  \n  Cooch Behar Government Engineering College \n  CPI 9.34\n\n\n\nDuring my undergraduate studies, I had the privilege of working under the mentorship of Dr. Sourav De, focusing on the fascinating field of soft computing and evolutionary algorithms. In my final year project, I transitioned into the realm of deep learning, concentrating on underwater object detection and image enhancement using state-of-the-art models, thereby expanding my expertise and research interests."
  },
  {
    "objectID": "blogs/Topic-On-Games-Class-Notes/index.html",
    "href": "blogs/Topic-On-Games-Class-Notes/index.html",
    "title": "IIT Gandhinagar Topic on Games Class Notes",
    "section": "",
    "text": "For more information, visit the course page."
  },
  {
    "objectID": "blogs/Byte by Byte Vision/index.html",
    "href": "blogs/Byte by Byte Vision/index.html",
    "title": "Byte by Byte Vision",
    "section": "",
    "text": "Generic\n\nML Was Hard Until I Learned These 5 Secrets!\nHow I’d learn ML (if I could start over)\nBasic probability: Joint, marginal and conditional probability | Independence\nWhat is Prior And Posterior\nMathematics for Deep Learning Playlist\nMathematics for Deep Learning with Codes\nNeural Networks Implemented From Scratch\n\n\n\nPyTorch and Visualizations\n\nPyTorch Tutorials - Complete Beginner Course\nPyTorch Lightning Tutorial\nComplete TensorBoard Guide\n\n\n\nGraph Neural Networks\n\nGraph Neural Networks - DeepFindr\nGraph Convolutional Networks using only NumPy\nFind the related codes here\n\n\n\nVision Transformers\n\nAttention in transformers, visually explained\nThe Illustrated Transformer\nTransformer Neural Networks Derived from Scratch\nVision Transformer from Scratch\nImplement and Train ViT From Scratch for Image Recognition - PyTorch\nVision Transformer in PyTorch\nFind the related codes here\n\n\n\nGenerative Adversarial Networks\n\nUnderstand the Math and Theory of GANs\nBuilding our first simple GAN\nPix2Pix Paper Walkthrough and implementation from scratch\nCycleGAN Paper Walkthrough and implementation from scratch\nProGAN Paper Walkthrough and implementation from scratch\nSRGAN Paper Walkthrough and implementation from scratch\nStyleGAN Paper Walkthorugh and implementation from scratch\nFind the related codes here\n\n\n\nVariational Autoencoder\n\nVariational Autoencoder Explained\nVariational AutoEncoder Paper Walkthrough and implementation from scratch\nFind the related codes here\n\n\n\nDiffusion Models\n\nDiffusion models explained in 4-difficulty levels\nWhat are Diffusion Models?\nDiffusion Models | Paper Explanation | Math Explained\nDiffusion Models - Live Coding Tutorial\nDDPM Explained and implementation from scratch\nWhat is Stable Diffusion?\nCoding Stable Diffusion from scratch in PyTorch\n\n\n\nNeural Radiance Field (NeRF)\n\nNeural Radiance Fields Paper Explained"
  },
  {
    "objectID": "blogs/IITGN-Algorithms-Class-Notes/index.html",
    "href": "blogs/IITGN-Algorithms-Class-Notes/index.html",
    "title": "IIT Gandhinagar Algorithms Class Notes",
    "section": "",
    "text": "algorithm getCouples(M, W){\n    Sort M in descending order\n    Sort W in descending order\n\n    i = 1\n    j = 1\n    while i &lt;= n and j &lt;= m {\n        if M[i] &lt; W[j] {\n            j += 1\n        }\n        else if M[i] &lt;= W[j] + 3 {\n            pair up M[i] and W[j]\n            i += 1\n            j += 1\n        }\n        else, i += 1\n    }  \n}\nLet the array of man M = \\([m_1,m_2,...m_n]\\) and array of woman be \\([w_1,w_2,...w_3]\\) in sorted order.  Let us assume for the argument, that our solution pairs up \\(m_1\\) and \\(w_1\\).Lemma : There exist an optimum that pairs up \\(m_1\\) with \\(w_1\\).\nLet us assume that \\(m_1\\) is not paired with \\(w_1\\) in the optimum, while it is a valid pair.Case 1 - Let us assume \\(m_1\\) is paired with \\(w_n\\) and \\(w_1\\) is unpaired in the optimum.Now, we can swap \\(w_n\\) with \\(w_1\\), that is, \\(Optimum - [m_1, w_n]+[m_1, w_1]\\), which will not alter the number of pairs in the optimum.Case 2 - Let us assume \\(w_1\\) is paired with \\(m_n\\) and \\(m_1\\) is unpaired in the optimum.We can swap \\(m_n\\) with \\(m_1\\), that is, \\(Optimum-[m_n, w_1]+[m_1, w_1]\\), which will not alter the number of pairs in the optimum.Case 3 - Let us assume \\(m_1\\) is paired with some \\(w_j\\) and \\(w_1\\) is paired with some \\(m_i\\).Since, \\([m_1, w_1]\\) is a valid pair, then, we can confirm that \\([m_i, w_j]\\) is also a valid pair. Thus, by exchange argument, we can perform the operation, \\(Optimum -[m_1, w_j]-[m_i, w_1]+[m_1, w_1]+[m_i, w_j]\\), which will not alter the number of pairs in the optimum.\nThus, from exchange argument, our soultion is as good as the optimum.\nAnalysis of the running time\nThe running time of the algorithm is dominated by the sorting step, thus the running time is \\(O(wlogw+mlogm)\\) where w and m are the sizes for list of males and females.\n\n\n\n\n\nalgorithm getCredits(B){\n    i = 1\n    s = 0\n    while i &lt;= h{\n        m = max between all B[i, j] for j in [1, n]\n        s = s + m \n    }\n    return s\n}\nLet us consider our solution contains the element B[1, 1].\nLemma 1 : There exist an optimum which includes the entry B[1, 1]\nLet us consider an optimum soultion for the problem which includes the entry B[1, t]. Now we can argue that B[1, t] cannot be greater than B[1,1], because if it were the case, then our algorithm should have chosen B[1, t] in its solution along all B[1, j] for all j in [1, n] for the first hour. Thus the credit value for B[1, t] should be as good as B[1, 1]. Thus, B[1, 1] belongs to some optimum.\nNow, Let us consider our solution contains the element B[a, b].Lemma 2 : There exist an optimum which includes the entry B[a, b]\nLet us consider an optimum soultion for the problem which includes the entry B[a, t]. Now we can argue that B[a, t] cannot be greater than B[a, b], because if it were the case, then our algorithm should have chosen B[a, t] in its solution along all B[a, j] for all j in [1,n] for the first hour.Thus the credit value for B[a, t] should be as good as B[a,b]. Thus, B[a, b] belongs to some optimum.\nAnalysis of the running time\nThe loop takes nh time to execute, thus the running time for the algorithm is \\(O(nh)\\).\n\n\n\n\n\ndef removeMax(f){\n    i = 2\n    while i &lt;= n-1{\n        if f[i] is a local maximum {\n            if(i+2&lt;=n) \n                then, f[i+1] = max(f[i], f[i+2])\n            else f[i+1]=f[i]\n        }\n    }\n    return f\n}\nLet us consider our solution alters the entry A[n] to A[c]\n\nLemma : The optimum alters the entry A[n] to A[c]\nCase 1 - Let us assume the optimum does not alters the value of A[n]. In this case, to nullify the local maximum at A[b] and A[c], the optimum will have to perform 2 operations, that is A[b] to A[n] and A[c] to A[n]. This will take an extra operation. Thus, our initial assumption was wrong, the optimum have to alter the entry A[n].\nCase 2 - Let us assume the optimum alters the value of A[n] but not to A[c].Suppose the optimum alters the value of A[n] to T. if T \\(\\ge\\) A[b] and T &lt; A[c], then it will diminish the maximum at A[b], but it will need another operation at A[c]. And if T&gt;A[c], still it will have to make another operation to counter the new peak at A[n].Thus, our initial assumption was wrong. The optimum will have to alter the value of A[n] to A[c].\nCase 1 and 2 proves the lemma.\nSimilarly, we can prove that for each change we make in the array, the optimum will have to make the exact change in the array. Thus our solution is as good as the optimum.\nAnalysis of the Running Time\nThe running time of the algorithm is dominated by the while loop, which takes \\(O(n)\\) time.\n\n\n\n\n\n\n    I would disagree with the Candidate Greedy Strategy I, as this strategy may fail in certain situations. As an counter example,\n\nLet as consider that we need to find machines for the spectrum range L=1 to H=12 and the available technologies \\(T_1[1, 4]\\) \\(T_2[5, 8]\\) \\(T_3[9, 12]\\) and \\(T_4[2,7]\\).\n\nAs per the Greedy Strategy I, the solution for the problem will be \\([T_1T_2T_3T_4]\\).\nBut for this problem, there exist a better solution for the problem, that is \\([T_1T_2T_3]\\).This is because, the given Greedy Strategy will start with technology \\(T_1\\) since it covers the longest spectrum. But however, we can see, it comes out as redundent, as anyway, we have to have all the other technologies in order to cover the spectrum [L, H].\n(b)    I would agree with the Candidate Greedy Strategy II, and I believe the strategy always returns an optimum solution to the problem.\nLet us consider that our soultion starts with the technology \\(T_1\\).\n\nLemma 1 : There exist an optimum that starts with \\(T_1\\)\nLet us assume that there is some optimum that does not starts with \\(T_1(l_1,h_1)\\), rather it starts with \\(T_n(l_n,h_n)\\). Now we can argue that \\(h_n\\) is always less than or equal to \\(h_1\\), this is because, if it were not the case, our algorithm shoud have chosen \\(h_n\\) over \\(h_1\\). Thus we can easily swap \\(T_n\\) with \\(T_1\\) and it will not allter the number of technologies in the soultion.\nLemma 2 : Our soultion is as good as the optimum\nLet us assume our soultion as \\([t_1t_2..t_n]\\) while there exist an optimum as \\([m_1m_2..m_n]\\)Consider the end range for an technology \\(i\\) as \\(h(t_i)\\) and \\(h(m_i)\\).\nCase 1: \\(i=1\\)From Lemma 1, we can assume the base case \\((i=1)\\) for this inductive proof, that is,\\(h(m_1)\\le h(t_1)\\) to be true.\nCase 2: \\(i=j\\) Let us assume, \\(h(m_j)\\le h(t_j)\\) to be true\nCase 3: \\(i=j+1\\) We can argue that \\(h(m_{j+1})\\le h(t_{j+1})\\) as if it were not the case, our algorithm must have taken \\(h(m_{j+1})\\) into consideration. It proves that our solution is as good as the optimum."
  },
  {
    "objectID": "blogs/IITGN-Algorithms-Class-Notes/index.html#part-1",
    "href": "blogs/IITGN-Algorithms-Class-Notes/index.html#part-1",
    "title": "IIT Gandhinagar Algorithms Class Notes",
    "section": "",
    "text": "algorithm getCouples(M, W){\n    Sort M in descending order\n    Sort W in descending order\n\n    i = 1\n    j = 1\n    while i &lt;= n and j &lt;= m {\n        if M[i] &lt; W[j] {\n            j += 1\n        }\n        else if M[i] &lt;= W[j] + 3 {\n            pair up M[i] and W[j]\n            i += 1\n            j += 1\n        }\n        else, i += 1\n    }  \n}\nLet the array of man M = \\([m_1,m_2,...m_n]\\) and array of woman be \\([w_1,w_2,...w_3]\\) in sorted order.  Let us assume for the argument, that our solution pairs up \\(m_1\\) and \\(w_1\\).Lemma : There exist an optimum that pairs up \\(m_1\\) with \\(w_1\\).\nLet us assume that \\(m_1\\) is not paired with \\(w_1\\) in the optimum, while it is a valid pair.Case 1 - Let us assume \\(m_1\\) is paired with \\(w_n\\) and \\(w_1\\) is unpaired in the optimum.Now, we can swap \\(w_n\\) with \\(w_1\\), that is, \\(Optimum - [m_1, w_n]+[m_1, w_1]\\), which will not alter the number of pairs in the optimum.Case 2 - Let us assume \\(w_1\\) is paired with \\(m_n\\) and \\(m_1\\) is unpaired in the optimum.We can swap \\(m_n\\) with \\(m_1\\), that is, \\(Optimum-[m_n, w_1]+[m_1, w_1]\\), which will not alter the number of pairs in the optimum.Case 3 - Let us assume \\(m_1\\) is paired with some \\(w_j\\) and \\(w_1\\) is paired with some \\(m_i\\).Since, \\([m_1, w_1]\\) is a valid pair, then, we can confirm that \\([m_i, w_j]\\) is also a valid pair. Thus, by exchange argument, we can perform the operation, \\(Optimum -[m_1, w_j]-[m_i, w_1]+[m_1, w_1]+[m_i, w_j]\\), which will not alter the number of pairs in the optimum.\nThus, from exchange argument, our soultion is as good as the optimum.\nAnalysis of the running time\nThe running time of the algorithm is dominated by the sorting step, thus the running time is \\(O(wlogw+mlogm)\\) where w and m are the sizes for list of males and females.\n\n\n\n\n\nalgorithm getCredits(B){\n    i = 1\n    s = 0\n    while i &lt;= h{\n        m = max between all B[i, j] for j in [1, n]\n        s = s + m \n    }\n    return s\n}\nLet us consider our solution contains the element B[1, 1].\nLemma 1 : There exist an optimum which includes the entry B[1, 1]\nLet us consider an optimum soultion for the problem which includes the entry B[1, t]. Now we can argue that B[1, t] cannot be greater than B[1,1], because if it were the case, then our algorithm should have chosen B[1, t] in its solution along all B[1, j] for all j in [1, n] for the first hour. Thus the credit value for B[1, t] should be as good as B[1, 1]. Thus, B[1, 1] belongs to some optimum.\nNow, Let us consider our solution contains the element B[a, b].Lemma 2 : There exist an optimum which includes the entry B[a, b]\nLet us consider an optimum soultion for the problem which includes the entry B[a, t]. Now we can argue that B[a, t] cannot be greater than B[a, b], because if it were the case, then our algorithm should have chosen B[a, t] in its solution along all B[a, j] for all j in [1,n] for the first hour.Thus the credit value for B[a, t] should be as good as B[a,b]. Thus, B[a, b] belongs to some optimum.\nAnalysis of the running time\nThe loop takes nh time to execute, thus the running time for the algorithm is \\(O(nh)\\).\n\n\n\n\n\ndef removeMax(f){\n    i = 2\n    while i &lt;= n-1{\n        if f[i] is a local maximum {\n            if(i+2&lt;=n) \n                then, f[i+1] = max(f[i], f[i+2])\n            else f[i+1]=f[i]\n        }\n    }\n    return f\n}\nLet us consider our solution alters the entry A[n] to A[c]\n\nLemma : The optimum alters the entry A[n] to A[c]\nCase 1 - Let us assume the optimum does not alters the value of A[n]. In this case, to nullify the local maximum at A[b] and A[c], the optimum will have to perform 2 operations, that is A[b] to A[n] and A[c] to A[n]. This will take an extra operation. Thus, our initial assumption was wrong, the optimum have to alter the entry A[n].\nCase 2 - Let us assume the optimum alters the value of A[n] but not to A[c].Suppose the optimum alters the value of A[n] to T. if T \\(\\ge\\) A[b] and T &lt; A[c], then it will diminish the maximum at A[b], but it will need another operation at A[c]. And if T&gt;A[c], still it will have to make another operation to counter the new peak at A[n].Thus, our initial assumption was wrong. The optimum will have to alter the value of A[n] to A[c].\nCase 1 and 2 proves the lemma.\nSimilarly, we can prove that for each change we make in the array, the optimum will have to make the exact change in the array. Thus our solution is as good as the optimum.\nAnalysis of the Running Time\nThe running time of the algorithm is dominated by the while loop, which takes \\(O(n)\\) time.\n\n\n\n\n\n\n    I would disagree with the Candidate Greedy Strategy I, as this strategy may fail in certain situations. As an counter example,\n\nLet as consider that we need to find machines for the spectrum range L=1 to H=12 and the available technologies \\(T_1[1, 4]\\) \\(T_2[5, 8]\\) \\(T_3[9, 12]\\) and \\(T_4[2,7]\\).\n\nAs per the Greedy Strategy I, the solution for the problem will be \\([T_1T_2T_3T_4]\\).\nBut for this problem, there exist a better solution for the problem, that is \\([T_1T_2T_3]\\).This is because, the given Greedy Strategy will start with technology \\(T_1\\) since it covers the longest spectrum. But however, we can see, it comes out as redundent, as anyway, we have to have all the other technologies in order to cover the spectrum [L, H].\n(b)    I would agree with the Candidate Greedy Strategy II, and I believe the strategy always returns an optimum solution to the problem.\nLet us consider that our soultion starts with the technology \\(T_1\\).\n\nLemma 1 : There exist an optimum that starts with \\(T_1\\)\nLet us assume that there is some optimum that does not starts with \\(T_1(l_1,h_1)\\), rather it starts with \\(T_n(l_n,h_n)\\). Now we can argue that \\(h_n\\) is always less than or equal to \\(h_1\\), this is because, if it were not the case, our algorithm shoud have chosen \\(h_n\\) over \\(h_1\\). Thus we can easily swap \\(T_n\\) with \\(T_1\\) and it will not allter the number of technologies in the soultion.\nLemma 2 : Our soultion is as good as the optimum\nLet us assume our soultion as \\([t_1t_2..t_n]\\) while there exist an optimum as \\([m_1m_2..m_n]\\)Consider the end range for an technology \\(i\\) as \\(h(t_i)\\) and \\(h(m_i)\\).\nCase 1: \\(i=1\\)From Lemma 1, we can assume the base case \\((i=1)\\) for this inductive proof, that is,\\(h(m_1)\\le h(t_1)\\) to be true.\nCase 2: \\(i=j\\) Let us assume, \\(h(m_j)\\le h(t_j)\\) to be true\nCase 3: \\(i=j+1\\) We can argue that \\(h(m_{j+1})\\le h(t_{j+1})\\) as if it were not the case, our algorithm must have taken \\(h(m_{j+1})\\) into consideration. It proves that our solution is as good as the optimum."
  },
  {
    "objectID": "blogs/IITGN-Algorithms-Class-Notes/index.html#part-2",
    "href": "blogs/IITGN-Algorithms-Class-Notes/index.html#part-2",
    "title": "IIT Gandhinagar Algorithms Class Notes",
    "section": "Part 2",
    "text": "Part 2\n\nQuestion 1.\n\nSolution Description\nWe will use the generic binary search algorithm. At every instance of the function call, we will check if the index of the Mid matches its value.\nalgorithm getElement(List, Base){\n    Mid = List.size()/2\n    if Base + Mid is equal to List[Mid], then return List[Mid]\n    if List[Mid] &gt; Mid + Base, then return getElement(List[:Mid], Base)\n    else return getElement(List[Mid+1:], Base + Mid)\n}\n\nL = Input List\nResult = getElement(L, 0)\nLet k be the size of the input list.\nLemma: The algorithm returns the correct result for the list of size n.\nBase Case : k = 1 Since there is only one element in the list, our algorithm just has to check if the value of this element is equal to its index (1 in this case) or not. Thus our algorithm returns the correct result for an input list of size k = 1.\nInductive hypothesis : As an inductive hypothesis, let us assume that our algorithm is correct for an input list of size k &lt; n.\nInductive Step : k = n At each instance, our algorithm checks if the middle value of the list is our required answer.\nIf the value of the middle element = index of the middle element, then this is our answer. If the value of the middle element M &gt; index of the middle element, then all the elements on the right side of M will surely not be the required answer. Thus we can discard this half and look for the result of the first half \\([1,2,3,...\\frac{n}{2}]\\).\nSimilarly, if the value of the middle element &lt; index of the middle element, we discard other half \\([\\frac{n}{2}+1, \\frac{n}{2}+2, ... n]\\).\nIn either case, the size of the subproblem is less than n; thus, by the induction hypothesis, we can say that the algorithm returns the correct solution for the subproblem.\nThis proves the lemma that the algorithm returns the correct result for the list of size n. \nAnalysis of the Running Time\nAt every iteration we are discarding half of the currently present elements from our search space. Thus, the recurrance relation for the algorithm can be given as\n\\(T(n) = T(\\frac{n}{2}) + c\\)\nOn solving the recurrance relation, we get, \\(T(n) = O(log(n))\\).\n\n\n\nQuestion 2.\n\nSolution Description \nWhile searching for a \\(k^{th}\\) smallest element, we are sure that the element we are searching for is among A[i] and B[j] such that i + j = k. Thus initially, we will keep a pointer to the \\(\\frac{k}{2}^{th}\\) element of both the arrays, and then we will compare the values of the pointed elements, based on which, at every level of recursion, we will discard half of either array from our scope of the solution.\nalgorithm getElement(A, B, k){\n    if A.size() &gt; B.size() then swap A and B\n    if A.size() = 0 and B.size() &gt; 0, then return B[k-1]\n    if K = 1 then return min(A[0], B[0]) \n    i = min(A.size(), k//2)\n    j = min(B.size(), k//2) \n    if a[i-1] &lt; B[j-1] then return getElement(A[i+1:], B, K-i)\n    else return getElement(A, B[j+1:], K-j)\n}\nLet a and b be the size of the input lists.\nLemma: The algorithm returns the correct result for the problem of size m+n where m and n are the sizes of the input lists.\nBase Case : a+b = 1\nSince there is only one element in the list and the other list is empty, we can at max return the 1-th smallest element. Thus our algorithm returns the correct result for an input list of size a+b = 1.\nInductive hypothesis : As an inductive hypothesis, let us assume that our algorithm is correct for an input list of size a+b &lt; m+n.\nInductive Step : a+b = m+n\nWe maintain two pointers i and j, for the lists A and B, such that i+j = k. At each instance, our algorithm checks if we have reached the k-th smallest element. If so, it returns us the result and halts it. \nHowever, if it’s not the case, then it compares the value of \\(A[i-1]\\) and \\(B[j-1]\\), and based on it, it discards either \\(A[:i]\\) or \\(B[:j]\\) elements from our solution space.\nIn either case, the size of the subproblem is less than m+n, that is a+b&lt;m+n; thus, by the induction hypothesis, we can say that the algorithm returns the correct solution for the subproblem.\nThis proves the lemma that the algorithm returns the correct result for the problem of size m+n where m and n are the sizes of the input lists.\nAnalysis of the Running Time\nAt any instance, we are discarding half of the elements from our search space from either list A or list B. Thus, the running time of the algorithm will come up to be as \\(O(log(m)+log(n))\\)\n\n\n\nQuestion 3.\nSolution Description\nFor this problem, we would use the partition scheme of the quick select algorithm. In order to report the \\(K\\) nearest points to the origin, we will search for the \\(K^{th}\\) smallest point from the array. On finding the point, we can claim that all the points that are on the left of the Kth smallest point will surely be less than \\(K\\).\nalgorithm getPartition(P, L, H){\n    place P[L] at index i such that for all points t of P[:i],    \n        dist(t) &lt;= dist(P[i]) and for all points h of P[i+1:], \n        dist(h) &gt; dist(P[i])\n    return i\n}\n\nalgorithm getPoints(points, low, high, K){\n    if low &lt;= high, then return points[:K]\n    p = getPartition(points, low, high)\n    if p = K then return points[:K]\n    if p &lt; K then return getPoints(points, p+1, high, K)\n    else return getPoints(points, low, p-1, K)\n}\nAlthough the solution given above is correct and the time complexity at best case in \\(O(n)\\), but in the worst case it might go till \\(O(n^2)\\).\nThus, to ensure \\(O(n)\\) time complexity throughout, we can use a different strategy, which is based on median-of-medians.\nalgorithm getPartition(P, T){\n    place P[T] at index i such that for all points t of P[:i], \n        dist(t) &lt;= dist(P[i]) and for all points h of P[i+1:], \n        dist(h) &gt; dist(P[i])\n    return i\n}\n\nalgorithm getPoints(P, L, H, K){\n    divide P into groups of 5\n    sort the groups internally\n\n    middle = []\n    for all a = middle elements of each group, do middle.append(a)\n\n    if middle.size() is 1, then M = middle[0] \n    otherwise, M = getPoints(middle, 0, middle.size(), middle.size()/2)\n\n    pivot = getPartition(P, M)\n\n    if pivot - L = K - 1, then return P[:pivot]\n    if pivot - L &gt; k - 1, then return getPoints(P, L, pivot - 1, K)\n    otherwise, getPoints(P, pivot + 1, H, K - pivot + L - 1)\n}\nLet k be the size of the input list.\nLemma: The algorithm returns the correct result for the list of size n.\nBase Case : k = 1 Since there is only one element in the list, our algorithm can at max can return the 1-st closest point to the origin, which in this case is trivial. Thus our algorithm returns the correct result for an input list of size k = 1.\nInductive hypothesis : As an inductive hypothesis, let us assume that our algorithm is correct for an input list of size k &lt; n.\nInductive Step : k = n \nAt each instance, our algorithm checks if the middle value of the list is equal to K (since we are searching for K-th nearest point to the origin), \nIf the value of the middle element = K-1, then it returns P[: K-1] points where P is the list of the points. Otherwise, the algorithm compares the value of the middle element with K, and based on it, it either discards P[middle :] Or P[: middle] elements from our solution space. \nIn either case, the size of the subproblem is less than n; thus, by the induction hypothesis, we can say that the algorithm returns the correct solution for the subproblem.\nThis proves the lemma that the algorithm returns the correct result for the list of size n.\nAnalysis of the Running Time\nThe running time of the algorithm is dominated by the median-of-medians.\n\n\n\n\n\nQuestion 4."
  },
  {
    "objectID": "blogs/IITGN-Algorithms-Class-Notes/index.html#part-3",
    "href": "blogs/IITGN-Algorithms-Class-Notes/index.html#part-3",
    "title": "IIT Gandhinagar Algorithms Class Notes",
    "section": "Part 3",
    "text": "Part 3\n\nQuestion 1.\n\nSoultion Description\nFor a given array A[1..N] as input, we would maintain a buffer array B[1..N] where entry B[x] represents the maximum sum of the contiguous subsequence with A[x] as the last element of the subsequence.\nB is a buffer array of size N\nalgorithm getMaxSubsequence(A[1..N]){\n    B[1] = A[1]\n    for i = 2 to N, do,\n        B[i] = max(B[i-1] + A[i], A[i])\n    index = index of the max entry of B\n    if B[index] &lt; 0:\n        return []\n    p = index\n    while p &gt;= 1 and B[p] &gt;= 0, do p = p - 1\n    return A[p+1..index]\n}\nThe entry B[x] represents the maximum sum of the contiguous subsequence with A[x] as the last element of the subsequence.\nLemma - B[N] calculated by our algorithm is correct\nBase Case : Entry B[1] is correct\nThe algorithm trivially returns the correct result for B[1]. This is because the max sum we can get as A[1] as the final element of the subarray is A[1].\nInductive Hypothesis : Let us assume that B[x] calculated by our algorithm is correct.\nTo prove : B[x+1] calculated by our algorithm is correct.\nSince the entry B[x+1] depends upon B[x], and from our assumption, B[x] is correct, then the algorithm returns the correct entry for B[x+1].\nThe lemma, B[N] calculated by our algorithm is correct is proved.\n\nAnalysis of the running time\nThe running time of the algorithm is dominated by the loop which take \\(O(N)\\) time fo fill all the entriess in the buffer array. Thus the running time of the algorithm is \\(O(N).\\)\n\n\n\nQuestion 2.\n\nSolution Description\nFor finding the length of the longest palindromic subsequence, we will first reverse the input string \\(X\\), let it be \\(X'\\), and search for the longest common subsequence between \\(X\\) and \\(X'\\).\nNow, for searching the longest common subsequence between the two stings, we would maintain a buffer array B[1..N, 1..N], where each entry in, say B[p, q] denotes the length of the longest common subsequence between the substrings \\(X[1..p]\\) and \\(X'[1..q]\\).\nB[0..N, 0..N] is a buffer array of size N+1 by N+1\nalgorithm getMaxPalindrome(X[1..N]){\n    X' = reverse(X)\n    for i = 0 to N, do, B[i, 0] = B[0, i] = 0\n    for i = 1 to N, do\n        for j = 1 to N, do{\n            if X[i] = X'[j], set B[i, j] = 1 + B[i-1, j-1]\n            else, B[i, j] = max(B[i-1, j], B[i, j-1])\n        }\n    return B[N, N]\n}\nB[N, N] denotes the length of the longest common subsequence between X and X’ which is the length of the longest palindromic subsequence in X.\nLemma - B[N, N] calculated by our algorithm is correct\nBase Case : B[1, 1] is correct.\nThe base case is trivially true. B[1, 1] = 1, if X[1] = X’[1], otherwise, B[1, 1] = 0.\nInductive Hypothesis : Let us assume the algorithm returns the correct result for all the entries B[\\(i\\leq x, j&lt;y\\)] and B[\\(i&lt; I , j\\le y\\)].\nTo Prove : B[x, y] calculated by the algorithm is correct.\nSince, B[x, y] depends on the entries B[x-1, y-1], B[x-1, y], and B[x, y-1], which are correct as per our assumption; thus the algorithm calculates the correct result for the entry B[x, y].\nThus the lemma that B[N, N] calculated by the algorithm is correct is proved.\n\nAnalysis of the running time\nThe running time of the algorithm is dominated by the nested loops. Thus the running time of the algorithm is \\(O(N^2)\\).\n\n\n\nQuestion 3.\n\nSolution Description\nFor solving the problem, we will maintain a buffer array of size M by N, where the entry B[i, j] denotes the longest possible common substring with x[i] and y[j] as the last characters of the common substring.\nB is a buffer array of size M by N\nM = length of string x\nN = length of string y\n\nalgorithm getMaxLength(x, y){\n    maxLength = 0\n    for i = 1 to M do\n        for j = 1 to N do {\n            if x[i] = y[j], set B[i, j] = B[i-1, j-1] + 1\n            else, B[i, j] = 0\n\n            maxLength = max(maxLength, B[i, j])\n        }\n    return maxLength\n}\nLemma - B[p, q] calculated by the algorithm is correct\nBase Case : Entry B[1, 1] is correct\nThe algorithm trivially returns the correct result for B[1, 1]. This is because, B[1, 1] will either be 0 if x[1] \\(\\neq\\) y[1], and 1 if x[1] \\(=\\) y[1].\nInductive Hypothesis : Let us assume that the algorithm returns the correct solution for all B[1\\(\\leq\\)(p-1), 1\\(\\leq\\)(q-1)].\nTo prove : The algorithm returns correct result for the entry B[p, q]\nSince the entry B[p, q] depends upon B[p-1, q-1], and from our assumption, B[p-1, q-1] is correct, then the algorithm returns the correct entry for B[p, q].\nThus, for all P in [1, M] and Q in [1, N], B[P, Q] is correct. The length of the longest possible substring is the max(B[1..M, 1..N]).\n\nAnalysis of the running time\nThe running time of the algorithm is dominated by the time required to fill up the entries in the buffer array B, which is of size M by N. Thus the running time of the algorithm is \\(O(MN)\\).\n\n\n\nQuestion 4.\n\nSolution Description\nGiven a bitmap A[1..M, 1..N] as input, we would create a buffer map B[1..M, 1..N] where entry B[a, b] represents a maximum area of a solid square block with B[a, b] as the bottom-right corner of the block.\nB is a buffer array of size M by N\n\nalgorithm getMaxBlock(A[1..M, 1..N]){\n   for i = 1 to M, do B[i, 1] = A[i, 1]\n   for i = 1 to N, do B[1, i] = A[1, i]\n\n   for i = 2 to M, do\n      for j = 2 to N, do{\n         if A[i, j] is 0, then set B[i, j] = 0\n         else, set B[i, j] = min(B[i-1, j], B[i, j-1], B[i-1, j-1]) + 1\n      }\n\n   maxArea = 0\n   for i = 1 to M, do\n      for j = 1 to N, do\n          maxArea = max(B[i,j], maxArea)\n\n   return maxArea*maxArea\n}\nAs entry B[i, j] represents the max solid block, with A[i, j] as the bottom right corner cell of the block.\nLemma - The algorithm return a correct result\nBase Case : Entry B[1, 1] is correct\nThe algorithm trivially returns the correct solution. For a bitmap of size M by N, and with A[1, 1] as the bottom right corner cell, there are only two possibilities. That is B[1, 1] = 1, if A[1, 1] = 1, or B[1, 1] = 0 if A[1, 1] = 0.\nInductive Hypothesis : Let us assume the algorithm returns the correct result for all the entries B[\\(i\\leq x, j&lt;y\\)] and B[\\(i&lt;x, j\\le y\\)].\nTo Prove : The algorithm returns the correct solution for B[x, y].\nSince, the max solid block possible with A[x, y] as the bottom right cell depends on the entries B[x-1, y-1], B[x-1, y], and B[x, y-1], which is correct as per our assumption; thus the algorithm returns the correct result for the entry B[x, y].\nThus, for all P in [1, M] and Q in [1, N], B[P, Q] is correct.\nThe max possible solid block is the max(B[1..M, 1..N]), and thus, the algorithm returns a correct result.\n\nAnalysis of the running time\nThe running time of the algorithm is dominated by the time required to fill up the entries in the buffer array B, which is of size M by N. Thus the running time of the algorithm is \\(O(MN)\\). Assuming \\(M \\thickapprox N\\), the running time is \\(O(N^2)\\)."
  },
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Blogs",
    "section": "",
    "text": "Byte by Byte Vision\n\n\n\n\n\nWelcome to Byte-by-Byte Vision, your go-to platform for high-quality, curated resources in deep learning and computer vision. Learn at your own pace with structured paths, whether you’re a beginner or refining your expertise. Dive in and explore the transformative world of CV!\n\n\n\n\n\n\nJul 1, 2024\n\n\nSoumyaratna Debnath\n\n\n\n\n\n\n  \n\n\n\n\nIIT Gandhinagar Topic on Games Class Notes\n\n\n\n\n\nThis is a collection of notes, solutions, and explanations for the problems discussed in the class for the course Special Topic on Games at IIT Gandhinagar that includes Combinatorial Game Theory, Impartial Games, and other related topics.\n\n\n\n\n\n\nMay 1, 2024\n\n\nSoumyaratna Debnath\n\n\n\n\n\n\n  \n\n\n\n\nIIT Gandhinagar Algorithms Class Notes\n\n\n\n\n\nThis is a collection of notes, solutions, and explanations for the problems discussed in the class for the course Algorithms at IIT Gandhinagar which is a Postgraduate course designed to provide a comprehensive introduction to the design and analysis of algorithms.\n\n\n\n\n\n\nJan 1, 2023\n\n\nSoumyaratna Debnath\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "experience.html",
    "href": "experience.html",
    "title": "Experience",
    "section": "",
    "text": "Researcher at CVIG Lab, IIT Gandhinagar\n  March 2024 to Present\n  \n  Indian Institute of Technology Gandhinagar\n  \n   I am currently a researcher in the Computer Vision, Imaging, and Graphics (CVIG) Lab at IIT Gandhinagar, working under the guidance of Prof. Shanmuganathan Raman. Here, I delve into cutting-edge projects in computer vision, pushing the boundaries of this exciting field.\n  \n\n\n\n  Software Developer at IMS, IIT Gandhinagar\n  August 2022 to February 2024\n  \n  Indian Institute of Technology Gandhinagar\n  \n  I served as a Project Fellow at IIT Gandhinagar for Nineteen months, where I played a key role in the development and maintenance of the Institute Management System (IMS), an ERP solution designed to streamline operations across various departments and facilities. My work involved using Angular for dynamic, responsive front-end development, and ASP.NET Core, C#, and MS-SQL for robust, scalable front-end and back-end solutions.\n  \n\n\n\n\nResearch Intern \nSept 2020 to Feb 2022\n  \n  Cooch Behar Government Engineering College \n\nLetter of Recommendation\n\n\n\nOver the course of Eighteen months, I dedicated myself to the development and publication of three significant research papers, as well as the successful acquisition of two copyright registrations. These notable achievements were realized in the advanced fields of computer vision, image processing, and soft computing. My research included meta-heuristics-based optimization for multi-level color image segmentation, as well as exploring image processing for theme modulation and image-based steganography."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "projects/000 The Crafted Nest/index.html",
    "href": "projects/000 The Crafted Nest/index.html",
    "title": "The Crafted Nest : AR Catalogue",
    "section": "",
    "text": "GitHub"
  },
  {
    "objectID": "projects/000 The Crafted Nest/index.html#andriod-application",
    "href": "projects/000 The Crafted Nest/index.html#andriod-application",
    "title": "The Crafted Nest : AR Catalogue",
    "section": "Andriod Application",
    "text": "Andriod Application\nDownload the  Andoid Application"
  },
  {
    "objectID": "projects/000 The Crafted Nest/index.html#catalogue",
    "href": "projects/000 The Crafted Nest/index.html#catalogue",
    "title": "The Crafted Nest : AR Catalogue",
    "section": "Catalogue",
    "text": "Catalogue"
  },
  {
    "objectID": "projects/000 The Crafted Nest/index.html#gallery",
    "href": "projects/000 The Crafted Nest/index.html#gallery",
    "title": "The Crafted Nest : AR Catalogue",
    "section": "Gallery",
    "text": "Gallery"
  },
  {
    "objectID": "projects/000 The Crafted Nest/index.html#description",
    "href": "projects/000 The Crafted Nest/index.html#description",
    "title": "The Crafted Nest : AR Catalogue",
    "section": "Description",
    "text": "Description\n\nIntroduction\nIn the ever-changing world of interior design and home decor, “The Crafted Nest” is a leader in innovation and creativity. To improve customer experience and engagement, I started an exciting project to create an Augmented Reality (AR)-based product catalog. This modern solution combines the physical and digital worlds, allowing customers to explore and see our beautiful furniture and decor items directly over the catalog.\n\n\nTechnologies Used\nTo make this vision a reality, I used Unity and Vuforia, two powerful tools in the AR and game development field. Unity, known for its flexibility and strong capabilities, was the foundation for developing our interactive catalog. Vuforia, a top AR platform, was seamlessly integrated to add augmented reality features, letting users interact with our products in real-time.\n\n\nFeatures and Usages\nCustomers can see and interact with 3D models of our furniture pieces directly over the physical catalog. This feature gives a detailed and realistic view of our products, helping customers understand the design, size, and look without needing to imagine them in their actual space. Users can hover their mobile devices over specific catalog pages to bring the products to life in 3D. This feature turns the traditional catalog browsing experience into an engaging and interactive journey."
  },
  {
    "objectID": "projects/000 The Crafted Nest/index.html#demo-video",
    "href": "projects/000 The Crafted Nest/index.html#demo-video",
    "title": "The Crafted Nest : AR Catalogue",
    "section": "Demo Video",
    "text": "Demo Video"
  },
  {
    "objectID": "projects/002 The Rail Guardians/index.html",
    "href": "projects/002 The Rail Guardians/index.html",
    "title": "The Rail Guardians",
    "section": "",
    "text": "GitHub      Play the Game \n\nIntroduction\nA vertex cover [1] is a set of vertices within a graph such that every edge in the graph has at least one endpoint in the vertex cover set. Mathematically, for a graph G(V, E), as set S ⊆ V(G) is said to be a vertex cover of G if for any (u, v) ∈ E(G) either u ∈ S or v ∈ S. The size of the smallest vertex cover of graph G is called the minimum vertex cover of G, denoted by mvc(G). Figure 1 shows a simple cycle graph G’ of 4 vertices with mvc(G’) = 2.\n\nFigure 1 – A cycle graph G’ with 4 vertices; mvc (G’) = 2.\nThe dynamic variant of the vertex cover problem involves a game scenario where guards are positioned on certain vertices of a graph by the defender [2]. The attacker plays by selecting and attacking an edge, challenging the defender to respond by moving the guards along the graph’s edges. The defender’s goal is to ensure that at least one guard moves along the attacked edge in response to each attack. If the defender is unable to make such a move,\nthe attacker wins. Conversely, if the defender can successfully defend against an infinite sequence of attacks, the defender wins. Figure 2 describes the possible scenarios of the gameplay. If both ends of the attacked edge are unguarded then the attacker wins. If only one end of the attacked edge is guarded, then that guard must move across the attacked edge to counter the attack. If both ends of the attacked edge are guarded, then the two guards swap their positions.\n\nFigure 2 - Possible scenarios of the gameplay. Red vertices are guarded, and greens are unguarded.\nThe minimum number of guards required for the defender to maintain a winning strategy is called the eternal vertex cover number of the graph, denoted as evc(G).\nThe walkthrough of the eternal vertex for a cyclic graph with 4 vertex is given in Figure 3.\n\nFigure 3 - Walkthrough of Eternal vertex cover. Red vertices are guarded, and greens are unguarded.\n\n\nBackground\nThe problem of Eternal Vertex Cover was introduced by Klostermeyer and Mynhardt in 2009 [2]. Klostermeyer et al. concluded that mvc(G) ≤ evc(G) ≤ 2mvc(G). Following that many works have studied EVC in greater detail. Calamoneri et al. [3] studies the Eternal Vertex Cover problem on infinite and finite grid graphs, offering insights into the transition between these two graph types. Babu, et al. [4] investigates graphs with a minimal Eternal Vertex Cover number, focusing on those with unique properties that allow for a smaller number of guards. Fomin et al. explored the complexity of the problem and concluded that the problem is NP-Hard, yet has a polynomial time 2-approximation algorithm [5]. Babu et al. also explored the graphs whose eternal vertex cover number and vertex cover number coincide [4]. Babu et al. also concluded that the problem does not admit a polynomial compression even on bipartite graphs of diameter six [6].\n\n\nThe Rail Guardians\nThe Rail Guardians is my interpretation of the Eternal Vertex Cover Problem, reimagined as a captivating two-player web-based game with an interesting storyline and across three different variants (modes).\nThe Storyline – The game is set in an imaginary world and the storyline goes as follows.\nYou’re in charge of keeping the railway tracks safe in a world filled with protests and demands for change. As the head of railway operations, your mission is to prevent accidents by making sure every piece of track is watched over by your team of dedicated track builders. Protesters might target and destroy unwatched sections of the railway to make their voices heard. Your job is to strategically place your builders so that no track is left unguarded. Ready to take on the challenge? Keep your eyes sharp and your mind sharper to keep the railways and your passengers safe.\nMode 1 – Rail Guardians reporting Sir!\nThis mode is the implementation of generic eternal vertex cover [2]. The game starts with the defender placing the builders on the graph. The attacker then tries to attack the tracks. With\nN builders on boards, in defender’s next move, defender can update the position of the builders (excluding the one builder who had to walk across the attacked track). Each builder\ncan at most move across one track. The attacker wins if an unguarded track is attacked. Defender wins if all tracks are guarded till the selected number of rounds.\n\nFigure 4 - Mode 1 home.\n\nFigure 5 - Mode 1 start menu.\n\nFigure 6 - Mode 1 gameplay.\nMode 2 – The Challenge has Escalated!\nStoryline - Protestors have managed to steal the railway track map, making it impossible for you to monitor the tracks through your usual systems. Now, the only way to keep track of the network is through reports from your field builders. In this level, you’ll need to rely on your memory and strategic planning skills. Your builders will relay information about sections of the track as they survey them, but once communication ends, you won’t have a visual map to refer back to. You must memorize and visualize the track layout based on their reports. Can you keep the railways safe using only the details you hold in your mind? Let’s find out in this thrilling new challenge!\nIn this mode, the game starts with the defender placing the builders on the graph. But there is a catch! The graph is not visible to the defender altogether. Rather only a part of the graph is visible to the defender at an instance. Hovering over each station gives the defender intel of the tracks connected to that station. Once the guards are placed, the gameplay continues as the previous mode.\n\nFigure 7 - Mode 2 home\n\nFigure 8 - Mode 2 start menu.\n\nFigure 9 - Mode 2 gameplay: hover action.\n\nFigure 10 - Mode 2 gameplay: attacker’s move.\nMode 3 – The Final Attempt!\nStoryline - As the day winds down, both your track builders and the protestors are feeling the wear of their efforts. Tonight, the energy levels of your builders are limited, and they can only shift their monitoring positions for limited times. Similarly, the protestors have just enough energy as the builders for final attempts to disrupt the railway tracks. Your task is to strategically redeploy your builders to the most critical sections of the track for these last few hours. Use your builders’ remaining energy efficiently and outsmart the protestors’ final moves. Remember, this is a crucial moment; if you can keep the tracks safe tonight, there’s a good chance the government will meet the protestors’ demands by tomorrow, and normal operations can resume.\nIn this mode, the game starts with the defender placing the builders on the graph. But now, each builder has a certain amount of energy associated with it. Whenever a builder is moved by the defender, the energy of the builder decreases by 1. When the defender passes the turn to the attacker without making a move, the builder with the max energy loses 1 energy. The game ends when the energy of any builder reaches 0, or when the attacker attacks an unguarded track. The number of rounds the defender has guarded the track is the score of the defender. The aim is to get the highest score!\n\nFigure 11 - Mode 3 home.\n\nFigure 12 - Mode 3 start menu.\n\nFigure 13 - Mode 3 gameplay.\nDark Mode – The game features a dark mode to accommodate users who prefer a darker theme or play in low-light environments, reducing eye strain and providing a visually comfortable experience.\n\nFigure 14 - Dark mode: home.\n\nFigure 15 - Dark mode: start menu.\n\nFigure 16 - Dark mode: gameplay.\nImplementation – The implementation uses only HTML, CSS, and vanilla JavaScript without any third-party libraries. This minimalistic approach ensures a lightweight experience.\n\n\nAnalysis of Max Score for Energy Aware EVC on Linear Paths\nIn Energy Aware Eternal Vertex Cover (Mode 3 of the game), each guard has a limited amount\nof energy, and the game ends if any guard’s energy reaches zero. Analyzing the maximum achievable score in this mode can help by revealing optimal guard movement strategies and energy management. By understanding the maximum score, players can develop tactics\nthat balance defense and energy conservation, thereby extending gameplay and achieving better results. This analysis can guide game design improvements, leading to a more engaging and challenging experience for players.\nI have performed some basic analysis of the maximum score achievable by the defender where each guard has energy K, and K is numerically big!\nTable 1 summarizes my analysis across paths of size 2 to 5, and different number of builders.\nTable 1 - Analysis of Max Score for Energy Aware Eternal Vertex Cover on Linear Graphs / Paths\n\n\n\nStations\nBuilders\nEnergy\nMax Score\n\n\n\n\n2\n1\nK\nK\n\n\n3\n1\nK\n1\n\n\n3\n2\nK\n2K – 1\n\n\n4\n1\nK\n0\n\n\n4\n2\nK\n2\n\n\n4\n3\nK\n3K – 2\n\n\n5\n1\nK\n0\n\n\n5\n2\nK\n1\n\n\n5\n3\nK\n3\n\n\n5\n4\nK\n4K – 3\n\n\n\nAnalysis of optimum placement of builders to achieve the maximum score\n\n\n\n\n\n\n\n\n\n\n\n\nConclusion\nIn conclusion, my final course project centered on implementing the Eternal Vertex Cover as a two-player UI based game through a process of planning, coding, and debugging. Alongside the core implementation, I also integrated additional modes and features to make the game engaging and enhance the gameplay. The project was a great experience for me, and the lessons I learned while building it helped me understand the intricacies of game development, problem-solving, and user interface design.\nI am deeply encouraged to work more on the problem statement. Also, I look forward to use meta-heuristics and genetic algorithms to compute the maximum achievable score in Energy Aware Eternal Vertex Cover.\n\n\nAcknowledgements\nI express my gratitude to Prof. Neeldhara Misra, Prof. Jyothi Krishnan, and Saraswati Girish Nanoti for their constant guidance and encouragement. I thank Sakshi for helping me with the beautiful storyline.\n\n\nReferences\n\n“Vertex cover,” Wikipedia. Mar. 01, 2024. Accessed: Apr. 26, 2024. [Online]. Available: https://en.wikipedia.org/w/index.php?title=Vertex_cover&oldid=1211245782\nW. F. Klostermeyer and C. M. Mynhardt, “Edge protection in graphs”.\nT. Calamoneri and F. Corò, “(Eternal) Vertex Cover Number of Infinite and Finite Grid Graphs.” arXiv, Sep. 12, 2022. doi: 10.48550/arXiv.2209.05102.\nJ. Babu, L. S. Chandran, M. Francis, V. Prabhakaran, D. Rajendraprasad, and J. N. Warrier, “On Graphs with Minimal Eternal Vertex Cover Number,” in Algorithms and Discrete Applied Mathematics, S. P. Pal and A. Vijayakumar, Eds., Cham: Springer International Publishing, 2019, pp. 263–273. doi: 10.1007/978-3-030-11509-8_22.\nF. V. Fomin, S. Gaspers, P. A. Golovach, D. Kratsch, and S. Saurabh, “Parameterized algorithm for eternal vertex cover,” Inf. Process. Lett., vol. 110, no. 16, pp. 702–706, Jul. 2010, doi: 10.1016/j.ipl.2010.05.029.\nJ. Babu, N. Misra, and S. G. Nanoti, “Eternal Vertex Cover on Bipartite Graphs,” in Computer Science – Theory and Applications, A. S. Kulikov and S. Raskhodnikova, Eds., Cham: Springer International Publishing, 2022, pp. 64–76. doi: 10.1007/978-3-031- 09574-0_5."
  },
  {
    "objectID": "projects/004 Picrypt/index.html",
    "href": "projects/004 Picrypt/index.html",
    "title": "Picrypt-It",
    "section": "",
    "text": "GitHub      Copyright \n\n\nAbstract\n\nInspired by the principle “why send less, when you can send more,” Picrypt is a user-friendly software featuring a clean and simplistic interface, designed for inscribing digital color images with passcode-protected messages. This software allows users to download and locally save these inscribed images, which can then be sent digitally across the globe while preserving and protecting the embedded data message. The passcode used to inscribe the messages serves as the key for successful decryption, ensuring that the messages remain secure and accessible only to intended recipients.\n\n\n\nIntroduction\n\nIn today’s digital age, billions of images are shared among people every day. Often, these images require accompanying descriptions, especially in professional contexts such as journalism, where detailed reports must be submitted alongside photographs. Traditional methods involve sending text separately, which can be cumbersome and insecure. Picrypt addresses this challenge by enabling users to embed descriptive text directly into images, protected by a passcode.\n\n\n\nFeatures\n\nUser-Friendly Interface\n\nPicrypt boasts a clean and simplistic user interface, making it accessible and easy to use for individuals of all technical backgrounds. The software’s design emphasizes simplicity and efficiency, allowing users to inscribe images with messages using just a few clicks.\n\n\n\nPasscode Protection\n\nTo ensure that the embedded messages remain secure, Picrypt employs passcode protection. The passcode acts as the key to decrypt the inscribed messages, guaranteeing that only intended recipients can access the data.\n\n\n\nLocal Storage and Sharing\n\nOnce an image is inscribed with a message, it can be downloaded and saved locally. Users can then share the image digitally across the globe, with the confidence that the embedded message remains protected and intact throughout the transmission.\n\n\n\n\nTechnical Implementation\n\nData Encryption\n\nPicrypt uses advanced encryption techniques to secure the data embedded within images. The data is encrypted using Vernam Cipher and Playfair Cipher before being inscribed into the image. This ensures that even if the image is intercepted, the message remains unreadable without the correct passcode.\n\n\n\nPixel-Based Inscription\n\nThe software leverages the pixel intensity values of a color image to inscribe encrypted data into the image on a pixel-by-pixel basis. This method preserves the original components, details, quality, and aspect ratio of the image, ensuring that the visual integrity of the image is maintained.\n\n\n\nParity Check\n\nTo maintain the consistency and integrity of the inscribed data, Picrypt implements an odd/even parity check. This additional layer of validation helps to ensure that the data remains accurate and consistent throughout the process.\n\n\n\n\nUse Cases\n\nJournalism\n\nJournalists can use Picrypt to embed detailed descriptions of situations directly into photographs. This simplifies the reporting process and ensures that critical information is securely transmitted alongside the images.\n\n\n\nSecure Communication\n\nPicrypt can be used for secure communication in various contexts where sensitive information needs to be shared alongside images. By embedding the message directly into the image and protecting it with a passcode, users can ensure that their communication remains confidential.\n\n\n\n\nConclusion\n\nPicrypt represents a significant advancement in the way we share images and information. By combining user-friendly design with robust security features, Picrypt offers a powerful tool for embedding and protecting messages within digital images. Whether for professional use in journalism or secure personal communication, Picrypt provides a reliable and efficient solution for inscribing and sharing information.\n\n\n\nFuture Work\n\nFuture enhancements to Picrypt could include support for different image formats, improved encryption algorithms, and the ability to embed larger amounts of data. Additionally, expanding the software’s compatibility with various operating systems and devices will further enhance its usability and accessibility."
  },
  {
    "objectID": "projects/006 Underwater Object Detection/index.html",
    "href": "projects/006 Underwater Object Detection/index.html",
    "title": "Underwater Object Detection",
    "section": "",
    "text": "GitHub \n\nAbstract\n\nWith the goal of recovering high-quality image content from its degraded version, image restoration finds numerous applications in fields such as surveillance, computational photography, medical imaging, and remote sensing. Recently, convolutional neural networks (CNNs) have significantly improved the image restoration process. Conventional CNN-based methods typically operate either on full-resolution or progressively low-resolution representations. While the former achieves spatial precision but lacks contextual robustness, the latter provides semantically reliable but spatially less accurate outputs. MIRNet is a novel architecture designed to maintain spatially precise high-resolution representations throughout the network while extracting strong contextual information from low-resolution representations. The core approach involves a multi-scale residual block featuring parallel multi-resolution convolution streams, information exchange across these streams, spatial and channel attention mechanisms, and attention-based multi-scale feature aggregation. This approach learns enriched features that combine contextual information from multiple scales while preserving high-resolution spatial details. The source code and pre-trained models are available at the MIRNet repository: https://github.com/swz30/MIRNet.\n\n\n\nIntroduction\n\nImage restoration and enhancement are critical tasks in computer vision, aimed at recovering high-quality images from degraded inputs. Applications of these tasks span across various fields including surveillance, computational photography, medical imaging, and remote sensing. Traditional image restoration pipelines often utilize either full-resolution processing or encoder-decoder architectures. While full-resolution processing retains precise spatial details, encoder-decoder architectures offer better contextual representations. However, these methods fail to satisfy both requirements simultaneously, which is essential for real-world image restoration tasks.\n\n\n\nMethodology\n\nMIRNet Architecture\n\nMIRNet introduces a novel architecture that integrates the benefits of both full-resolution processing and multi-scale feature extraction. The main branch of MIRNet is dedicated to full-resolution processing, ensuring spatial precision. Complementary parallel branches provide enhanced contextual features. The architecture employs multi-scale residual blocks containing several key elements: parallel multi-resolution convolution streams for multi-scale feature extraction, information exchange across these streams, spatial and channel attention mechanisms to capture contextual information, and attention-based multi-scale feature aggregation.\n\n\n\nMulti-Scale Residual Block\n\nThe multi-scale residual block is the cornerstone of MIRNet’s architecture. It extracts features at multiple scales through parallel convolution streams, allowing the network to capture both fine details and broader contextual information. Information exchange across these streams ensures a comprehensive understanding of the image, while spatial and channel attention mechanisms highlight relevant features, facilitating effective feature aggregation and improved restoration quality.\n\n\n\n\nImplementation\n\nFull-Resolution and Multi-Scale Processing\n\nMIRNet processes the image in full resolution through its main branch, retaining spatial details. Simultaneously, parallel branches operate at multiple scales, capturing contextual information. The integration of features from these branches results in a comprehensive restoration approach that combines spatial precision with contextual robustness.\n\n\n\nAttention Mechanisms\n\nSpatial and channel attention mechanisms play a crucial role in MIRNet’s architecture. These mechanisms enable the network to focus on important regions and features within the image, enhancing the overall restoration process. By leveraging attention mechanisms, MIRNet can effectively combine multi-scale features, resulting in high-quality restored images.\n\n\n\n\nResults and Discussion\n\nMIRNet’s architecture demonstrates significant improvements in image restoration and enhancement tasks. By combining full-resolution processing with multi-scale feature extraction, the network achieves a balance between spatial precision and contextual robustness. The attention mechanisms further enhance the restoration quality, ensuring high-resolution details and enriched contextual information.\n\n\n\n\nYOLO Object Detection\n\nYOLO, an abbreviation for ‘You Only Look Once,’ is a cutting-edge algorithm designed for real-time object detection and recognition. By treating object detection as a regression problem, YOLO predicts class probabilities and bounding boxes with a single forward pass through a convolutional neural network (CNN). This approach enables YOLO to perform object detection efficiently and accurately, making it a preferred choice for various applications in computer vision.\n\n\n\nMethodology\n\nYOLO Architecture\n\nYOLO’s architecture processes the entire image in a single pass, dividing it into an (N N) grid. For each grid cell, YOLO predicts multiple bounding boxes and class probabilities. This fully convolutional approach contrasts with traditional region proposal networks, which require multiple predictions for various regions in the image. YOLO’s single-pass method significantly enhances detection speed and efficiency.\n\n\n\nRegression Approach\n\nBy reframing object detection as a regression problem, YOLO maps image pixels directly to bounding box coordinates and class probabilities. This global reasoning allows YOLO to capture contextual information about object classes and appearances, reducing background errors and improving detection accuracy. The network’s ability to reason globally about the image context enhances its performance compared to traditional methods.\n\n\n\n\nBenefits of YOLO\n\nSpeed and Efficiency\n\nYOLO’s primary advantage is its speed. By processing the image in a single pass, YOLO can perform real-time object detection, making it suitable for applications requiring fast response times. The simplified detection pipeline further enhances efficiency, allowing the algorithm to handle high-resolution images and complex scenes effectively.\n\n\n\nGlobal Reasoning\n\nYOLO’s global reasoning capability leads to more accurate predictions. Unlike sliding window and region proposal-based techniques, YOLO considers the entire image context during both training and testing. This comprehensive approach reduces background errors and improves object localization, enabling YOLO to differentiate objects from their surroundings more effectively.\n\n\n\nGeneralization\n\nYOLO excels in learning generalizable representations of objects. When trained on natural images, YOLO outperforms other top detection methods, such as DPM and R-CNN, in diverse domains, including artwork. This robust generalization capability ensures that YOLO performs well even when applied to new and unexpected inputs, making it a versatile tool for various object detection tasks.\n\n\n\n\nResults\n\nYOLO utilizes features from the entire image to predict each bounding box and class probability simultaneously. This design allows the network to maintain high average precision while achieving real-time detection speeds. The ability to train and optimize on full images directly enhances the overall detection performance.\n\n\n\n\nDataset Description\n\nFor training YOLO, the Brackish Underwater Dataset is utilized. This dataset is the first publicly available European underwater image dataset with bounding box annotations of fish, crabs, and other marine organisms. Recorded in Limfjorden, a brackish strait in northern Denmark, the dataset offers a unique set of underwater images captured using a camera setup consisting of three cameras and three LED lights mounted on a concrete pillar of the Limfjords bridge. Currently, only data from one camera has been annotated and published, with more expected to be added. The setup, located 9 meters below the surface, operates with a single LED light during recordings, influencing the behavior of marine animals, such as the schooling of sticklebacks directly in front of the camera.\n\n\n\nConclusion\n\nYOLO stands out as a groundbreaking object detection algorithm that combines speed, accuracy, and generalization. By treating object detection as a regression problem and leveraging a fully convolutional neural network, YOLO simplifies the detection process while enhancing performance. The use of the Brackish Underwater Dataset for training highlights YOLO’s applicability to diverse environments and its potential for various real-world applications. The algorithm’s ability to perform real-time detection with high precision makes it a valuable tool in the field of computer vision.\n\n\n\nFuture Work\n\nFuture improvements to YOLO could involve refining the network architecture to enhance detection accuracy and speed further. Exploring additional datasets and domains will also help validate and extend YOLO’s capabilities. Integrating more advanced techniques, such as attention mechanisms and multi-scale feature extraction, could further boost the algorithm’s performance in complex scenarios."
  },
  {
    "objectID": "projects/008 The Magic of Ensemble Algorithms/index.html",
    "href": "projects/008 The Magic of Ensemble Algorithms/index.html",
    "title": "The Magic of Ensemble Algorithms",
    "section": "",
    "text": "GitHub \n\nQuestion 1\nShow plots for bias and variance vs increasing complexity (depth) of decision tree on the given regression dataset. You can use the decision tree implementation from assignment 1 (or sklearn tree).\nimport numpy as np\n\nnp.random.seed(1234)\nx = np.linspace(0, 10, 50)\neps = np.random.normal(0, 5, 50)\ny = x**2 +1 + eps\n\n#for plotting\nimport matplotlib.pyplot as plt\nplt.plot(x, y, 'o')\nplt.plot(x, x**2 + 1, 'r-')\nplt.show()\n\nSolution\nThe toy data used for training and testing is as follows -\n\nThe training and testing curve with respect to mean squared error and increasing complexity is as follows -\nTrain-test split = 70:30\n\nThe bias-varience vs depth curve is as follows -\n\n\nReference\nEstimator. (2023, February 7). In Wikipedia. https://en.wikipedia.org/wiki/Estimator\n\n\n\n\nQuestion 2\nShuffle the dataset and split the classification dataset into a training set (70%) and a test set (30%). Implement a weighted decision tree and train it using the training set. Use uniform(0,1) distribution to assign weights randomly to the samples. Plot and visualise the decision tree boundary. Use the test set to evaluate the performance of the weighted decision tree and compare your implementation with sklearn. You can copy your implementation of decision tree from assignment 1 to this repository and edit it to take sample weights as an argument while learning the decision tree(Default weight is 1 for each sample).\nfrom sklearn.datasets import make_classification\nX, y = make_classification(\nn_features=2, n_redundant=0, n_informative=2, \nrandom_state=1, n_clusters_per_class=2, class_sep=0.5)\n\n# For plotting\nimport matplotlib.pyplot as plt\nplt.scatter(X[:, 0], X[:, 1], c=y)\n\nSoultuion\nThe data used for training and testing looks like -\n\n\nCriterion = information_gain\nMetrics for Weighted Decision Tree\nTraining metrics\nAccuracy:  1.0\n\nClass:  1\n    Precision:  1.0\n    Recall:  1.0\n\nClass:  0\n    Precision:  1.0\n    Recall:  1.0\n\nTesting metrics\nAccuracy:  0.7666666666666667\n\nClass:  1\n    Precision:  0.6875\n    Recall:  0.8461538461538461\n\nClass:  0\n    Precision:  0.8571428571428571\n    Recall:  0.7058823529411765\n\nMetrics for Sklearn Implementation\nTraining metrics\nAccuracy:  1.0\n\nClass:  1\n    Precision:  1.0\n    Recall:  1.0\n\nClass:  0\n    Precision:  1.0\n    Recall:  1.0\n\nTesting metrics\nAccuracy:  0.7333333333333333\n\nClass:  1\n    Precision:  0.6666666666666666\n    Recall:  0.7692307692307693\n\nClass:  0\n    Precision:  0.8\n    Recall:  0.7058823529411765\n\nCriterion = gini_index\nMetrics for Weighted Decision Tree\nTraining metrics\nAccuracy:  1.0\n\nClass:  1\n    Precision:  1.0\n    Recall:  1.0\n\nClass:  0\n    Precision:  1.0\n    Recall:  1.0\n\nTesting metrics\nAccuracy:  0.8\n\nClass:  1\n    Precision:  0.7333333333333333\n    Recall:  0.8461538461538461\n\nClass:  0\n    Precision:  0.8666666666666667\n    Recall:  0.7647058823529411\n\nMetrics for Sklearn Implementation\nTraining metrics\nAccuracy:  1.0\n\nClass:  1\n    Precision:  1.0\n    Recall:  1.0\n\nClass:  0\n    Precision:  1.0\n    Recall:  1.0\n\nTesting metrics\nAccuracy:  0.8\n\nClass:  1\n    Precision:  0.7333333333333333\n    Recall:  0.8461538461538461\n\nClass:  0\n    Precision:  0.8666666666666667\n    Recall:  0.7647058823529411\n\n\n\n\nQuestion 3\nPart A\nImplement Adaboost on Decision Stump (depth=1 tree). You can use Decision Tree learnt in assignment 1 or sklearn decision tree and solve it for the case of real input and discrete output. Edit ensemble/ADABoost.py \n\nSolution\nThe dataset used for tarining looks like -\nNumber of samples = 100 Number of features = 2 Number of classes = 2 Numbeer of estimators = 3 Criteria = gini\n\nThe performace of the model can be comprehended as -\nCriteria  : gini\nAccuracy  : 0.99\n\nClass     : 0\nPrecision : 1.0\nRecall    : 0.98\n\nClass     : 1\nPrecision : 0.9803921568627451\nRecall    : 1.0\n\nPart B\nImplement AdaBoostClassifier on classification data set. Plot the decision surfaces and compare the accuracy of AdaBoostClassifier using 3 estimators over decision stump. Include your code in q3_ADABoost.py.\n\n\nSolution\nThe dataset used for tarining looks like -\nNumber of samples = 100 Number of features = 2 Number of classes = 2 Numbeer of estimators = 3 Criteria = gini\n\nDecision surfaces of the indivisual estimators -\n\nThe combined decision surface looks like -\n\n\n\n\nQuestion 4\nPart A Implement Bagging(BaseModel, num_estimators): where base model is the DecisionTree you had implemented in assignment 1 (or sklearn decision tree). In a later assignment, you would have to implement the above over LinearRegression() also, but for now you only have to implement it for Decision Trees. Edit ensemble/bagging.py. Use q4_Bagging.py for testing.\n\nSoultion\nThe toy dataset which is used for training the model is as follows -\nNumber of samples = 50 Number of features = 2 Number of classes = 2 Number of estimators = 5 Max depth of each estimator = 20\n\nThe decision boundries of each estimator is as follows -\n\nThe combined decision boundary is as follows -\n\nThe performce of the model can be estimated from the following results\nCriteria   : gini\nAccuracy   : 0.82\n\nClass      : 0\nPrecision  : 0.8181818181818182\nRecall     : 0.782608695652174\n\nClass      : 1\nPrecision  : 0.8214285714285714\nRecall     : 0.8518518518518519\n\nPart B Implement bagging in parallel fashion, where each decision tree is learnt simultaneously. Perform timing analysis for parallel implementation and normal implementation.\n\n\nSolution\nStudy 1\nThe toy dataset which is used for training both the model is as follows -\nNumber of samples = 2000 Number of features = 2 Number of classes = 4 Number of estimators = 20 Max depth of each estimator = 20\n\nThe performace and time required for tarining of the models can be compared from the following metrics -\nCriteria            : gini\nAccuracy Sequencial : 0.5385\nAccuracy Parallel   : 0.536\n\nTime elapsed in sequencial execution : 0.1579 seconds\nTime elapsed in parallel execution   : 0.1095 seconds\nStudy 2\nThe toy dataset which is used for training both the model is as follows -\nNumber of samples = 500 Number of features = 2 Number of classes = 4 Number of estimators = 50 Max depth of each estimator = 100\n\nThe performace and time required for tarining of the models can be compared from the following metrics -\nCriteria            : gini\nAccuracy Sequencial : 0.683\nAccuracy Parallel   : 0.678\n\nTime elapsed in sequencial execution : 0.9955 seconds\nTime elapsed in parallel execution   : 0.5297 seconds\nStudy 3\nThe toy dataset which is used for training both the model is as follows -\nNumber of samples = 10000 Number of features = 2 Number of classes = 6 Number of estimators = 500 Max depth of each estimator = 1000\n\nThe performace and time required for tarining of the models can be compared from the following metrics -\nCriteria            : gini\nAccuracy Sequencial : 0.457\nAccuracy Parallel   : 0.4561\n\nTime elapsed in sequencial execution : 32.3758 seconds\nTime elapsed in parallel execution   : 27.2234 seconds\n\n\n\nQuestion 5\nPart A\nImplement RandomForestClassifier() and RandomForestRegressor() classes in tree/randomForest.py. Use q5_RandomForest.py for testing.\n\nSolution\nRandom forest classifier over entropy as criterion returns the following trees as output -\nNumber of samples = 30 Number of features = 5 Sampling = row sampling\nMax Depth = 5 Number of estimators = 3 Number of classes = 5\nThe performace of the model can be quantified as -\nCriteria  : entropy\nAccuracy  : 0.8333333333333334\n\nClass 0\nPrecision : 0.7777777777777778\nRecall    : 0.7777777777777778\n\nClass 3\nPrecision : 0.875\nRecall    : 1.0\n\nClass 2\nPrecision : 1.0\nRecall    : 0.7142857142857143\n\nClass 4\nPrecision : 0.6\nRecall    : 0.75\n\nClass 1\nPrecision : 1.0\nRecall    : 1.0\n\nRandom forest classifier over gini as criterion returns the following trees as output -\nNumber of samples = 30 Number of features = 5 Sampling = row sampling\nMax Depth = 5 Number of estimators = 3 Number of classes = 5\n\nThe performace of the model can be quantified as -\nCriteria  : gini\nAccuracy  : 0.8333333333333334\n\nClass 0\nPrecision : 0.875\nRecall    : 0.7777777777777778\n\nClass 3\nPrecision : 1.0\nRecall    : 0.7142857142857143\n\nClass 2\nPrecision : 0.875\nRecall    : 1.0\n\nClass 4\nPrecision : 1.0\nRecall    : 0.75\n\nClass 1\nPrecision : 0.5\nRecall    : 1.0\n\nRandom forest classifier over squared_error as criterion returns the following trees as output -\nNumber of samples = 30 Number of features = 5 Sampling = row sampling\nMax Depth = 5 Number of estimators = 3\n\nThe performace of the model can be quantified as -\nCriteria : squared_error\nRMSE     : 0.5159266120836465\nMAE      : 0.364302808727380\n\nRandom forest classifier over squared_error as criterion returns the following trees as output -\nNumber of samples = 30 Number of features = 3 Sampling = column sampling\nMax Depth = 5 Number of estimators = 3\n\nThe performace of the model can be quantified as -\nCriteria : squared_error\nRMSE     : 40.52918401932447\nMAE      : 24.29257288596476\n\nPart B\nGenerate the plots for classification data set. Include you code in random_forest_classification.py\n\n\nSolution\nThe toy dataset used for tarining is as follows -\n\nRandom forest classifier over gini as criterion returns the following trees as output -\nNumber of samples = 100 Number of features = 3 Sampling = column sampling\nMax Depth = 5 Number of estimators = 3 Number of classes = 2\n\nThe decision surfaces for the corrosponding estimators is as follows -\n\nThe combined decision surface is as follows -\n\nThe performace of the model can be quantified as -\nCriteria  : gini\nAccuracy  : 0.91\n\nClass 1\nPrecision : 1.0\nRecall    : 0.88\n\nClass 0\nPrecision : 1.0\nRecall    : 0.94\n\n\n\nQuestion 6\nImplement Gradient Boosted Decision trees on the regression dataset given below. You need to edit the ensemble/gradientBoosted.py and q6_gradientBoosted.py\nfrom sklearn.datasets import make_regression\n\nX, y= make_regression(\n    n_features=3,\n    n_informative=3,\n    noise=10,\n    tail_strength=10,\n    random_state=42,\n)\n\n# For plotting\nimport matplotlib.pyplot as plt\nplt.scatter(X[:, 0], y)\n\nSolution\nThe rabdomly generated data for the training looks like -\n\nWe trained an ensamble with gradient boosted decision tree upon the given data with the following parameters -\nCriteria = squared_error Number of estimators = 20 Learning rate = 0.1 Max depth = 3\nThe performace of the model can be comprehended with the following results -\n\nCriteria : squared_error\nRMSE     : 8.74626791232833\nMAE      : 7.075693381159919"
  },
  {
    "objectID": "projects/010 Image Stitching from Scratch/index.html",
    "href": "projects/010 Image Stitching from Scratch/index.html",
    "title": "Image Stitching from Scratch",
    "section": "",
    "text": "GitHub \n\nImporting the Libraries and defining the helper functions\n\n# importing the libraries\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\n\n# function to open and image and resize it\ndef open_resize(img, w, h):\n    image = cv2.imread(img) # read the image\n    image = cv2.resize(image, (w, h)) # resize the image\n    return image # return the image\n\n# function to display an image\ndef show_image(img, w=0, h=0, axis=False, title=''):\n    if w == 0 or h == 0: plt.figure()\n    else: plt.figure(figsize=(w, h)) # set the size of the figure\n    plt.title(title) # set the title of the figure\n    plt.imshow(img) # show the image\n    if axis == False: plt.axis('off') # turn off the axis\n    plt.show() # show the image    \n\n# function to convert BGR to RGB\ndef BGR2RGB(img):\n    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # convert the image from BGR to RGB\n\n#### TEST THE FUNCTIONS ####\ntest = open_resize('Dataset/scene1/I11.jpg', 600, 400)\nshow_image(BGR2RGB(test), axis=True)\n\n\n\n\n\n\nStep 1 Detect, extract, and match features\nUsing SIFT detector to detect the features, and Brute-Force matcher to match the features. The result is shown below.\n\n# function to get the keypoints and descriptors\ndef get_keypoints_descriptors(img):\n    sift = cv2.SIFT_create() # create a SIFT object\n    kp, des = sift.detectAndCompute(img, None) # get the keypoints and descriptors\n    des = des.astype(np.uint8)\n    return kp, des # return the keypoints and descriptors\n\n# function to draw the keypoints\ndef draw_keypoints(img, kp):\n    return cv2.drawKeypoints(img, kp, None) # draw the keypoints\n\n# functio to get the src and dst points\ndef get_src_dst_points(image1, image2):\n    gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY) # convert the image to grayscale\n    gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY) # convert the image to grayscale\n    keypoints1, descriptors1 = get_keypoints_descriptors(gray1) # get the keypoints and descriptors\n    keypoints2, descriptors2 = get_keypoints_descriptors(gray2) # get the keypoints and descriptors\n    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True) # create a BFMatcher object\n    matches = bf.match(descriptors1, descriptors2) # get the matches\n    matches = sorted(matches, key=lambda x: x.distance) # sort the matches\n    # Draw the matches between the 2 images\n    matching_result = cv2.drawMatches(image1, keypoints1, image2, keypoints2, matches, outImg=None)\n    # get the coordinates of the matched points\n    src_pts = np.float32([keypoints1[m.queryIdx].pt for m in matches]).reshape(-1, 2)\n    dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in matches]).reshape(-1, 2)\n    return src_pts, dst_pts, matching_result # return the src and dst points and the matching result    \n\n\n#### TEST THE FUNCTIONS ####\nimage1 = open_resize('Dataset/scene1/I12.jpg', 600, 400)\nimage2 = open_resize('Dataset/scene1/I13.jpg', 600, 400)\nsrc_pts, dst_pts, matching_result = get_src_dst_points(image1, image2)\nshow_image(BGR2RGB(matching_result), w=10, h=5)\n\n\n\n\n\n\nStep 2 Estimate homography matrix between two images using RANSAC.\nGiven the source points and destination points compute_homography() function computes the homography matrix. The best homography matrix is selected by RANSAC algorithm, implemented as ransac_homography() function. The result is shown below.\n\n# function to compute the homography\ndef compute_homography(src_pts, dst_pts): \n    num_points = src_pts.shape[0] # get the number of points\n    A_matrix = [] # matrix A for homography calculation\n    for i in range(num_points):\n        src_x, src_y = src_pts[i, 0], src_pts[i, 1]\n        dst_x, dst_y = dst_pts[i, 0], dst_pts[i, 1]\n        # Constructing the rows of the A matrix\n        A_matrix.append([src_x, src_y, 1, 0, 0, 0, -dst_x * src_x, -dst_x * src_y, -dst_x])\n        A_matrix.append([0, 0, 0, src_x, src_y, 1, -dst_y * src_x, -dst_y * src_y, -dst_y])\n    A_matrix = np.asarray(A_matrix) # convert the matrix to numpy array\n    U, S, Vh = np.linalg.svd(A_matrix) # perform SVD\n    L = Vh[-1, :] / Vh[-1, -1] # calculate the homography\n    H = L.reshape(3, 3) # reshape the homography\n    return H # return the homography\n\n\n#### TEST THE FUNCTION ####\nimage1 = open_resize('Dataset/scene1/I11.jpg', 600, 400)\nimage2 = open_resize('Dataset/scene1/I12.jpg', 600, 400)\nsrc_pts, dst_pts, matching_result = get_src_dst_points(image1, image2)\nidx = np.random.choice(src_pts.shape[0], 4, replace=False) \nH = compute_homography(src_pts[idx], dst_pts[idx]) \nprint('Homography Matrix:')\nprint(H)\n\nHomography Matrix:\n[[ 3.91213361e-01 -1.56540855e-01 -7.96598341e+01]\n [-1.76497515e-01  3.67446603e-01  9.76646463e+01]\n [-7.92663451e-04 -7.74891393e-04  1.00000000e+00]]\n\n\n\n# RANSAC to find the best homography matrix\ndef ransac_homography(src_pts, dst_pts, n_iter=1000, threshold=0.5):\n    n = src_pts.shape[0] # get the number of points\n    best_H = None # initialize the best homography matrix\n    max_inliers = 0 # initialize the maximum number of inliers\n    for i in range(n_iter):\n        idx = np.random.choice(n, 4, replace=False) # randomly select 4 points\n        H = compute_homography(src_pts[idx], dst_pts[idx]) # compute the homography matrix\n        src_pts_hat = np.hstack((src_pts, np.ones((n, 1)))) # convert to homogeneous coordinates\n        dst_pts_hat = np.hstack((dst_pts, np.ones((n, 1)))) # convert to homogeneous coordinates\n        dst_pts_hat_hat = np.matmul(H, src_pts_hat.T).T # apply the homography matrix\n        dst_pts_hat_hat = dst_pts_hat_hat[:, :2] / dst_pts_hat_hat[:, 2:] # convert back to non-homogeneous coordinates\n        diff = np.linalg.norm(dst_pts_hat_hat - dst_pts, axis=1) # compute the difference between the predicted and actual coordinates\n        inliers = np.sum(diff &lt; threshold) # count the number of inliers\n        if inliers &gt; max_inliers: # update the best homography matrix\n            max_inliers = inliers \n            best_H = H\n    return best_H # return the best homography matrix\n\n\n#### TEST THE FUNCTION ####\nimage1 = open_resize('Dataset/scene1/I12.jpg', 600, 400)\nimage2 = open_resize('Dataset/scene1/I13.jpg', 600, 400)\nsrc_pts, dst_pts, matching_result = get_src_dst_points(image1, image2)\nbest_H = ransac_homography(src_pts, dst_pts, n_iter=10000, threshold=0.5)\nprint('Best Homography Matrix:')\nprint(best_H)\n\nBest Homography Matrix:\n[[ 1.10942139e+00  1.24788612e-03 -2.86918367e+02]\n [ 1.51381324e-02  1.07799975e+00 -1.47459874e+01]\n [ 1.83085652e-04 -3.87132426e-06  1.00000000e+00]]\n\n\n\n\nStep 3 Stitch color images of each scene from the dataset using the homography matrix estimated in step (2) to create a panorama.\n\nDefining the helper functions for stitching\n\n# function to align the images\ndef align_images(image, H, factor):\n    h, w, _ = image.shape # get the shape of image\n    _h, _w = factor*h, factor*w  # create a canvas that is 4 times larger than image\n    aligned_image = np.zeros((_h, _w, 3), dtype=np.uint8) # initialize the output image\n\n    # Loop through each pixel in the output image\n    for y in range(-h, _h-h):\n        for x in range(-w, _w-w):\n            # Apply the homography transformation to find the corresponding pixel in image\n            pt = np.dot(H, np.array([x, y, 1]))\n            pt = pt / pt[2]  # Normalize the coordinates\n            # Check if the transformed coordinates are within the bounds of image\n            if 0 &lt;= pt[0] &lt; image.shape[1] and 0 &lt;= pt[1] &lt; image.shape[0]:\n                # Interpolate the pixel color value using bilinear interpolation\n                x0, y0 = int(pt[0]), int(pt[1])\n                x1, y1 = x0 + 1, y0 + 1\n                alpha = pt[0] - x0\n                beta = pt[1] - y0\n                # Check if the neighboring pixels are within the bounds of image\n                if 0 &lt;= x0 &lt; image.shape[1] and 0 &lt;= x1 &lt; image.shape[1] and \\\n                   0 &lt;= y0 &lt; image.shape[0] and 0 &lt;= y1 &lt; image.shape[0]:\n                    # Bilinear interpolation\n                    interpolated_color = (1 - alpha) * (1 - beta) * image[y0, x0] + \\\n                                         alpha * (1 - beta) * image[y0, x1] + \\\n                                         (1 - alpha) * beta * image[y1, x0] + \\\n                                         alpha * beta * image[y1, x1]\n                    aligned_image[y+h, x+w] = interpolated_color.astype(np.uint8)  # Set the pixel value in the canvas as transformed pixel value         \n    return aligned_image, h, w # return the aligned image and canvas parameters\n\n# function to remove the black background\ndef remove_black_background(img):\n    mask = img.sum(axis=2) &gt; 0 # create a mask to remove black pixels\n    y, x = np.where(mask) # get the coordinates of the non-black pixels\n    x_min, x_max = x.min(), x.max() # get the minimum and maximum x coordinates\n    y_min, y_max = y.min(), y.max() # get the minimum and maximum y coordinates\n    img = img[y_min:y_max+1, x_min:x_max+1, :] # crop the image\n    return img # return the image\n\n# function to align the images\ndef get_transformed_images(img1, img2, H, focus=2, blend=True, factor=4, b_region=5):\n    h1, w1 = img1.shape[:2] # height and width of the first image\n    h2, w2 = img2.shape[:2] # height and width of the second image\n    corners1 = np.array([[0, 0], [0, h1], [w1, h1], [w1, 0]], dtype=np.float32) # corners of the first image\n    corners2 = np.array([[0, 0], [0, h2], [w2, h2], [w2, 0]], dtype=np.float32) # corners of the second image\n   \n    # coordinates of the four corners of the transformed image\n    corners2_transformed = cv2.perspectiveTransform(corners2.reshape(1, -1, 2), H).reshape(-1, 2)\n    # coordinates of the four corners of the new image\n    corners = np.concatenate((corners1, corners2_transformed), axis=0)\n    x_min, y_min = np.int32(corners.min(axis=0).ravel() - 0.5)\n    x_max, y_max = np.int32(corners.max(axis=0).ravel() + 0.5)\n    \n    T = np.array([[1, 0, -x_min], [0, 1, -y_min], [0, 0, 1]]) # translation matrix\n    H_inv = np.linalg.inv(T.dot(H)) # inverse of the homography matrix\n\n    if focus == 1: # wrap the second image into the first image\n        img_transformed, h_, w_ = align_images(img2, H, factor) # align the second image\n        img_res = img_transformed.copy() # copy the transformed image\n        img_res[h_ : h1 + h_, w_ : w1 + w_] = img1 # paste the first image\n\n        if blend == True: # blend around the edges\n            img_reg = img_res[h_ : h_ + h1 , -b_region + w_ : w_ + b_region] # right edge\n            img_res[h_:h_+h1 , -b_region + w_ : w_ + b_region] = cv2.GaussianBlur(img_reg, (3, 1), b_region, b_region)\n            img_reg = img_res[h_ : h_ + h1 , w2 - b_region + w_ : w_ + b_region + w2] # left edge\n            img_res[h_ : h_ + h1 , w2 - b_region + w_ : w_ + b_region + w2] =  cv2.GaussianBlur(img_reg, (3, 1), b_region, b_region)\n            img_reg = img_res[-b_region + h_ : h_ + b_region , w_ : w_ + w1] # top edge\n            img_res[-b_region + h_ : h_ + b_region , w_ : w_ + w1] =  cv2.GaussianBlur(img_reg, (1, 3), b_region, b_region)\n            img_reg = img_res[h2 - b_region + h_ : h_ + b_region  + h2, + w_ : w_ + w1] # bottom edge\n            img_res[h2 - b_region + h_ : h_ + b_region  + h2, w_ : w_ + w1] =  cv2.GaussianBlur(img_reg, (1, 3), b_region, b_region)\n\n    else: # wrap the first image into the second image\n        img_transformed, h_, w_ = align_images(img1, H_inv, factor) # align the first image\n        img_res = img_transformed.copy()  # copy the transformed image        \n        img_res[-y_min + h_ : h2 - y_min + h_, -x_min + w_ : w2 - x_min + w_] = img2 # paste the second image  \n        \n        if blend == True: # blend around the edges\n            img_reg = img_res[-y_min + h_ : h_ + h2 - y_min, -x_min - b_region + w_ : w_ + b_region - x_min] # right edge\n            img_res[-y_min + h_:h_+h2 - y_min, -x_min -b_region+ w_:w_+b_region - x_min] = cv2.GaussianBlur(img_reg, (3, 1), b_region, b_region)\n            img_reg = img_res[-y_min + h_ : h_ + h2 - y_min, -x_min + w2 - b_region+  w_ : w_ + b_region - x_min + w2] # left edge\n            img_res[-y_min + h_ : h_ + h2 - y_min, - x_min + w2 - b_region+ w_ : w_ + b_region - x_min + w2] =  cv2.GaussianBlur(img_reg, (3, 1), b_region, b_region)\n            img_reg = img_res[-y_min - b_region + h_ : h_ + b_region - y_min, -x_min + w_ : w_ + w2 - x_min] # top edge\n            img_res[-y_min - b_region + h_ : h_ + b_region - y_min, - x_min + w_ : w_ + w2 - x_min] =  cv2.GaussianBlur(img_reg, (1, 3), b_region, b_region)\n            img_reg = img_res[-y_min + h2 - b_region + h_ : h_ + b_region - y_min + h2, - x_min + w_ : w_ + w2 - x_min] # bottom edge\n            img_res[-y_min + h2 - b_region + h_ : h_ + b_region - y_min + h2, -x_min + w_ : w_ + w2 - x_min] =  cv2.GaussianBlur(img_reg, (1, 3), b_region, b_region)\n\n    return img_res # return the transformed image\n\n\n#### TEST THE FUNCTIONS ####\nimage1 = open_resize('Dataset/scene4/I41.jpg', 600, 400)\nimage2 = open_resize('Dataset/scene4/I42.jpg', 600, 400)\nsrc_pts, dst_pts, matching_result = get_src_dst_points(image1, image2)\nshow_image(BGR2RGB(matching_result), w=6, h=3)\nbest_H = ransac_homography(src_pts, dst_pts, n_iter=1000, threshold=0.5)\n\naligned_image_1 = get_transformed_images(image1, image2, best_H, focus=1)\naligned_image_1 = remove_black_background(BGR2RGB(aligned_image_1))\nshow_image(aligned_image_1, w=6, h=3)\n\naligned_image_2 = get_transformed_images(image1, image2, best_H, focus=2)\naligned_image_2 = remove_black_background(BGR2RGB(aligned_image_2))\nshow_image(aligned_image_2, w=6, h=3)\n\n\n\n\n\n\n\n\n\n\n\n\nScene 1\n\n# creating folder for scene1\nif not os.path.exists('Results/scene1'):\n    os.makedirs('Results/scene1')\n\n# operning the images\nimage1 = open_resize('Dataset/scene1/I11.jpg', 600, 400)\nimage2 = open_resize('Dataset/scene1/I12.jpg', 600, 400)\nimage3 = open_resize('Dataset/scene1/I13.jpg', 600, 400)\nimage4 = open_resize('Dataset/scene1/I14.jpg', 600, 400)\n\n\n# stitiching image1 and image2\nsrc_pts, dst_pts, matching_result = get_src_dst_points(image1, image2)\nshow_image(BGR2RGB(matching_result), w=6, h=3)\nbest_H = ransac_homography(src_pts, dst_pts, n_iter=10000, threshold=5)\naligned_image_1 = get_transformed_images(image1, image2, best_H, focus=2, blend=True)\naligned_image_1 = remove_black_background(BGR2RGB(aligned_image_1))\nshow_image(aligned_image_1, w=6, h=3)\n\n\n\n\n\n\n\n\n# stitiching image3 and image4 \nsrc_pts, dst_pts, matching_result = get_src_dst_points(image3, image4)\nshow_image(BGR2RGB(matching_result), w=6, h=3)\nbest_H = ransac_homography(src_pts, dst_pts, n_iter=10000, threshold=0.5)\naligned_image_2 = get_transformed_images(image3, image4, best_H, focus=1, blend=False)\naligned_image_2 = remove_black_background(BGR2RGB(aligned_image_2))\nshow_image(aligned_image_2, w=6, h=3)\n\n\n\n\n\n\n\n\n# stitiching aligned_image_1 and aligned_image_2\naligned_1 = BGR2RGB(aligned_image_1)\naligned_2 = BGR2RGB(aligned_image_2)\naligned_1 = cv2.resize(aligned_1, (800, 400))\naligned_2 = cv2.resize(aligned_2, (800, 400))\n\nsrc_pts, dst_pts, matching_result = get_src_dst_points(aligned_1, aligned_2)\nshow_image(BGR2RGB(matching_result), w=12, h=6)\nbest_H = ransac_homography(src_pts, dst_pts, n_iter=10000, threshold=0.5)\naligned_image_3 = get_transformed_images(aligned_1, aligned_2, best_H, focus=2)\naligned_image_3 = remove_black_background(BGR2RGB(aligned_image_3))\nshow_image(aligned_image_3, w=12, h=6)\n\n# saving the stitched image\naligned_image_3 = BGR2RGB(aligned_image_3)\ncv2.imwrite('Results/scene1/scene1.jpg', aligned_image_3)\n\n\n\n\n\n\n\nTrue\n\n\n\n\nScene 2\n\n# creating folder for scene1\nif not os.path.exists('Results/scene2'):\n    os.makedirs('Results/scene2')\n\n# operning the images\nimage1 = open_resize('Dataset/scene2/I21.jpg', 600, 400)\nimage2 = open_resize('Dataset/scene2/I22.jpg', 600, 400)\nimage3 = open_resize('Dataset/scene2/I23.jpg', 600, 400)\nimage4 = open_resize('Dataset/scene2/I24.jpg', 600, 400)\n\n\n# stitiching image1 and image2\nsrc_pts, dst_pts, matching_result = get_src_dst_points(image1, image2)\nshow_image(BGR2RGB(matching_result), w=6, h=3)\nbest_H = ransac_homography(src_pts, dst_pts, n_iter=1000, threshold=0.5)\naligned_image_1 = get_transformed_images(image1, image2, best_H, focus=1, blend=True)\naligned_image_1 = remove_black_background(BGR2RGB(aligned_image_1))\nshow_image(aligned_image_1, w=6, h=3)\n\n\n\n\n\n\n\n\n# stitiching aligned_image_1 and image3\naligned_1 = BGR2RGB(aligned_image_1)\naligned_1 = cv2.resize(aligned_1, (800, 400))\n\nsrc_pts, dst_pts, matching_result = get_src_dst_points(aligned_1, image3)\nshow_image(BGR2RGB(matching_result), w=10, h=5)\nbest_H = ransac_homography(src_pts, dst_pts, n_iter=10000, threshold=5)\naligned_image_2 = get_transformed_images(aligned_1, image3, best_H, focus=2, blend=False)\naligned_image_2 = remove_black_background(BGR2RGB(aligned_image_2))\nshow_image(aligned_image_2, w=10, h=5)\n\n\n\n\n\n\n\n\n# stitiching aligned_image_2 and image4\naligned_2 = BGR2RGB(aligned_image_2)\naligned_2 = cv2.resize(aligned_2, (1000, 400))\n\nsrc_pts, dst_pts, matching_result = get_src_dst_points(aligned_2, image4)\nshow_image(BGR2RGB(matching_result), w=12, h=6)\nbest_H = ransac_homography(src_pts, dst_pts, n_iter=10000, threshold=0.5)\naligned_image_3 = get_transformed_images(aligned_2, image4, best_H, focus=1, blend=True)\naligned_image_3 = remove_black_background(BGR2RGB(aligned_image_3))\nshow_image(aligned_image_3, w=12, h=6)\n\n# saving the stitched image\naligned_image_3 = BGR2RGB(aligned_image_3)\ncv2.imwrite('Results/scene2/scene2.jpg', aligned_image_3)\n\n\n\n\n\n\n\nTrue\n\n\n\n\nScene 3\n\n# creating folder for scene1\nif not os.path.exists('Results/scene3'):\n    os.makedirs('Results/scene3')\n\n# operning the images\nimage1 = open_resize('Dataset/scene3/I31.jpg', 600, 400)\nimage2 = open_resize('Dataset/scene3/I32.jpg', 600, 400)\nimage3 = open_resize('Dataset/scene3/I33.jpg', 600, 400)\nimage4 = open_resize('Dataset/scene3/I34.jpg', 600, 400)\n\n\n# stitiching image1 and image2\nsrc_pts, dst_pts, matching_result = get_src_dst_points(image1, image2)\nshow_image(BGR2RGB(matching_result), w=6, h=3)\nbest_H = ransac_homography(src_pts, dst_pts, n_iter=1000, threshold=0.5)\naligned_image_1 = get_transformed_images(image1, image2, best_H, focus=2, blend=True)\naligned_image_1 = remove_black_background(BGR2RGB(aligned_image_1))\nshow_image(aligned_image_1, w=6, h=3)\n\n\n\n\n\n\n\n\n# stitching aligned_image_1 and image3\naligned_1 = BGR2RGB(aligned_image_1)\naligned_1 = cv2.resize(aligned_1, (800, 400))\n\nsrc_pts, dst_pts, matching_result = get_src_dst_points(aligned_1, image3)\nshow_image(BGR2RGB(matching_result), w=10, h=5)\nbest_H = ransac_homography(src_pts, dst_pts, n_iter=10000, threshold=0.5)\naligned_image_2 = get_transformed_images(aligned_1, image3, best_H, focus=2, blend=False)\naligned_image_2 = remove_black_background(BGR2RGB(aligned_image_2))\nshow_image(aligned_image_2, w=10, h=5)\n\n\n\n\n\n\n\n\n# stitching aligned_image_2 and image4\naligned_2 = BGR2RGB(aligned_image_2)\naligned_2 = cv2.resize(aligned_2, (1000, 400))\n\nsrc_pts, dst_pts, matching_result = get_src_dst_points(aligned_2, image4)\nshow_image(BGR2RGB(matching_result), w=12, h=6)\nbest_H = ransac_homography(src_pts, dst_pts, n_iter=10000, threshold=0.5)\naligned_image_3 = get_transformed_images(aligned_2, image4, best_H, focus=1, blend=False)\naligned_image_3 = remove_black_background(BGR2RGB(aligned_image_3))\nshow_image(aligned_image_3, w=12, h=6)\n\n# saving the stitched image\naligned_image_3 = BGR2RGB(aligned_image_3)\ncv2.imwrite('Results/scene3/scene3.jpg', aligned_image_3)\n\n\n\n\n\n\n\nTrue\n\n\n\n\nScene 4\n\n# creating folder for scene1\nif not os.path.exists('Results/scene4'):\n    os.makedirs('Results/scene4')\n\n# operning the images\nimage1 = open_resize('Dataset/scene4/I41.jpg', 600, 400)\nimage2 = open_resize('Dataset/scene4/I42.jpg', 600, 400)\n\n\n# stitiching image1 and image2\nsrc_pts, dst_pts, matching_result = get_src_dst_points(image1, image2)\nshow_image(BGR2RGB(matching_result), w=10, h=5)\nbest_H = ransac_homography(src_pts, dst_pts, n_iter=10000, threshold=0.5)\naligned_image_1 = get_transformed_images(image1, image2, best_H, focus=2, blend=True)\naligned_image_1 = remove_black_background(BGR2RGB(aligned_image_1))\nshow_image(aligned_image_1, w=10, h=10)\n\n# saving the stitched image\naligned_image_1 = BGR2RGB(aligned_image_1)\ncv2.imwrite('Results/scene4/scene4.jpg', aligned_image_1)\n\n\n\n\n\n\n\nTrue\n\n\n\n\nScene 5\n\n# creating folder for scene1\nif not os.path.exists('Results/scene5'):\n    os.makedirs('Results/scene5')\n\n# operning the images\nimage1 = open_resize('Dataset/scene5/I51.jpg', 600, 400)\nimage2 = open_resize('Dataset/scene5/I52.jpg', 600, 400)\n\n\n# stitiching image1 and image2\nsrc_pts, dst_pts, matching_result = get_src_dst_points(image1, image2)\nshow_image(BGR2RGB(matching_result), w=10, h=5)\nbest_H = ransac_homography(src_pts, dst_pts, n_iter=10000, threshold=0.5)\naligned_image_1 = get_transformed_images(image1, image2, best_H, focus=2, blend=True)\naligned_image_1 = remove_black_background(BGR2RGB(aligned_image_1))\nshow_image(aligned_image_1, w=10, h=8)\n\n# saving the stitched image\naligned_image_1 = BGR2RGB(aligned_image_1)\ncv2.imwrite('Results/scene5/scene5.jpg', aligned_image_1)\n\n\n\n\n\n\n\nTrue\n\n\n\n\nScene 6\n\n# creating folder for scene1\nif not os.path.exists('Results/scene6'):\n    os.makedirs('Results/scene6')\n\n# operning the images\nimage1 = open_resize('Dataset/scene6/I61.jpg', 600, 400)\nimage2 = open_resize('Dataset/scene6/I62.jpg', 600, 400)\n\n\n# stitiching image1 and image2\nsrc_pts, dst_pts, matching_result = get_src_dst_points(image1, image2)\nshow_image(BGR2RGB(matching_result), w=10, h=5)\nbest_H = ransac_homography(src_pts, dst_pts, n_iter=10000, threshold=5)\naligned_image_1 = get_transformed_images(image1, image2, best_H, focus=2, blend=True)\naligned_image_1 = remove_black_background(BGR2RGB(aligned_image_1))\nshow_image(aligned_image_1, w=10, h=8)\n\n# saving the stitched image\naligned_image_1 = BGR2RGB(aligned_image_1)\ncv2.imwrite('Results/scene6/scene6.jpg', aligned_image_1)\n\n\n\n\n\n\n\nTrue\n\n\n\n\n\nStep 4 Stitch the images used as input in step (3) using in-built command for homography estimation and compare it with the panorama obtained in step (3).\n\nHelper function\n\n# function for finding the best homography matrix using cv2 library\ndef cv2_get_RANSAC_Homography(src_pts, dst_pts, threshold=5):\n    H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, threshold) # find the best homography matrix\n    return H # return the best homography matrix\n\n\n\nScene 1\n\n# operning the images\nimage1 = open_resize('Dataset/scene1/I11.jpg', 600, 400)\nimage2 = open_resize('Dataset/scene1/I12.jpg', 600, 400)\nimage3 = open_resize('Dataset/scene1/I13.jpg', 600, 400)\nimage4 = open_resize('Dataset/scene1/I14.jpg', 600, 400)\n\n\n# stitiching image1 and image2\nsrc_pts, dst_pts, matching_result = get_src_dst_points(image1, image2)\nbest_H = cv2_get_RANSAC_Homography(src_pts, dst_pts, threshold=5)\naligned_image_1 = get_transformed_images(image1, image2, best_H, focus=2, blend=True)\naligned_image_1 = remove_black_background(BGR2RGB(aligned_image_1))\nshow_image(aligned_image_1, w=6, h=3)\n\n\n\n\n\n# stitiching image3 and image4 \nsrc_pts, dst_pts, matching_result = get_src_dst_points(image3, image4)\nbest_H = cv2_get_RANSAC_Homography(src_pts, dst_pts, threshold=0.5)\naligned_image_2 = get_transformed_images(image3, image4, best_H, focus=1, blend=False)\naligned_image_2 = remove_black_background(BGR2RGB(aligned_image_2))\nshow_image(aligned_image_2, w=6, h=3)\n\n\n\n\n\n# stitiching aligned_image_1 and aligned_image_2\naligned_1 = BGR2RGB(aligned_image_1)\naligned_2 = BGR2RGB(aligned_image_2)\naligned_1 = cv2.resize(aligned_1, (800, 400))\naligned_2 = cv2.resize(aligned_2, (800, 400))\n\nsrc_pts, dst_pts, matching_result = get_src_dst_points(aligned_1, aligned_2)\nbest_H = cv2_get_RANSAC_Homography(src_pts, dst_pts, threshold=0.5)\naligned_image_3 = get_transformed_images(aligned_1, aligned_2, best_H, focus=2)\naligned_image_3 = remove_black_background(BGR2RGB(aligned_image_3))\nshow_image(aligned_image_3, w=12, h=6)\n\n# saving the stitched image\naligned_image_3 = BGR2RGB(aligned_image_3)\ncv2.imwrite('Results/scene1/scene1_cv2.jpg', aligned_image_3)\n\n\n\n\nTrue\n\n\nComparing the results of the two methods\n\ncustom = cv2.imread('Results/scene1/scene1.jpg')\nshow_image(BGR2RGB(custom), w=12, h=6, title='Custom Homography and RANSAC')\n\ninbuilt = cv2.imread('Results/scene1/scene1_cv2.jpg')\nshow_image(BGR2RGB(inbuilt), w=12, h=6, title='Inbuilt Homography and RANSAC')\n\n\n\n\n\n\n\nInference\n\nThe image obtained through a custom implementation of homography computation and RANSAC looks similar to the image obtained through the built-in function for homography estimation.\n\n\nScene 2\n\n# operning the images\nimage1 = open_resize('Dataset/scene2/I21.jpg', 600, 400)\nimage2 = open_resize('Dataset/scene2/I22.jpg', 600, 400)\nimage3 = open_resize('Dataset/scene2/I23.jpg', 600, 400)\nimage4 = open_resize('Dataset/scene2/I24.jpg', 600, 400)\n\n\n# stitiching image1 and image2\nsrc_pts, dst_pts, matching_result = get_src_dst_points(image1, image2)\nbest_H = cv2_get_RANSAC_Homography(src_pts, dst_pts, threshold=0.5)\naligned_image_1 = get_transformed_images(image1, image2, best_H, focus=1, blend=True)\naligned_image_1 = remove_black_background(BGR2RGB(aligned_image_1))\nshow_image(aligned_image_1, w=6, h=3)\n\n\n\n\n\n# stitiching aligned_image_1 and image3\naligned_1 = BGR2RGB(aligned_image_1)\naligned_1 = cv2.resize(aligned_1, (800, 400))\n\nsrc_pts, dst_pts, matching_result = get_src_dst_points(aligned_1, image3)\nbest_H = cv2_get_RANSAC_Homography(src_pts, dst_pts, threshold=5)\naligned_image_2 = get_transformed_images(aligned_1, image3, best_H, focus=2, blend=False)\naligned_image_2 = remove_black_background(BGR2RGB(aligned_image_2))\nshow_image(aligned_image_2, w=10, h=5)\n\n\n\n\n\n# stitiching aligned_image_2 and image4\naligned_2 = BGR2RGB(aligned_image_2)\naligned_2 = cv2.resize(aligned_2, (1000, 400))\n\nsrc_pts, dst_pts, matching_result = get_src_dst_points(aligned_2, image4)\nbest_H = cv2_get_RANSAC_Homography(src_pts, dst_pts, threshold=0.5)\naligned_image_3 = get_transformed_images(aligned_2, image4, best_H, focus=1, blend=True)\naligned_image_3 = remove_black_background(BGR2RGB(aligned_image_3))\nshow_image(aligned_image_3, w=12, h=6)\n\n# saving the stitched image\naligned_image_3 = BGR2RGB(aligned_image_3)\ncv2.imwrite('Results/scene2/scene2_cv2.jpg', aligned_image_3)\n\n\n\n\nTrue\n\n\nComparing the results of the two methods\n\ncustom = cv2.imread('Results/scene2/scene2.jpg')\nshow_image(BGR2RGB(custom), w=12, h=6, title='Custom Homography and RANSAC')\n\ninbuilt = cv2.imread('Results/scene2/scene2_cv2.jpg')\nshow_image(BGR2RGB(inbuilt), w=12, h=6, title='Inbuilt Homography and RANSAC')\n\n\n\n\n\n\n\nInference\n\nThe image obtained through a custom implementation of homography computation and RANSAC looks similar to the image obtained through the built-in function for homography estimation.\n\n\nScene 3\n\n# operning the images\nimage1 = open_resize('Dataset/scene3/I31.jpg', 600, 400)\nimage2 = open_resize('Dataset/scene3/I32.jpg', 600, 400)\nimage3 = open_resize('Dataset/scene3/I33.jpg', 600, 400)\nimage4 = open_resize('Dataset/scene3/I34.jpg', 600, 400)\n\n\n# stitiching image1 and image2\nsrc_pts, dst_pts, matching_result = get_src_dst_points(image1, image2)\nbest_H = cv2_get_RANSAC_Homography(src_pts, dst_pts, threshold=0.5)\naligned_image_1 = get_transformed_images(image1, image2, best_H, focus=2, blend=True)\naligned_image_1 = remove_black_background(BGR2RGB(aligned_image_1))\nshow_image(aligned_image_1, w=6, h=3)\n\n\n\n\n\n# stitching aligned_image_1 and image3\naligned_1 = BGR2RGB(aligned_image_1)\naligned_1 = cv2.resize(aligned_1, (800, 400))\n\nsrc_pts, dst_pts, matching_result = get_src_dst_points(aligned_1, image3)\nbest_H = cv2_get_RANSAC_Homography(src_pts, dst_pts, threshold=2)\naligned_image_2 = get_transformed_images(aligned_1, image3, best_H, focus=2, blend=False)\naligned_image_2 = remove_black_background(BGR2RGB(aligned_image_2))\nshow_image(aligned_image_2, w=10, h=5)\n\n\n\n\n\n# stitching aligned_image_2 and image4\naligned_2 = BGR2RGB(aligned_image_2)\naligned_2 = cv2.resize(aligned_2, (1000, 400))\n\nsrc_pts, dst_pts, matching_result = get_src_dst_points(aligned_2, image4)\nbest_H = cv2_get_RANSAC_Homography(src_pts, dst_pts, threshold=0.5)\naligned_image_3 = get_transformed_images(aligned_2, image4, best_H, focus=1, blend=False)\naligned_image_3 = remove_black_background(BGR2RGB(aligned_image_3))\nshow_image(aligned_image_3, w=12, h=6)\n\n# saving the stitched image\naligned_image_3 = BGR2RGB(aligned_image_3)\ncv2.imwrite('Results/scene3/scene3_cv2.jpg', aligned_image_3)\n\n\n\n\nTrue\n\n\nComparing the results of the two methods\n\ncustom = cv2.imread('Results/scene3/scene3.jpg')\nshow_image(BGR2RGB(custom), w=12, h=6, title='Custom Homography and RANSAC')\n\ninbuilt = cv2.imread('Results/scene3/scene3_cv2.jpg')\nshow_image(BGR2RGB(inbuilt), w=12, h=6, title='Inbuilt Homography and RANSAC')\n\n\n\n\n\n\n\nInference\n\nThe image obtained through a custom implementation of homography computation and RANSAC looks similar to the image obtained through the built-in function for homography estimation.\n\n\nScene 4\n\n# operning the images\nimage1 = open_resize('Dataset/scene4/I41.jpg', 600, 400)\nimage2 = open_resize('Dataset/scene4/I42.jpg', 600, 400)\n\n\n# stitiching image1 and image2\nsrc_pts, dst_pts, matching_result = get_src_dst_points(image1, image2)\nbest_H = cv2_get_RANSAC_Homography(src_pts, dst_pts, threshold=0.5)\naligned_image_1 = get_transformed_images(image1, image2, best_H, focus=2, blend=True)\naligned_image_1 = remove_black_background(BGR2RGB(aligned_image_1))\nshow_image(aligned_image_1, w=10, h=10)\n\n# saving the stitched image\naligned_image_1 = BGR2RGB(aligned_image_1)\ncv2.imwrite('Results/scene4/scene4_cv2.jpg', aligned_image_1)\n\n\n\n\nTrue\n\n\nComparing the results of the two methods\n\ncustom = cv2.imread('Results/scene4/scene4.jpg')\nshow_image(BGR2RGB(custom), w=12, h=6, title='Custom Homography and RANSAC')\n\ninbuilt = cv2.imread('Results/scene4/scene4_cv2.jpg')\nshow_image(BGR2RGB(inbuilt), w=12, h=6, title='Inbuilt Homography and RANSAC')\n\n\n\n\n\n\n\nInference\n\nThe image obtained through a custom implementation of homography computation and RANSAC looks similar to the image obtained through the built-in function for homography estimation.\n\n\nScene 5\n\n# operning the images\nimage1 = open_resize('Dataset/scene5/I51.jpg', 600, 400)\nimage2 = open_resize('Dataset/scene5/I52.jpg', 600, 400)\n\n\n# stitiching image1 and image2\nsrc_pts, dst_pts, matching_result = get_src_dst_points(image1, image2)\nbest_H = cv2_get_RANSAC_Homography(src_pts, dst_pts, threshold=0.5)\naligned_image_1 = get_transformed_images(image1, image2, best_H, focus=2, blend=True)\naligned_image_1 = remove_black_background(BGR2RGB(aligned_image_1))\nshow_image(aligned_image_1, w=10, h=8)\n\n# saving the stitched image\naligned_image_1 = BGR2RGB(aligned_image_1)\ncv2.imwrite('Results/scene5/scene5_cv2.jpg', aligned_image_1)\n\n\n\n\nTrue\n\n\nComparing the results of the two methods\n\ncustom = cv2.imread('Results/scene5/scene5.jpg')\nshow_image(BGR2RGB(custom), w=12, h=6, title='Custom Homography and RANSAC')\n\ninbuilt = cv2.imread('Results/scene5/scene5_cv2.jpg')\nshow_image(BGR2RGB(inbuilt), w=12, h=6, title='Inbuilt Homography and RANSAC')\n\n\n\n\n\n\n\nInference\n\nThe image obtained through a custom implementation of homography computation and RANSAC looks similar to the image obtained through the built-in function for homography estimation.\n\n\nScene 6\n\n# operning the images\nimage1 = open_resize('Dataset/scene6/I61.jpg', 600, 400)\nimage2 = open_resize('Dataset/scene6/I62.jpg', 600, 400)\n\n\n# stitiching image1 and image2\nsrc_pts, dst_pts, matching_result = get_src_dst_points(image1, image2)\nbest_H = cv2_get_RANSAC_Homography(src_pts, dst_pts, threshold=5)\naligned_image_1 = get_transformed_images(image1, image2, best_H, focus=2, blend=True)\naligned_image_1 = remove_black_background(BGR2RGB(aligned_image_1))\nshow_image(aligned_image_1, w=10, h=8)\n\n# saving the stitched image\naligned_image_1 = BGR2RGB(aligned_image_1)\ncv2.imwrite('Results/scene6/scene6_cv2.jpg', aligned_image_1)\n\n\n\n\nTrue\n\n\nComparing the results of the two methods\n\ncustom = cv2.imread('Results/scene6/scene6.jpg')\nshow_image(BGR2RGB(custom), w=12, h=6, title='Custom Homography and RANSAC')\n\ninbuilt = cv2.imread('Results/scene6/scene6_cv2.jpg')\nshow_image(BGR2RGB(inbuilt), w=12, h=6, title='Inbuilt Homography and RANSAC')\n\n\n\n\n\n\n\nInference\n\nThe image obtained through a custom implementation of homography computation and RANSAC looks similar to the image obtained through the built-in function for homography estimation.\n\n\n\nReferences\n\nImage stitching, https://en.wikipedia.org/wiki/Image_stitching\nPerspective Transformation, https://www.tutorialspoint.com/dip/perspective_transformation.htm\nFirst Principles of Computer Vision, Columbia Engineering, https://fpcv.cs.columbia.edu\nFeature Based Panoramic Image Stitching, https://in.mathworks.com/help/vision/ug/feature-based-panoramic-image-stitching.html"
  },
  {
    "objectID": "projects/012 CameraSHOT/index.html",
    "href": "projects/012 CameraSHOT/index.html",
    "title": "CameraSHOT",
    "section": "",
    "text": "GitHub \n\n\n\nAbstract\n\nCameraSHOT is a simple and effective application software designed to alter the theme or color scheme of an image effortlessly. Inspired by Windows’ adaptive theme feature, which automatically selects an accent color from the desktop background for various interface elements, CameraSHOT takes this concept further. It analyzes the pre-existing theme or color scheme of an image and modifies it with minimal user intervention. The core principle of CameraSHOT is to examine an image and alter its theme or color scheme comprehensively.\n\n\n\nProject Description\n\nThe internet offers billions of free wallpapers, yet there are times when users may wish to change the theme or color scheme of an image. Theme alteration goes beyond merely changing the image background; it involves adjusting the color scheme at the pixel level while preserving the original components, details, and quality. Although theme alteration can be achieved using advanced image editing software like Adobe Photoshop, this process is often time-consuming and requires significant manual effort. Most image editing software does not provide specific tools tailored for color scheme alterations, making it a cumbersome task for users.\n\n\nA practical example of theme alteration can be observed through two images taken outdoors from the same point and angle at different times of the day, such as morning and evening. The morning image might feature a bluish sky with a yellowish hue and high brightness, while the evening image might display a reddish sky with an orangish hue and a comparatively dull appearance. With CameraSHOT, users can transform the morning image to resemble the evening one and vice versa with just a few clicks.\n\n\nCameraSHOT is designed with simplicity in mind, offering a button-based, click-to-go mechanism that enables users to perform impressive alterations to an image with minimal effort. The software comes pre-packed with features such as scheme priority alteration, scheme swap, and scheme casting, allowing for a wide range of theme modifications. CameraSHOT’s intuitive interface requires no prior knowledge of complex tools, making it accessible to all users. The software is minimalistic and self-explanatory, requiring zero dependencies, making it the perfect tool for altering the theme of jpeg/jpg images.\n\n\n\nFeatures\n\nUser-Friendly Interface\n\nCameraSHOT boasts a minimalistic and intuitive user interface, designed for ease of use. Users can navigate through the software effortlessly and perform theme alterations with just a few clicks, without needing any prior knowledge of image editing tools.\n\n\n\nScheme Priority Alteration\n\nThis feature allows users to prioritize specific color schemes within an image, enabling a focused alteration of the theme based on user preferences. Users can highlight and modify dominant colors to achieve the desired effect.\n\n\n\nScheme Swap\n\nWith the scheme swap feature, users can interchange the existing color scheme of an image with a new one, effectively transforming the overall appearance of the image. This feature is particularly useful for creating variations of the same image with different color themes.\n\n\n\nScheme Casting\n\nScheme casting enables users to apply a predefined set of color schemes to an image, providing a quick and efficient way to achieve professional-grade theme alterations. Users can choose from a variety of preset themes to enhance their images.\n\n\n\n\nTechnical Implementation\n\nPixel-Level Analysis\n\nCameraSHOT performs a detailed pixel-level analysis of the image to accurately identify and alter the color scheme. This method ensures that the theme alteration is comprehensive, affecting every pixel while preserving the image’s original components and details.\n\n\n\nAutomated Process\n\nThe software automates the theme alteration process, requiring minimal user intervention. By leveraging advanced algorithms, CameraSHOT simplifies the complex task of theme alteration, making it accessible to users of all skill levels.\n\n\n\nZero Dependencies\n\nCameraSHOT operates independently of other software or libraries, requiring zero dependencies. This design choice enhances the software’s portability and ease of installation, allowing users to start altering themes immediately after installation.\n\n\n\n\nUse Cases\n\nPhotography Enhancement\n\nPhotographers can use CameraSHOT to enhance their images by altering the color schemes to match different moods or themes. This allows for creative experimentation and the production of unique image variations.\n\n\n\nGraphic Design\n\nGraphic designers can benefit from CameraSHOT’s capabilities to quickly adjust the color schemes of images, ensuring consistency with design projects. This tool can streamline the design process and improve workflow efficiency.\n\n\n\nPersonal Use\n\nIndividuals who enjoy personalizing their digital wallpapers can use CameraSHOT to create custom themes for their images, making their desktops and devices more visually appealing and unique.\n\n\n\n\nConclusion\n\nCameraSHOT revolutionizes the way users can alter the theme and color scheme of their images. By providing a user-friendly interface and powerful features, CameraSHOT makes the process of theme alteration simple and efficient. Whether for professional use in photography and graphic design or for personal enjoyment, CameraSHOT offers a versatile and accessible solution for all users.\n\n\n\nFuture Work\n\nFuture enhancements to CameraSHOT could include support for additional image formats, improved theme alteration algorithms, and the inclusion of more advanced editing tools. Expanding the software’s compatibility with various operating systems and devices will further enhance its usability and appeal."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "The Crafted Nest : AR Catalogue\n\n\n\n\n\n\n\nAugmented Reality\n\n\nUnity\n\n\nVuforia\n\n\n\n\nThe project combines Unity and Vuforia to create an interactive experience for customers to explore and see furniture and decor items directly over the catalog.\n\n\n\n\n\n\nJul 2, 2023\n\n\nSoumyaratna Debnath\n\n\n\n\n\n\n  \n\n\n\n\nVision Transformer Implementation from Scratch\n\n\n\n\n\n\n\nComputer Vision\n\n\nDeep Learning\n\n\nPytorch\n\n\nVision Transformer\n\n\n\n\nCustom Implemenation of Vison Trasformer from Scratch for Image Classification using Pytorch.\n\n\n\n\n\n\nJul 8, 2024\n\n\nSoumyaratna Debnath\n\n\n\n\n\n\n  \n\n\n\n\nThe Rail Guardians\n\n\n\n\n\n\n\nGame Theory\n\n\nGraph Theory\n\n\nGame Development\n\n\n\n\nInspired from Eternal Vertex Cover problem, The Rail Guardians is a two-player game where the players alternate between defending and attacking positions in a dynamic, strategic showdown.\n\n\n\n\n\n\nApr 21, 2024\n\n\nSoumyaratna Debnath\n\n\n\n\n\n\n  \n\n\n\n\nEvolving Artistry\n\n\n\n\n\n\n\nImage Processing\n\n\nGame of Life\n\n\nGenetic Algorithm\n\n\n\n\nUsed Genetic Algorithm and Conway’s Game of Life to generate unique artworks!\n\n\n\n\n\n\nOct 30, 2023\n\n\nSoumyaratna Debnath\n\n\n\n\n\n\n  \n\n\n\n\nPicrypt-It\n\n\n\n\n\n\n\nImage Processing\n\n\nSteganography\n\n\nDesktop Application\n\n\n\n\nSoftware with a clean and simplistic user interface, which allows a user to inscribe a digital color image with passcode protected messages.\n\n\n\n\n\n\nMay 25, 2021\n\n\nSoumyaratna Debnath\n\n\n\n\n\n\n  \n\n\n\n\nA Tribute to IITGN’s CSE Faculty\n\n\n\n\n\n\n\nImage Processing\n\n\n\n\nCreated a cover image featuring individual portraits of IIT Gandhinagar CSE faculty.\n\n\n\n\n\n\nJan 11, 2024\n\n\nSoumyaratna Debnath\n\n\n\n\n\n\n  \n\n\n\n\nUnderwater Object Detection\n\n\n\n\n\n\n\nObject Detection\n\n\nImage Enhancement\n\n\n\n\nUnderwater Image Enhancement and Object Detection using MIRNet and YOLO algorithms.\n\n\n\n\n\n\nJul 7, 2022\n\n\nSoumyaratna Debnath\n\n\n\n\n\n\n  \n\n\n\n\nA Comparative Study Between Different VGG Models\n\n\n\n\n\n\n\nMachine Learning\n\n\nDeep Learning\n\n\nComputer Vision\n\n\n\n\nCustom Implementation of VGG 1 Block, VGG 3 Block, VGG 16 and MLP 18 for a detailed comparative study.\n\n\n\n\n\n\nApr 19, 2023\n\n\nSoumyaratna Debnath\n\n\n\n\n\n\n  \n\n\n\n\nThe Magic of Ensemble Algorithms\n\n\n\n\n\n\n\nMachine Learning\n\n\nEnsemble Algorithms\n\n\n\n\nCustom Implementation of Bagging, ADABoost, Decission Tree, Weighted Decision Tree, Gradient Boosted Decision Tree and Random Forest Algorithms for solving some subjective questions.\n\n\n\n\n\n\nJun 3, 2023\n\n\nSoumyaratna Debnath\n\n\n\n\n\n\n  \n\n\n\n\nK-Nearest Neighbors Visualized\n\n\n\n\n\n\n\nImage Processing\n\n\nGame of Life\n\n\nGenetic Algorithm\n\n\n\n\nVisuzliation of K-Nearest Neighbors Algorithm using Python and Streamlit\n\n\n\n\n\n\nApr 24, 2023\n\n\nSoumyaratna Debnath\n\n\n\n\n\n\n  \n\n\n\n\nImage Stitching from Scratch\n\n\n\n\n\n\n\nImage Processing\n\n\n\n\nCustom Implemenation of the Image Stitching Algorithm from Scratch\n\n\n\n\n\n\nJan 26, 2024\n\n\nSoumyaratna Debnath\n\n\n\n\n\n\n  \n\n\n\n\nHistogram Matching from Scratch\n\n\n\n\n\n\n\nImage Processing\n\n\n\n\nCustom Implementation of the Histogram Matching Algorithm from Scratch\n\n\n\n\n\n\nOct 20, 2023\n\n\nSoumyaratna Debnath\n\n\n\n\n\n\n  \n\n\n\n\nCameraSHOT\n\n\n\n\n\n\n\nImage Processing\n\n\nDesktop Application\n\n\n\n\nSimple and effective application software that for altering the theme/color-scheme of an image.\n\n\n\n\n\n\nMay 1, 2021\n\n\nSoumyaratna Debnath\n\n\n\n\n\n\n  \n\n\n\n\nOctober Virtual Mouse\n\n\n\n\n\n\n\nObject Detection\n\n\nImage Enhancement\n\n\n\n\nOctober is a virtual assistant for controlling the mouse pointer using only voice commands.\n\n\n\n\n\n\nJul 24, 2021\n\n\nSoumyaratna Debnath\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/modifiedRDA-1/index.html",
    "href": "publications/modifiedRDA-1/index.html",
    "title": "A New Modified Red Deer Algorithm for Multi-level Image Thresholding",
    "section": "",
    "text": "DOI : 10.1109/ICRCICN50933.2020.9296166\n\n\n\n\nPublished in - 2020 Fifth International Conference on Research in Computational Intelligence and Communication Networks (ICRCICN)\n\nThis paper presents a modified evolution strategy based meta-heuristic, named Modified Red Deer Algorithm (MRDA), which can be effectively and methodically applied to solve single-objective optimization problems. Recently, the actions of red deers have been analysed during their breading time, that in turn inspired the researchers to develop a popular meta-heuristic, called Red Deer Algorithm (RDA). The RDA has been designed to deal with different combinatorial optimization problems in a variety of real-life applications. This paper introduces few adaptive approaches to modify the inherent operators and parameters of RDA to enhance its efficacy. As a comparative study, the performance of MRDA has been evaluated with RDA and Classical Genetic Algorithm (CGA) by utilizing some real-life gray-scale images. At the outset, the results of these competitive algorithms have been assessed with respect to optimum fitness, worst fitness, average fitness, standard deviation, convergence time at best case and average convergence time at three distinct level of thresholding for each test image. Finally, t-test and Friedman Test have been conducted among themselves to check out the superiority. This comparative analysis establishes that MRDA outperforms others in all facets and furnish exceedingly competitive results."
  },
  {
    "objectID": "publications/picrypt/index.html",
    "href": "publications/picrypt/index.html",
    "title": "Picrypt - Inscribe Images with Encrypted Texts",
    "section": "",
    "text": "Draft : Copyright Draft\n\n\n\n\n\n\nInspired by the idea “why to send less, when you can send more”, Picrypt is user-friendly, click-to-go-like software with a clean and simplistic user interface, which allows users to inscribe digital colour images with encrypted messages while preserving the original properties and qualities of the image. This image containing the encrypted message then can be downloaded and saved locally, and can even be shared to anyone across the globe digitally, while the data/message remains protected and preserved throughout. The passcode with which the messages are inscribed acts as the key for the successful decryption of the messages inscribed within. A detailed description of the software including the enciphering and deciphering of the messages has been presented in this work. The basic principle behind the software rests on using the state of pixel intensity values of a color image for inscribing textual data, encrypted with popular encryption methods like Vernam Cipher and Playfair Cipher, into the image pixel-by-pixel."
  }
]