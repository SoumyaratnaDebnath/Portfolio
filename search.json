[
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMay 3, 2023\n\n\nModified Harris Hawk Optimization Algorithm for Multi-level Image Thresholding\n\n\nSoumyaratna Debnath, Abhirup Deb, Sourav De\n\n\n\n\nAug 25, 2022\n\n\nPicrypt - Inscribe Images with Encrypted Texts\n\n\nSoumyaratna Debnath, Sourav De, Siddhartha B.\n\n\n\n\nMay 21, 2021\n\n\nCameraSHOT\n\n\nSoumyaratna Debnath, Sourav De, Siddhartha B.\n\n\n\n\nMar 15, 2021\n\n\nMultilevel Image Segmentation Using Modified Red Deer Algorithm\n\n\nSandip Dey, Sourav De, Abhirup Deb, Soumyaratna Debnath\n\n\n\n\nDec 21, 2020\n\n\nA New Modified Red Deer Algorithm for Multi-level Image Thresholding\n\n\nSourav De, Sandip Dey, Soumyaratna Debnath, Abhirup Deb\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/modifiedRDA-2/index.html",
    "href": "publications/modifiedRDA-2/index.html",
    "title": "Multilevel Image Segmentation Using Modified Red Deer Algorithm",
    "section": "",
    "text": "DOI : 10.1109/Confluence51648.2021.9377112\n\n\n\n\nPublished in - 2021 11th International Conference on Cloud Computing, Data Science & Engineering (Confluence)\n\nIn this paper, an evolution strategy based, Modified Red Deer Algorithm (MRDA) has been proposed to find optimum thresholds of publicly available gray scale images. In the recent year, the activity of red deers has been thoroughly observed in the course of their breading season by a group of researchers. Later, this inspired them to introduce a renowned meta-heuristic, popularly known as Red Deer Algorithm (RDA). The RDA is capable to handle several combinatorial optimization problems of various real-world applications. In this paper, a number of adaptive strategy has been suggested to alter the intrinsic operators and parameters used in RDA to improve its performance. The efficiency of MRDA has been judged with RDA and classical Particle Swarm Optimization (PSO) using two publicly accessible real world benchmark images. The performance of each of the competitive method has been analysed with regards to a variety of measure quantitatively and visually. Finally, as a statistical comparison, Kruskal-Wallis test has been carried out between the proposed method and others. The obtained results prove that MRDA is the best performing method than others in all aspects and provides immensely competitive results"
  },
  {
    "objectID": "publications/modifiedHHO/index.html",
    "href": "publications/modifiedHHO/index.html",
    "title": "Modified Harris Hawk Optimization Algorithm for Multi-level Image Thresholding",
    "section": "",
    "text": "ISBN : 9781032393025\n\n\n\n\n\n\nPublished as a chapter in - Hybrid Computational Intelligent Systems Modeling, Simulation and Optimization\n\nHybrid Computational Intelligent Systems – Modeling, Simulation and Optimization unearths the latest advances in evolving hybrid intelligent modeling and simulation of human-centric data-intensive applications optimized for real-time use, thereby enabling researchers to come up with novel breakthroughs in this ever-growing field.\n\nSalient features include the fundamentals of modeling and simulation with recourse to knowledge-based simulation, interaction paradigms, and human factors, along with the enhancement of the existing state of art in a high-performance computing setup. In addition, this book presents optimization strategies to evolve robust and failsafe intelligent system modeling and simulation.\n\nThe volume also highlights novel applications for different engineering problems including signal and data processing, speech, image, sensor data processing, innovative intelligent systems, and swarm intelligent manufacturing systems."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Image Stitching from Scratch\n\n\n\n\n\n\n\nImage Processing\n\n\n\n\nCustom Implemenation of the Image Stitching Algorithm from Scratch\n\n\n\n\n\n\nJan 26, 2024\n\n\nSoumyaratna Debnath\n\n\n\n\n\n\n  \n\n\n\n\nWiener Filter from Scratch\n\n\n\n\n\n\n\nImage Processing\n\n\n\n\nCustom Implemenation of the Wiener Filter from Scratch\n\n\n\n\n\n\nJan 26, 2024\n\n\nSoumyaratna Debnath\n\n\n\n\n\n\n  \n\n\n\n\nHistogram Matching from Scratch\n\n\n\n\n\n\n\nImage Processing\n\n\n\n\nCustom Implementation of the Histogram Matching Algorithm from Scratch\n\n\n\n\n\n\nOct 20, 2023\n\n\nSoumyaratna Debnath\n\n\n\n\n\n\n  \n\n\n\n\nNon Local Means Denoising from Scratch\n\n\n\n\n\n\n\nImage Processing\n\n\n\n\nCustom Implemenation of the Non-Local Means Denoising Algorithm from Scratch\n\n\n\n\n\n\nOct 20, 2023\n\n\nSoumyaratna Debnath\n\n\n\n\n\n\n  \n\n\n\n\nThe Magic of Ensemble Algorithms\n\n\n\n\n\n\n\nMachine Learning\n\n\nEnsemble Algorithms\n\n\n\n\nCustom Implementation of Bagging, ADABoost, Decission Tree, Weighted Decision Tree, Gradient Boosted Decision Tree and Random Forest Algorithms for solving some subjective questions.\n\n\n\n\n\n\nJun 3, 2023\n\n\nSoumyaratna Debnath\n\n\n\n\n\n\n  \n\n\n\n\nA Comparative Study Between Different VGG Models\n\n\n\n\n\n\n\nMachine Learning\n\n\nDeep Learning\n\n\nComputer Vision\n\n\n\n\nCustom Implementation of VGG 1 Block, VGG 3 Block, VGG 16 and MLP 18 for a detailed comparative study.\n\n\n\n\n\n\nApr 19, 2023\n\n\nSoumyaratna Debnath\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/The Magic of Ensemble Algorithms/index.html",
    "href": "projects/The Magic of Ensemble Algorithms/index.html",
    "title": "The Magic of Ensemble Algorithms",
    "section": "",
    "text": "GitHub \n\nQuestion 1\nShow plots for bias and variance vs increasing complexity (depth) of decision tree on the given regression dataset. You can use the decision tree implementation from assignment 1 (or sklearn tree).\nimport numpy as np\n\nnp.random.seed(1234)\nx = np.linspace(0, 10, 50)\neps = np.random.normal(0, 5, 50)\ny = x**2 +1 + eps\n\n#for plotting\nimport matplotlib.pyplot as plt\nplt.plot(x, y, 'o')\nplt.plot(x, x**2 + 1, 'r-')\nplt.show()\n\nSolution\nThe toy data used for training and testing is as follows -\n\nThe training and testing curve with respect to mean squared error and increasing complexity is as follows -\nTrain-test split = 70:30\n\nThe bias-varience vs depth curve is as follows -\n\n\nReference\nEstimator. (2023, February 7). In Wikipedia. https://en.wikipedia.org/wiki/Estimator\n\n\n\n\nQuestion 2\nShuffle the dataset and split the classification dataset into a training set (70%) and a test set (30%). Implement a weighted decision tree and train it using the training set. Use uniform(0,1) distribution to assign weights randomly to the samples. Plot and visualise the decision tree boundary. Use the test set to evaluate the performance of the weighted decision tree and compare your implementation with sklearn. You can copy your implementation of decision tree from assignment 1 to this repository and edit it to take sample weights as an argument while learning the decision tree(Default weight is 1 for each sample).\nfrom sklearn.datasets import make_classification\nX, y = make_classification(\nn_features=2, n_redundant=0, n_informative=2, \nrandom_state=1, n_clusters_per_class=2, class_sep=0.5)\n\n# For plotting\nimport matplotlib.pyplot as plt\nplt.scatter(X[:, 0], X[:, 1], c=y)\n\nSoultuion\nThe data used for training and testing looks like -\n\n\nCriterion = information_gain\nMetrics for Weighted Decision Tree\nTraining metrics\nAccuracy:  1.0\n\nClass:  1\n    Precision:  1.0\n    Recall:  1.0\n\nClass:  0\n    Precision:  1.0\n    Recall:  1.0\n\nTesting metrics\nAccuracy:  0.7666666666666667\n\nClass:  1\n    Precision:  0.6875\n    Recall:  0.8461538461538461\n\nClass:  0\n    Precision:  0.8571428571428571\n    Recall:  0.7058823529411765\n\nMetrics for Sklearn Implementation\nTraining metrics\nAccuracy:  1.0\n\nClass:  1\n    Precision:  1.0\n    Recall:  1.0\n\nClass:  0\n    Precision:  1.0\n    Recall:  1.0\n\nTesting metrics\nAccuracy:  0.7333333333333333\n\nClass:  1\n    Precision:  0.6666666666666666\n    Recall:  0.7692307692307693\n\nClass:  0\n    Precision:  0.8\n    Recall:  0.7058823529411765\n\nCriterion = gini_index\nMetrics for Weighted Decision Tree\nTraining metrics\nAccuracy:  1.0\n\nClass:  1\n    Precision:  1.0\n    Recall:  1.0\n\nClass:  0\n    Precision:  1.0\n    Recall:  1.0\n\nTesting metrics\nAccuracy:  0.8\n\nClass:  1\n    Precision:  0.7333333333333333\n    Recall:  0.8461538461538461\n\nClass:  0\n    Precision:  0.8666666666666667\n    Recall:  0.7647058823529411\n\nMetrics for Sklearn Implementation\nTraining metrics\nAccuracy:  1.0\n\nClass:  1\n    Precision:  1.0\n    Recall:  1.0\n\nClass:  0\n    Precision:  1.0\n    Recall:  1.0\n\nTesting metrics\nAccuracy:  0.8\n\nClass:  1\n    Precision:  0.7333333333333333\n    Recall:  0.8461538461538461\n\nClass:  0\n    Precision:  0.8666666666666667\n    Recall:  0.7647058823529411\n\n\n\n\nQuestion 3\nPart A\nImplement Adaboost on Decision Stump (depth=1 tree). You can use Decision Tree learnt in assignment 1 or sklearn decision tree and solve it for the case of real input and discrete output. Edit ensemble/ADABoost.py \n\nSolution\nThe dataset used for tarining looks like -\nNumber of samples = 100 Number of features = 2 Number of classes = 2 Numbeer of estimators = 3 Criteria = gini\n\nThe performace of the model can be comprehended as -\nCriteria  : gini\nAccuracy  : 0.99\n\nClass     : 0\nPrecision : 1.0\nRecall    : 0.98\n\nClass     : 1\nPrecision : 0.9803921568627451\nRecall    : 1.0\n\nPart B\nImplement AdaBoostClassifier on classification data set. Plot the decision surfaces and compare the accuracy of AdaBoostClassifier using 3 estimators over decision stump. Include your code in q3_ADABoost.py.\n\n\nSolution\nThe dataset used for tarining looks like -\nNumber of samples = 100 Number of features = 2 Number of classes = 2 Numbeer of estimators = 3 Criteria = gini\n\nDecision surfaces of the indivisual estimators -\n\nThe combined decision surface looks like -\n\n\n\n\nQuestion 4\nPart A Implement Bagging(BaseModel, num_estimators): where base model is the DecisionTree you had implemented in assignment 1 (or sklearn decision tree). In a later assignment, you would have to implement the above over LinearRegression() also, but for now you only have to implement it for Decision Trees. Edit ensemble/bagging.py. Use q4_Bagging.py for testing.\n\nSoultion\nThe toy dataset which is used for training the model is as follows -\nNumber of samples = 50 Number of features = 2 Number of classes = 2 Number of estimators = 5 Max depth of each estimator = 20\n\nThe decision boundries of each estimator is as follows -\n\nThe combined decision boundary is as follows -\n\nThe performce of the model can be estimated from the following results\nCriteria   : gini\nAccuracy   : 0.82\n\nClass      : 0\nPrecision  : 0.8181818181818182\nRecall     : 0.782608695652174\n\nClass      : 1\nPrecision  : 0.8214285714285714\nRecall     : 0.8518518518518519\n\nPart B Implement bagging in parallel fashion, where each decision tree is learnt simultaneously. Perform timing analysis for parallel implementation and normal implementation.\n\n\nSolution\nStudy 1\nThe toy dataset which is used for training both the model is as follows -\nNumber of samples = 2000 Number of features = 2 Number of classes = 4 Number of estimators = 20 Max depth of each estimator = 20\n\nThe performace and time required for tarining of the models can be compared from the following metrics -\nCriteria            : gini\nAccuracy Sequencial : 0.5385\nAccuracy Parallel   : 0.536\n\nTime elapsed in sequencial execution : 0.1579 seconds\nTime elapsed in parallel execution   : 0.1095 seconds\nStudy 2\nThe toy dataset which is used for training both the model is as follows -\nNumber of samples = 500 Number of features = 2 Number of classes = 4 Number of estimators = 50 Max depth of each estimator = 100\n\nThe performace and time required for tarining of the models can be compared from the following metrics -\nCriteria            : gini\nAccuracy Sequencial : 0.683\nAccuracy Parallel   : 0.678\n\nTime elapsed in sequencial execution : 0.9955 seconds\nTime elapsed in parallel execution   : 0.5297 seconds\nStudy 3\nThe toy dataset which is used for training both the model is as follows -\nNumber of samples = 10000 Number of features = 2 Number of classes = 6 Number of estimators = 500 Max depth of each estimator = 1000\n\nThe performace and time required for tarining of the models can be compared from the following metrics -\nCriteria            : gini\nAccuracy Sequencial : 0.457\nAccuracy Parallel   : 0.4561\n\nTime elapsed in sequencial execution : 32.3758 seconds\nTime elapsed in parallel execution   : 27.2234 seconds\n\n\n\nQuestion 5\nPart A\nImplement RandomForestClassifier() and RandomForestRegressor() classes in tree/randomForest.py. Use q5_RandomForest.py for testing.\n\nSolution\nRandom forest classifier over entropy as criterion returns the following trees as output -\nNumber of samples = 30 Number of features = 5 Sampling = row sampling\nMax Depth = 5 Number of estimators = 3 Number of classes = 5\nThe performace of the model can be quantified as -\nCriteria  : entropy\nAccuracy  : 0.8333333333333334\n\nClass 0\nPrecision : 0.7777777777777778\nRecall    : 0.7777777777777778\n\nClass 3\nPrecision : 0.875\nRecall    : 1.0\n\nClass 2\nPrecision : 1.0\nRecall    : 0.7142857142857143\n\nClass 4\nPrecision : 0.6\nRecall    : 0.75\n\nClass 1\nPrecision : 1.0\nRecall    : 1.0\n\nRandom forest classifier over gini as criterion returns the following trees as output -\nNumber of samples = 30 Number of features = 5 Sampling = row sampling\nMax Depth = 5 Number of estimators = 3 Number of classes = 5\n\nThe performace of the model can be quantified as -\nCriteria  : gini\nAccuracy  : 0.8333333333333334\n\nClass 0\nPrecision : 0.875\nRecall    : 0.7777777777777778\n\nClass 3\nPrecision : 1.0\nRecall    : 0.7142857142857143\n\nClass 2\nPrecision : 0.875\nRecall    : 1.0\n\nClass 4\nPrecision : 1.0\nRecall    : 0.75\n\nClass 1\nPrecision : 0.5\nRecall    : 1.0\n\nRandom forest classifier over squared_error as criterion returns the following trees as output -\nNumber of samples = 30 Number of features = 5 Sampling = row sampling\nMax Depth = 5 Number of estimators = 3\n\nThe performace of the model can be quantified as -\nCriteria : squared_error\nRMSE     : 0.5159266120836465\nMAE      : 0.364302808727380\n\nRandom forest classifier over squared_error as criterion returns the following trees as output -\nNumber of samples = 30 Number of features = 3 Sampling = column sampling\nMax Depth = 5 Number of estimators = 3\n\nThe performace of the model can be quantified as -\nCriteria : squared_error\nRMSE     : 40.52918401932447\nMAE      : 24.29257288596476\n\nPart B\nGenerate the plots for classification data set. Include you code in random_forest_classification.py\n\n\nSolution\nThe toy dataset used for tarining is as follows -\n\nRandom forest classifier over gini as criterion returns the following trees as output -\nNumber of samples = 100 Number of features = 3 Sampling = column sampling\nMax Depth = 5 Number of estimators = 3 Number of classes = 2\n\nThe decision surfaces for the corrosponding estimators is as follows -\n\nThe combined decision surface is as follows -\n\nThe performace of the model can be quantified as -\nCriteria  : gini\nAccuracy  : 0.91\n\nClass 1\nPrecision : 1.0\nRecall    : 0.88\n\nClass 0\nPrecision : 1.0\nRecall    : 0.94\n\n\n\nQuestion 6\nImplement Gradient Boosted Decision trees on the regression dataset given below. You need to edit the ensemble/gradientBoosted.py and q6_gradientBoosted.py\nfrom sklearn.datasets import make_regression\n\nX, y= make_regression(\n    n_features=3,\n    n_informative=3,\n    noise=10,\n    tail_strength=10,\n    random_state=42,\n)\n\n# For plotting\nimport matplotlib.pyplot as plt\nplt.scatter(X[:, 0], y)\n\nSolution\nThe rabdomly generated data for the training looks like -\n\nWe trained an ensamble with gradient boosted decision tree upon the given data with the following parameters -\nCriteria = squared_error Number of estimators = 20 Learning rate = 0.1 Max depth = 3\nThe performace of the model can be comprehended with the following results -\n\nCriteria : squared_error\nRMSE     : 8.74626791232833\nMAE      : 7.075693381159919"
  },
  {
    "objectID": "projects/Image Stitching/index.html",
    "href": "projects/Image Stitching/index.html",
    "title": "Image Stitching from Scratch",
    "section": "",
    "text": "GitHub \n\nImporting the Libraries and defining the helper functions\n\n# importing the libraries\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\n\n# function to open and image and resize it\ndef open_resize(img, w, h):\n    image = cv2.imread(img) # read the image\n    image = cv2.resize(image, (w, h)) # resize the image\n    return image # return the image\n\n# function to display an image\ndef show_image(img, w=0, h=0, axis=False, title=''):\n    if w == 0 or h == 0: plt.figure()\n    else: plt.figure(figsize=(w, h)) # set the size of the figure\n    plt.title(title) # set the title of the figure\n    plt.imshow(img) # show the image\n    if axis == False: plt.axis('off') # turn off the axis\n    plt.show() # show the image    \n\n# function to convert BGR to RGB\ndef BGR2RGB(img):\n    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # convert the image from BGR to RGB\n\n#### TEST THE FUNCTIONS ####\ntest = open_resize('Dataset/scene1/I11.jpg', 600, 400)\nshow_image(BGR2RGB(test), axis=True)\n\n\n\n\n\n\nStep 1 Detect, extract, and match features\nUsing SIFT detector to detect the features, and Brute-Force matcher to match the features. The result is shown below.\n\n# function to get the keypoints and descriptors\ndef get_keypoints_descriptors(img):\n    sift = cv2.SIFT_create() # create a SIFT object\n    kp, des = sift.detectAndCompute(img, None) # get the keypoints and descriptors\n    des = des.astype(np.uint8)\n    return kp, des # return the keypoints and descriptors\n\n# function to draw the keypoints\ndef draw_keypoints(img, kp):\n    return cv2.drawKeypoints(img, kp, None) # draw the keypoints\n\n# functio to get the src and dst points\ndef get_src_dst_points(image1, image2):\n    gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY) # convert the image to grayscale\n    gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY) # convert the image to grayscale\n    keypoints1, descriptors1 = get_keypoints_descriptors(gray1) # get the keypoints and descriptors\n    keypoints2, descriptors2 = get_keypoints_descriptors(gray2) # get the keypoints and descriptors\n    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True) # create a BFMatcher object\n    matches = bf.match(descriptors1, descriptors2) # get the matches\n    matches = sorted(matches, key=lambda x: x.distance) # sort the matches\n    # Draw the matches between the 2 images\n    matching_result = cv2.drawMatches(image1, keypoints1, image2, keypoints2, matches, outImg=None)\n    # get the coordinates of the matched points\n    src_pts = np.float32([keypoints1[m.queryIdx].pt for m in matches]).reshape(-1, 2)\n    dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in matches]).reshape(-1, 2)\n    return src_pts, dst_pts, matching_result # return the src and dst points and the matching result    \n\n\n#### TEST THE FUNCTIONS ####\nimage1 = open_resize('Dataset/scene1/I12.jpg', 600, 400)\nimage2 = open_resize('Dataset/scene1/I13.jpg', 600, 400)\nsrc_pts, dst_pts, matching_result = get_src_dst_points(image1, image2)\nshow_image(BGR2RGB(matching_result), w=10, h=5)\n\n\n\n\n\n\nStep 2 Estimate homography matrix between two images using RANSAC.\nGiven the source points and destination points compute_homography() function computes the homography matrix. The best homography matrix is selected by RANSAC algorithm, implemented as ransac_homography() function. The result is shown below.\n\n# function to compute the homography\ndef compute_homography(src_pts, dst_pts): \n    num_points = src_pts.shape[0] # get the number of points\n    A_matrix = [] # matrix A for homography calculation\n    for i in range(num_points):\n        src_x, src_y = src_pts[i, 0], src_pts[i, 1]\n        dst_x, dst_y = dst_pts[i, 0], dst_pts[i, 1]\n        # Constructing the rows of the A matrix\n        A_matrix.append([src_x, src_y, 1, 0, 0, 0, -dst_x * src_x, -dst_x * src_y, -dst_x])\n        A_matrix.append([0, 0, 0, src_x, src_y, 1, -dst_y * src_x, -dst_y * src_y, -dst_y])\n    A_matrix = np.asarray(A_matrix) # convert the matrix to numpy array\n    U, S, Vh = np.linalg.svd(A_matrix) # perform SVD\n    L = Vh[-1, :] / Vh[-1, -1] # calculate the homography\n    H = L.reshape(3, 3) # reshape the homography\n    return H # return the homography\n\n\n#### TEST THE FUNCTION ####\nimage1 = open_resize('Dataset/scene1/I11.jpg', 600, 400)\nimage2 = open_resize('Dataset/scene1/I12.jpg', 600, 400)\nsrc_pts, dst_pts, matching_result = get_src_dst_points(image1, image2)\nidx = np.random.choice(src_pts.shape[0], 4, replace=False) \nH = compute_homography(src_pts[idx], dst_pts[idx]) \nprint('Homography Matrix:')\nprint(H)\n\nHomography Matrix:\n[[ 3.91213361e-01 -1.56540855e-01 -7.96598341e+01]\n [-1.76497515e-01  3.67446603e-01  9.76646463e+01]\n [-7.92663451e-04 -7.74891393e-04  1.00000000e+00]]\n\n\n\n# RANSAC to find the best homography matrix\ndef ransac_homography(src_pts, dst_pts, n_iter=1000, threshold=0.5):\n    n = src_pts.shape[0] # get the number of points\n    best_H = None # initialize the best homography matrix\n    max_inliers = 0 # initialize the maximum number of inliers\n    for i in range(n_iter):\n        idx = np.random.choice(n, 4, replace=False) # randomly select 4 points\n        H = compute_homography(src_pts[idx], dst_pts[idx]) # compute the homography matrix\n        src_pts_hat = np.hstack((src_pts, np.ones((n, 1)))) # convert to homogeneous coordinates\n        dst_pts_hat = np.hstack((dst_pts, np.ones((n, 1)))) # convert to homogeneous coordinates\n        dst_pts_hat_hat = np.matmul(H, src_pts_hat.T).T # apply the homography matrix\n        dst_pts_hat_hat = dst_pts_hat_hat[:, :2] / dst_pts_hat_hat[:, 2:] # convert back to non-homogeneous coordinates\n        diff = np.linalg.norm(dst_pts_hat_hat - dst_pts, axis=1) # compute the difference between the predicted and actual coordinates\n        inliers = np.sum(diff &lt; threshold) # count the number of inliers\n        if inliers &gt; max_inliers: # update the best homography matrix\n            max_inliers = inliers \n            best_H = H\n    return best_H # return the best homography matrix\n\n\n#### TEST THE FUNCTION ####\nimage1 = open_resize('Dataset/scene1/I12.jpg', 600, 400)\nimage2 = open_resize('Dataset/scene1/I13.jpg', 600, 400)\nsrc_pts, dst_pts, matching_result = get_src_dst_points(image1, image2)\nbest_H = ransac_homography(src_pts, dst_pts, n_iter=10000, threshold=0.5)\nprint('Best Homography Matrix:')\nprint(best_H)\n\nBest Homography Matrix:\n[[ 1.10942139e+00  1.24788612e-03 -2.86918367e+02]\n [ 1.51381324e-02  1.07799975e+00 -1.47459874e+01]\n [ 1.83085652e-04 -3.87132426e-06  1.00000000e+00]]\n\n\n\n\nStep 3 Stitch color images of each scene from the dataset using the homography matrix estimated in step (2) to create a panorama.\n\nDefining the helper functions for stitching\n\n# function to align the images\ndef align_images(image, H, factor):\n    h, w, _ = image.shape # get the shape of image\n    _h, _w = factor*h, factor*w  # create a canvas that is 4 times larger than image\n    aligned_image = np.zeros((_h, _w, 3), dtype=np.uint8) # initialize the output image\n\n    # Loop through each pixel in the output image\n    for y in range(-h, _h-h):\n        for x in range(-w, _w-w):\n            # Apply the homography transformation to find the corresponding pixel in image\n            pt = np.dot(H, np.array([x, y, 1]))\n            pt = pt / pt[2]  # Normalize the coordinates\n            # Check if the transformed coordinates are within the bounds of image\n            if 0 &lt;= pt[0] &lt; image.shape[1] and 0 &lt;= pt[1] &lt; image.shape[0]:\n                # Interpolate the pixel color value using bilinear interpolation\n                x0, y0 = int(pt[0]), int(pt[1])\n                x1, y1 = x0 + 1, y0 + 1\n                alpha = pt[0] - x0\n                beta = pt[1] - y0\n                # Check if the neighboring pixels are within the bounds of image\n                if 0 &lt;= x0 &lt; image.shape[1] and 0 &lt;= x1 &lt; image.shape[1] and \\\n                   0 &lt;= y0 &lt; image.shape[0] and 0 &lt;= y1 &lt; image.shape[0]:\n                    # Bilinear interpolation\n                    interpolated_color = (1 - alpha) * (1 - beta) * image[y0, x0] + \\\n                                         alpha * (1 - beta) * image[y0, x1] + \\\n                                         (1 - alpha) * beta * image[y1, x0] + \\\n                                         alpha * beta * image[y1, x1]\n                    aligned_image[y+h, x+w] = interpolated_color.astype(np.uint8)  # Set the pixel value in the canvas as transformed pixel value         \n    return aligned_image, h, w # return the aligned image and canvas parameters\n\n# function to remove the black background\ndef remove_black_background(img):\n    mask = img.sum(axis=2) &gt; 0 # create a mask to remove black pixels\n    y, x = np.where(mask) # get the coordinates of the non-black pixels\n    x_min, x_max = x.min(), x.max() # get the minimum and maximum x coordinates\n    y_min, y_max = y.min(), y.max() # get the minimum and maximum y coordinates\n    img = img[y_min:y_max+1, x_min:x_max+1, :] # crop the image\n    return img # return the image\n\n# function to align the images\ndef get_transformed_images(img1, img2, H, focus=2, blend=True, factor=4, b_region=5):\n    h1, w1 = img1.shape[:2] # height and width of the first image\n    h2, w2 = img2.shape[:2] # height and width of the second image\n    corners1 = np.array([[0, 0], [0, h1], [w1, h1], [w1, 0]], dtype=np.float32) # corners of the first image\n    corners2 = np.array([[0, 0], [0, h2], [w2, h2], [w2, 0]], dtype=np.float32) # corners of the second image\n   \n    # coordinates of the four corners of the transformed image\n    corners2_transformed = cv2.perspectiveTransform(corners2.reshape(1, -1, 2), H).reshape(-1, 2)\n    # coordinates of the four corners of the new image\n    corners = np.concatenate((corners1, corners2_transformed), axis=0)\n    x_min, y_min = np.int32(corners.min(axis=0).ravel() - 0.5)\n    x_max, y_max = np.int32(corners.max(axis=0).ravel() + 0.5)\n    \n    T = np.array([[1, 0, -x_min], [0, 1, -y_min], [0, 0, 1]]) # translation matrix\n    H_inv = np.linalg.inv(T.dot(H)) # inverse of the homography matrix\n\n    if focus == 1: # wrap the second image into the first image\n        img_transformed, h_, w_ = align_images(img2, H, factor) # align the second image\n        img_res = img_transformed.copy() # copy the transformed image\n        img_res[h_ : h1 + h_, w_ : w1 + w_] = img1 # paste the first image\n\n        if blend == True: # blend around the edges\n            img_reg = img_res[h_ : h_ + h1 , -b_region + w_ : w_ + b_region] # right edge\n            img_res[h_:h_+h1 , -b_region + w_ : w_ + b_region] = cv2.GaussianBlur(img_reg, (3, 1), b_region, b_region)\n            img_reg = img_res[h_ : h_ + h1 , w2 - b_region + w_ : w_ + b_region + w2] # left edge\n            img_res[h_ : h_ + h1 , w2 - b_region + w_ : w_ + b_region + w2] =  cv2.GaussianBlur(img_reg, (3, 1), b_region, b_region)\n            img_reg = img_res[-b_region + h_ : h_ + b_region , w_ : w_ + w1] # top edge\n            img_res[-b_region + h_ : h_ + b_region , w_ : w_ + w1] =  cv2.GaussianBlur(img_reg, (1, 3), b_region, b_region)\n            img_reg = img_res[h2 - b_region + h_ : h_ + b_region  + h2, + w_ : w_ + w1] # bottom edge\n            img_res[h2 - b_region + h_ : h_ + b_region  + h2, w_ : w_ + w1] =  cv2.GaussianBlur(img_reg, (1, 3), b_region, b_region)\n\n    else: # wrap the first image into the second image\n        img_transformed, h_, w_ = align_images(img1, H_inv, factor) # align the first image\n        img_res = img_transformed.copy()  # copy the transformed image        \n        img_res[-y_min + h_ : h2 - y_min + h_, -x_min + w_ : w2 - x_min + w_] = img2 # paste the second image  \n        \n        if blend == True: # blend around the edges\n            img_reg = img_res[-y_min + h_ : h_ + h2 - y_min, -x_min - b_region + w_ : w_ + b_region - x_min] # right edge\n            img_res[-y_min + h_:h_+h2 - y_min, -x_min -b_region+ w_:w_+b_region - x_min] = cv2.GaussianBlur(img_reg, (3, 1), b_region, b_region)\n            img_reg = img_res[-y_min + h_ : h_ + h2 - y_min, -x_min + w2 - b_region+  w_ : w_ + b_region - x_min + w2] # left edge\n            img_res[-y_min + h_ : h_ + h2 - y_min, - x_min + w2 - b_region+ w_ : w_ + b_region - x_min + w2] =  cv2.GaussianBlur(img_reg, (3, 1), b_region, b_region)\n            img_reg = img_res[-y_min - b_region + h_ : h_ + b_region - y_min, -x_min + w_ : w_ + w2 - x_min] # top edge\n            img_res[-y_min - b_region + h_ : h_ + b_region - y_min, - x_min + w_ : w_ + w2 - x_min] =  cv2.GaussianBlur(img_reg, (1, 3), b_region, b_region)\n            img_reg = img_res[-y_min + h2 - b_region + h_ : h_ + b_region - y_min + h2, - x_min + w_ : w_ + w2 - x_min] # bottom edge\n            img_res[-y_min + h2 - b_region + h_ : h_ + b_region - y_min + h2, -x_min + w_ : w_ + w2 - x_min] =  cv2.GaussianBlur(img_reg, (1, 3), b_region, b_region)\n\n    return img_res # return the transformed image\n\n\n#### TEST THE FUNCTIONS ####\nimage1 = open_resize('Dataset/scene4/I41.jpg', 600, 400)\nimage2 = open_resize('Dataset/scene4/I42.jpg', 600, 400)\nsrc_pts, dst_pts, matching_result = get_src_dst_points(image1, image2)\nshow_image(BGR2RGB(matching_result), w=6, h=3)\nbest_H = ransac_homography(src_pts, dst_pts, n_iter=1000, threshold=0.5)\n\naligned_image_1 = get_transformed_images(image1, image2, best_H, focus=1)\naligned_image_1 = remove_black_background(BGR2RGB(aligned_image_1))\nshow_image(aligned_image_1, w=6, h=3)\n\naligned_image_2 = get_transformed_images(image1, image2, best_H, focus=2)\naligned_image_2 = remove_black_background(BGR2RGB(aligned_image_2))\nshow_image(aligned_image_2, w=6, h=3)\n\n\n\n\n\n\n\n\n\n\n\n\nScene 1\n\n# creating folder for scene1\nif not os.path.exists('Results/scene1'):\n    os.makedirs('Results/scene1')\n\n# operning the images\nimage1 = open_resize('Dataset/scene1/I11.jpg', 600, 400)\nimage2 = open_resize('Dataset/scene1/I12.jpg', 600, 400)\nimage3 = open_resize('Dataset/scene1/I13.jpg', 600, 400)\nimage4 = open_resize('Dataset/scene1/I14.jpg', 600, 400)\n\n\n# stitiching image1 and image2\nsrc_pts, dst_pts, matching_result = get_src_dst_points(image1, image2)\nshow_image(BGR2RGB(matching_result), w=6, h=3)\nbest_H = ransac_homography(src_pts, dst_pts, n_iter=10000, threshold=5)\naligned_image_1 = get_transformed_images(image1, image2, best_H, focus=2, blend=True)\naligned_image_1 = remove_black_background(BGR2RGB(aligned_image_1))\nshow_image(aligned_image_1, w=6, h=3)\n\n\n\n\n\n\n\n\n# stitiching image3 and image4 \nsrc_pts, dst_pts, matching_result = get_src_dst_points(image3, image4)\nshow_image(BGR2RGB(matching_result), w=6, h=3)\nbest_H = ransac_homography(src_pts, dst_pts, n_iter=10000, threshold=0.5)\naligned_image_2 = get_transformed_images(image3, image4, best_H, focus=1, blend=False)\naligned_image_2 = remove_black_background(BGR2RGB(aligned_image_2))\nshow_image(aligned_image_2, w=6, h=3)\n\n\n\n\n\n\n\n\n# stitiching aligned_image_1 and aligned_image_2\naligned_1 = BGR2RGB(aligned_image_1)\naligned_2 = BGR2RGB(aligned_image_2)\naligned_1 = cv2.resize(aligned_1, (800, 400))\naligned_2 = cv2.resize(aligned_2, (800, 400))\n\nsrc_pts, dst_pts, matching_result = get_src_dst_points(aligned_1, aligned_2)\nshow_image(BGR2RGB(matching_result), w=12, h=6)\nbest_H = ransac_homography(src_pts, dst_pts, n_iter=10000, threshold=0.5)\naligned_image_3 = get_transformed_images(aligned_1, aligned_2, best_H, focus=2)\naligned_image_3 = remove_black_background(BGR2RGB(aligned_image_3))\nshow_image(aligned_image_3, w=12, h=6)\n\n# saving the stitched image\naligned_image_3 = BGR2RGB(aligned_image_3)\ncv2.imwrite('Results/scene1/scene1.jpg', aligned_image_3)\n\n\n\n\n\n\n\nTrue\n\n\n\n\nScene 2\n\n# creating folder for scene1\nif not os.path.exists('Results/scene2'):\n    os.makedirs('Results/scene2')\n\n# operning the images\nimage1 = open_resize('Dataset/scene2/I21.jpg', 600, 400)\nimage2 = open_resize('Dataset/scene2/I22.jpg', 600, 400)\nimage3 = open_resize('Dataset/scene2/I23.jpg', 600, 400)\nimage4 = open_resize('Dataset/scene2/I24.jpg', 600, 400)\n\n\n# stitiching image1 and image2\nsrc_pts, dst_pts, matching_result = get_src_dst_points(image1, image2)\nshow_image(BGR2RGB(matching_result), w=6, h=3)\nbest_H = ransac_homography(src_pts, dst_pts, n_iter=1000, threshold=0.5)\naligned_image_1 = get_transformed_images(image1, image2, best_H, focus=1, blend=True)\naligned_image_1 = remove_black_background(BGR2RGB(aligned_image_1))\nshow_image(aligned_image_1, w=6, h=3)\n\n\n\n\n\n\n\n\n# stitiching aligned_image_1 and image3\naligned_1 = BGR2RGB(aligned_image_1)\naligned_1 = cv2.resize(aligned_1, (800, 400))\n\nsrc_pts, dst_pts, matching_result = get_src_dst_points(aligned_1, image3)\nshow_image(BGR2RGB(matching_result), w=10, h=5)\nbest_H = ransac_homography(src_pts, dst_pts, n_iter=10000, threshold=5)\naligned_image_2 = get_transformed_images(aligned_1, image3, best_H, focus=2, blend=False)\naligned_image_2 = remove_black_background(BGR2RGB(aligned_image_2))\nshow_image(aligned_image_2, w=10, h=5)\n\n\n\n\n\n\n\n\n# stitiching aligned_image_2 and image4\naligned_2 = BGR2RGB(aligned_image_2)\naligned_2 = cv2.resize(aligned_2, (1000, 400))\n\nsrc_pts, dst_pts, matching_result = get_src_dst_points(aligned_2, image4)\nshow_image(BGR2RGB(matching_result), w=12, h=6)\nbest_H = ransac_homography(src_pts, dst_pts, n_iter=10000, threshold=0.5)\naligned_image_3 = get_transformed_images(aligned_2, image4, best_H, focus=1, blend=True)\naligned_image_3 = remove_black_background(BGR2RGB(aligned_image_3))\nshow_image(aligned_image_3, w=12, h=6)\n\n# saving the stitched image\naligned_image_3 = BGR2RGB(aligned_image_3)\ncv2.imwrite('Results/scene2/scene2.jpg', aligned_image_3)\n\n\n\n\n\n\n\nTrue\n\n\n\n\nScene 3\n\n# creating folder for scene1\nif not os.path.exists('Results/scene3'):\n    os.makedirs('Results/scene3')\n\n# operning the images\nimage1 = open_resize('Dataset/scene3/I31.jpg', 600, 400)\nimage2 = open_resize('Dataset/scene3/I32.jpg', 600, 400)\nimage3 = open_resize('Dataset/scene3/I33.jpg', 600, 400)\nimage4 = open_resize('Dataset/scene3/I34.jpg', 600, 400)\n\n\n# stitiching image1 and image2\nsrc_pts, dst_pts, matching_result = get_src_dst_points(image1, image2)\nshow_image(BGR2RGB(matching_result), w=6, h=3)\nbest_H = ransac_homography(src_pts, dst_pts, n_iter=1000, threshold=0.5)\naligned_image_1 = get_transformed_images(image1, image2, best_H, focus=2, blend=True)\naligned_image_1 = remove_black_background(BGR2RGB(aligned_image_1))\nshow_image(aligned_image_1, w=6, h=3)\n\n\n\n\n\n\n\n\n# stitching aligned_image_1 and image3\naligned_1 = BGR2RGB(aligned_image_1)\naligned_1 = cv2.resize(aligned_1, (800, 400))\n\nsrc_pts, dst_pts, matching_result = get_src_dst_points(aligned_1, image3)\nshow_image(BGR2RGB(matching_result), w=10, h=5)\nbest_H = ransac_homography(src_pts, dst_pts, n_iter=10000, threshold=0.5)\naligned_image_2 = get_transformed_images(aligned_1, image3, best_H, focus=2, blend=False)\naligned_image_2 = remove_black_background(BGR2RGB(aligned_image_2))\nshow_image(aligned_image_2, w=10, h=5)\n\n\n\n\n\n\n\n\n# stitching aligned_image_2 and image4\naligned_2 = BGR2RGB(aligned_image_2)\naligned_2 = cv2.resize(aligned_2, (1000, 400))\n\nsrc_pts, dst_pts, matching_result = get_src_dst_points(aligned_2, image4)\nshow_image(BGR2RGB(matching_result), w=12, h=6)\nbest_H = ransac_homography(src_pts, dst_pts, n_iter=10000, threshold=0.5)\naligned_image_3 = get_transformed_images(aligned_2, image4, best_H, focus=1, blend=False)\naligned_image_3 = remove_black_background(BGR2RGB(aligned_image_3))\nshow_image(aligned_image_3, w=12, h=6)\n\n# saving the stitched image\naligned_image_3 = BGR2RGB(aligned_image_3)\ncv2.imwrite('Results/scene3/scene3.jpg', aligned_image_3)\n\n\n\n\n\n\n\nTrue\n\n\n\n\nScene 4\n\n# creating folder for scene1\nif not os.path.exists('Results/scene4'):\n    os.makedirs('Results/scene4')\n\n# operning the images\nimage1 = open_resize('Dataset/scene4/I41.jpg', 600, 400)\nimage2 = open_resize('Dataset/scene4/I42.jpg', 600, 400)\n\n\n# stitiching image1 and image2\nsrc_pts, dst_pts, matching_result = get_src_dst_points(image1, image2)\nshow_image(BGR2RGB(matching_result), w=10, h=5)\nbest_H = ransac_homography(src_pts, dst_pts, n_iter=10000, threshold=0.5)\naligned_image_1 = get_transformed_images(image1, image2, best_H, focus=2, blend=True)\naligned_image_1 = remove_black_background(BGR2RGB(aligned_image_1))\nshow_image(aligned_image_1, w=10, h=10)\n\n# saving the stitched image\naligned_image_1 = BGR2RGB(aligned_image_1)\ncv2.imwrite('Results/scene4/scene4.jpg', aligned_image_1)\n\n\n\n\n\n\n\nTrue\n\n\n\n\nScene 5\n\n# creating folder for scene1\nif not os.path.exists('Results/scene5'):\n    os.makedirs('Results/scene5')\n\n# operning the images\nimage1 = open_resize('Dataset/scene5/I51.jpg', 600, 400)\nimage2 = open_resize('Dataset/scene5/I52.jpg', 600, 400)\n\n\n# stitiching image1 and image2\nsrc_pts, dst_pts, matching_result = get_src_dst_points(image1, image2)\nshow_image(BGR2RGB(matching_result), w=10, h=5)\nbest_H = ransac_homography(src_pts, dst_pts, n_iter=10000, threshold=0.5)\naligned_image_1 = get_transformed_images(image1, image2, best_H, focus=2, blend=True)\naligned_image_1 = remove_black_background(BGR2RGB(aligned_image_1))\nshow_image(aligned_image_1, w=10, h=8)\n\n# saving the stitched image\naligned_image_1 = BGR2RGB(aligned_image_1)\ncv2.imwrite('Results/scene5/scene5.jpg', aligned_image_1)\n\n\n\n\n\n\n\nTrue\n\n\n\n\nScene 6\n\n# creating folder for scene1\nif not os.path.exists('Results/scene6'):\n    os.makedirs('Results/scene6')\n\n# operning the images\nimage1 = open_resize('Dataset/scene6/I61.jpg', 600, 400)\nimage2 = open_resize('Dataset/scene6/I62.jpg', 600, 400)\n\n\n# stitiching image1 and image2\nsrc_pts, dst_pts, matching_result = get_src_dst_points(image1, image2)\nshow_image(BGR2RGB(matching_result), w=10, h=5)\nbest_H = ransac_homography(src_pts, dst_pts, n_iter=10000, threshold=5)\naligned_image_1 = get_transformed_images(image1, image2, best_H, focus=2, blend=True)\naligned_image_1 = remove_black_background(BGR2RGB(aligned_image_1))\nshow_image(aligned_image_1, w=10, h=8)\n\n# saving the stitched image\naligned_image_1 = BGR2RGB(aligned_image_1)\ncv2.imwrite('Results/scene6/scene6.jpg', aligned_image_1)\n\n\n\n\n\n\n\nTrue\n\n\n\n\n\nStep 4 Stitch the images used as input in step (3) using in-built command for homography estimation and compare it with the panorama obtained in step (3).\n\nHelper function\n\n# function for finding the best homography matrix using cv2 library\ndef cv2_get_RANSAC_Homography(src_pts, dst_pts, threshold=5):\n    H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, threshold) # find the best homography matrix\n    return H # return the best homography matrix\n\n\n\nScene 1\n\n# operning the images\nimage1 = open_resize('Dataset/scene1/I11.jpg', 600, 400)\nimage2 = open_resize('Dataset/scene1/I12.jpg', 600, 400)\nimage3 = open_resize('Dataset/scene1/I13.jpg', 600, 400)\nimage4 = open_resize('Dataset/scene1/I14.jpg', 600, 400)\n\n\n# stitiching image1 and image2\nsrc_pts, dst_pts, matching_result = get_src_dst_points(image1, image2)\nbest_H = cv2_get_RANSAC_Homography(src_pts, dst_pts, threshold=5)\naligned_image_1 = get_transformed_images(image1, image2, best_H, focus=2, blend=True)\naligned_image_1 = remove_black_background(BGR2RGB(aligned_image_1))\nshow_image(aligned_image_1, w=6, h=3)\n\n\n\n\n\n# stitiching image3 and image4 \nsrc_pts, dst_pts, matching_result = get_src_dst_points(image3, image4)\nbest_H = cv2_get_RANSAC_Homography(src_pts, dst_pts, threshold=0.5)\naligned_image_2 = get_transformed_images(image3, image4, best_H, focus=1, blend=False)\naligned_image_2 = remove_black_background(BGR2RGB(aligned_image_2))\nshow_image(aligned_image_2, w=6, h=3)\n\n\n\n\n\n# stitiching aligned_image_1 and aligned_image_2\naligned_1 = BGR2RGB(aligned_image_1)\naligned_2 = BGR2RGB(aligned_image_2)\naligned_1 = cv2.resize(aligned_1, (800, 400))\naligned_2 = cv2.resize(aligned_2, (800, 400))\n\nsrc_pts, dst_pts, matching_result = get_src_dst_points(aligned_1, aligned_2)\nbest_H = cv2_get_RANSAC_Homography(src_pts, dst_pts, threshold=0.5)\naligned_image_3 = get_transformed_images(aligned_1, aligned_2, best_H, focus=2)\naligned_image_3 = remove_black_background(BGR2RGB(aligned_image_3))\nshow_image(aligned_image_3, w=12, h=6)\n\n# saving the stitched image\naligned_image_3 = BGR2RGB(aligned_image_3)\ncv2.imwrite('Results/scene1/scene1_cv2.jpg', aligned_image_3)\n\n\n\n\nTrue\n\n\nComparing the results of the two methods\n\ncustom = cv2.imread('Results/scene1/scene1.jpg')\nshow_image(BGR2RGB(custom), w=12, h=6, title='Custom Homography and RANSAC')\n\ninbuilt = cv2.imread('Results/scene1/scene1_cv2.jpg')\nshow_image(BGR2RGB(inbuilt), w=12, h=6, title='Inbuilt Homography and RANSAC')\n\n\n\n\n\n\n\nInference\n\nThe image obtained through a custom implementation of homography computation and RANSAC looks similar to the image obtained through the built-in function for homography estimation.\n\n\nScene 2\n\n# operning the images\nimage1 = open_resize('Dataset/scene2/I21.jpg', 600, 400)\nimage2 = open_resize('Dataset/scene2/I22.jpg', 600, 400)\nimage3 = open_resize('Dataset/scene2/I23.jpg', 600, 400)\nimage4 = open_resize('Dataset/scene2/I24.jpg', 600, 400)\n\n\n# stitiching image1 and image2\nsrc_pts, dst_pts, matching_result = get_src_dst_points(image1, image2)\nbest_H = cv2_get_RANSAC_Homography(src_pts, dst_pts, threshold=0.5)\naligned_image_1 = get_transformed_images(image1, image2, best_H, focus=1, blend=True)\naligned_image_1 = remove_black_background(BGR2RGB(aligned_image_1))\nshow_image(aligned_image_1, w=6, h=3)\n\n\n\n\n\n# stitiching aligned_image_1 and image3\naligned_1 = BGR2RGB(aligned_image_1)\naligned_1 = cv2.resize(aligned_1, (800, 400))\n\nsrc_pts, dst_pts, matching_result = get_src_dst_points(aligned_1, image3)\nbest_H = cv2_get_RANSAC_Homography(src_pts, dst_pts, threshold=5)\naligned_image_2 = get_transformed_images(aligned_1, image3, best_H, focus=2, blend=False)\naligned_image_2 = remove_black_background(BGR2RGB(aligned_image_2))\nshow_image(aligned_image_2, w=10, h=5)\n\n\n\n\n\n# stitiching aligned_image_2 and image4\naligned_2 = BGR2RGB(aligned_image_2)\naligned_2 = cv2.resize(aligned_2, (1000, 400))\n\nsrc_pts, dst_pts, matching_result = get_src_dst_points(aligned_2, image4)\nbest_H = cv2_get_RANSAC_Homography(src_pts, dst_pts, threshold=0.5)\naligned_image_3 = get_transformed_images(aligned_2, image4, best_H, focus=1, blend=True)\naligned_image_3 = remove_black_background(BGR2RGB(aligned_image_3))\nshow_image(aligned_image_3, w=12, h=6)\n\n# saving the stitched image\naligned_image_3 = BGR2RGB(aligned_image_3)\ncv2.imwrite('Results/scene2/scene2_cv2.jpg', aligned_image_3)\n\n\n\n\nTrue\n\n\nComparing the results of the two methods\n\ncustom = cv2.imread('Results/scene2/scene2.jpg')\nshow_image(BGR2RGB(custom), w=12, h=6, title='Custom Homography and RANSAC')\n\ninbuilt = cv2.imread('Results/scene2/scene2_cv2.jpg')\nshow_image(BGR2RGB(inbuilt), w=12, h=6, title='Inbuilt Homography and RANSAC')\n\n\n\n\n\n\n\nInference\n\nThe image obtained through a custom implementation of homography computation and RANSAC looks similar to the image obtained through the built-in function for homography estimation.\n\n\nScene 3\n\n# operning the images\nimage1 = open_resize('Dataset/scene3/I31.jpg', 600, 400)\nimage2 = open_resize('Dataset/scene3/I32.jpg', 600, 400)\nimage3 = open_resize('Dataset/scene3/I33.jpg', 600, 400)\nimage4 = open_resize('Dataset/scene3/I34.jpg', 600, 400)\n\n\n# stitiching image1 and image2\nsrc_pts, dst_pts, matching_result = get_src_dst_points(image1, image2)\nbest_H = cv2_get_RANSAC_Homography(src_pts, dst_pts, threshold=0.5)\naligned_image_1 = get_transformed_images(image1, image2, best_H, focus=2, blend=True)\naligned_image_1 = remove_black_background(BGR2RGB(aligned_image_1))\nshow_image(aligned_image_1, w=6, h=3)\n\n\n\n\n\n# stitching aligned_image_1 and image3\naligned_1 = BGR2RGB(aligned_image_1)\naligned_1 = cv2.resize(aligned_1, (800, 400))\n\nsrc_pts, dst_pts, matching_result = get_src_dst_points(aligned_1, image3)\nbest_H = cv2_get_RANSAC_Homography(src_pts, dst_pts, threshold=2)\naligned_image_2 = get_transformed_images(aligned_1, image3, best_H, focus=2, blend=False)\naligned_image_2 = remove_black_background(BGR2RGB(aligned_image_2))\nshow_image(aligned_image_2, w=10, h=5)\n\n\n\n\n\n# stitching aligned_image_2 and image4\naligned_2 = BGR2RGB(aligned_image_2)\naligned_2 = cv2.resize(aligned_2, (1000, 400))\n\nsrc_pts, dst_pts, matching_result = get_src_dst_points(aligned_2, image4)\nbest_H = cv2_get_RANSAC_Homography(src_pts, dst_pts, threshold=0.5)\naligned_image_3 = get_transformed_images(aligned_2, image4, best_H, focus=1, blend=False)\naligned_image_3 = remove_black_background(BGR2RGB(aligned_image_3))\nshow_image(aligned_image_3, w=12, h=6)\n\n# saving the stitched image\naligned_image_3 = BGR2RGB(aligned_image_3)\ncv2.imwrite('Results/scene3/scene3_cv2.jpg', aligned_image_3)\n\n\n\n\nTrue\n\n\nComparing the results of the two methods\n\ncustom = cv2.imread('Results/scene3/scene3.jpg')\nshow_image(BGR2RGB(custom), w=12, h=6, title='Custom Homography and RANSAC')\n\ninbuilt = cv2.imread('Results/scene3/scene3_cv2.jpg')\nshow_image(BGR2RGB(inbuilt), w=12, h=6, title='Inbuilt Homography and RANSAC')\n\n\n\n\n\n\n\nInference\n\nThe image obtained through a custom implementation of homography computation and RANSAC looks similar to the image obtained through the built-in function for homography estimation.\n\n\nScene 4\n\n# operning the images\nimage1 = open_resize('Dataset/scene4/I41.jpg', 600, 400)\nimage2 = open_resize('Dataset/scene4/I42.jpg', 600, 400)\n\n\n# stitiching image1 and image2\nsrc_pts, dst_pts, matching_result = get_src_dst_points(image1, image2)\nbest_H = cv2_get_RANSAC_Homography(src_pts, dst_pts, threshold=0.5)\naligned_image_1 = get_transformed_images(image1, image2, best_H, focus=2, blend=True)\naligned_image_1 = remove_black_background(BGR2RGB(aligned_image_1))\nshow_image(aligned_image_1, w=10, h=10)\n\n# saving the stitched image\naligned_image_1 = BGR2RGB(aligned_image_1)\ncv2.imwrite('Results/scene4/scene4_cv2.jpg', aligned_image_1)\n\n\n\n\nTrue\n\n\nComparing the results of the two methods\n\ncustom = cv2.imread('Results/scene4/scene4.jpg')\nshow_image(BGR2RGB(custom), w=12, h=6, title='Custom Homography and RANSAC')\n\ninbuilt = cv2.imread('Results/scene4/scene4_cv2.jpg')\nshow_image(BGR2RGB(inbuilt), w=12, h=6, title='Inbuilt Homography and RANSAC')\n\n\n\n\n\n\n\nInference\n\nThe image obtained through a custom implementation of homography computation and RANSAC looks similar to the image obtained through the built-in function for homography estimation.\n\n\nScene 5\n\n# operning the images\nimage1 = open_resize('Dataset/scene5/I51.jpg', 600, 400)\nimage2 = open_resize('Dataset/scene5/I52.jpg', 600, 400)\n\n\n# stitiching image1 and image2\nsrc_pts, dst_pts, matching_result = get_src_dst_points(image1, image2)\nbest_H = cv2_get_RANSAC_Homography(src_pts, dst_pts, threshold=0.5)\naligned_image_1 = get_transformed_images(image1, image2, best_H, focus=2, blend=True)\naligned_image_1 = remove_black_background(BGR2RGB(aligned_image_1))\nshow_image(aligned_image_1, w=10, h=8)\n\n# saving the stitched image\naligned_image_1 = BGR2RGB(aligned_image_1)\ncv2.imwrite('Results/scene5/scene5_cv2.jpg', aligned_image_1)\n\n\n\n\nTrue\n\n\nComparing the results of the two methods\n\ncustom = cv2.imread('Results/scene5/scene5.jpg')\nshow_image(BGR2RGB(custom), w=12, h=6, title='Custom Homography and RANSAC')\n\ninbuilt = cv2.imread('Results/scene5/scene5_cv2.jpg')\nshow_image(BGR2RGB(inbuilt), w=12, h=6, title='Inbuilt Homography and RANSAC')\n\n\n\n\n\n\n\nInference\n\nThe image obtained through a custom implementation of homography computation and RANSAC looks similar to the image obtained through the built-in function for homography estimation.\n\n\nScene 6\n\n# operning the images\nimage1 = open_resize('Dataset/scene6/I61.jpg', 600, 400)\nimage2 = open_resize('Dataset/scene6/I62.jpg', 600, 400)\n\n\n# stitiching image1 and image2\nsrc_pts, dst_pts, matching_result = get_src_dst_points(image1, image2)\nbest_H = cv2_get_RANSAC_Homography(src_pts, dst_pts, threshold=5)\naligned_image_1 = get_transformed_images(image1, image2, best_H, focus=2, blend=True)\naligned_image_1 = remove_black_background(BGR2RGB(aligned_image_1))\nshow_image(aligned_image_1, w=10, h=8)\n\n# saving the stitched image\naligned_image_1 = BGR2RGB(aligned_image_1)\ncv2.imwrite('Results/scene6/scene6_cv2.jpg', aligned_image_1)\n\n\n\n\nTrue\n\n\nComparing the results of the two methods\n\ncustom = cv2.imread('Results/scene6/scene6.jpg')\nshow_image(BGR2RGB(custom), w=12, h=6, title='Custom Homography and RANSAC')\n\ninbuilt = cv2.imread('Results/scene6/scene6_cv2.jpg')\nshow_image(BGR2RGB(inbuilt), w=12, h=6, title='Inbuilt Homography and RANSAC')\n\n\n\n\n\n\n\nInference\n\nThe image obtained through a custom implementation of homography computation and RANSAC looks similar to the image obtained through the built-in function for homography estimation.\n\n\n\nReferences\n\nImage stitching, https://en.wikipedia.org/wiki/Image_stitching\nPerspective Transformation, https://www.tutorialspoint.com/dip/perspective_transformation.htm\nFirst Principles of Computer Vision, Columbia Engineering, https://fpcv.cs.columbia.edu\nFeature Based Panoramic Image Stitching, https://in.mathworks.com/help/vision/ug/feature-based-panoramic-image-stitching.html"
  },
  {
    "objectID": "projects/A Comparative Study Between Different VGG Models/index.html",
    "href": "projects/A Comparative Study Between Different VGG Models/index.html",
    "title": "A Comparative Study Between Different VGG Models",
    "section": "",
    "text": "GitHub \n\nCode\n\n# Importing the required libraries\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\nfrom keras.callbacks import TensorBoard\nimport time\nimport csv\nimport os\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport shutil\n\n2023-04-19 14:37:07.771981: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2023-04-19 14:37:07.807696: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2023-04-19 14:37:07.808495: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-04-19 14:37:08.701505: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n\n\n\nDriver Code\n\nprint('Are you sure want to override previous report ', end='')\ncommand = input()\n\nif command == 'Yes':\n    print('Overriding previous report')\n    # Open the results file in write mode and adding the head\n    with open('results.csv', mode='w', newline='') as results_file:\n        results_writer = csv.writer(results_file)\n        results_writer.writerow(['Model Name', 'Training Time', 'Train Loss', 'Train Acc', 'Test Acc', 'Num Params'])\n    \n    folders_to_remove = [\"saved_models\", \"log_images\", \"log_stats\"]\n    for folder in folders_to_remove:\n        if os.path.exists(folder):\n            shutil.rmtree(folder)\n            print(f\"Directory {folder} removed.\")\n        else:\n            print(f\"Directory {folder} does not exist.\")\n\nAre you sure want to override previous report  Yes\nOverriding previous report\nDirectory saved_models does not exist.\nDirectory log_images does not exist.\nDirectory log_stats does not exist.\n\n\n\n# Define directories for training and testing data\ntrain_dir = 'dataset/train/'\ntest_dir = 'dataset/test/'\n\n# Define the image size to be used for resizing the images\nimg_size = (128, 128)\n\n# Define the input image size (including the number of color channels)\ninput_img_size = (128, 128, 3)\n\n# Define the batch size for training the model\nbatch_size = 20\n\n# Define the number of epochs for training the model\nnum_epochs = 20\n\n\n# Create an ImageDataGenerator object for data augmentation and normalization of training data\ntrain_datagen = ImageDataGenerator(rescale=1./255)\n\n# Create a generator for loading training data from the directory\ntrain_generator = train_datagen.flow_from_directory(train_dir, \n                                                    target_size=img_size, # Resizes the images to a target size\n                                                    batch_size=batch_size, # Defines the batch size\n                                                    class_mode='binary') # Defines the type of labels to use\n\n# Create an ImageDataGenerator object for normalization of testing data\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\n# Create a generator for loading testing data from the directory\ntest_generator = test_datagen.flow_from_directory(test_dir, \n                                                  target_size=img_size, # Resizes the images to a target size\n                                                  batch_size=batch_size, # Defines the batch size\n                                                  class_mode='binary') # Defines the type of labels to use\n\n# Data generators for prediction\nprediction_datagen = ImageDataGenerator(rescale=1./255)\npreprocess_input = tf.keras.applications.vgg16.preprocess_input\nprediction_datagen_vgg = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)\nprediction_generator = test_datagen.flow_from_directory(test_dir, target_size=img_size, batch_size=1, class_mode='binary', shuffle=False) \nprediction_generator_vgg = prediction_datagen_vgg.flow_from_directory(test_dir, target_size=img_size, batch_size=1, class_mode='binary', shuffle=False) \n\nFound 160 images belonging to 2 classes.\nFound 40 images belonging to 2 classes.\nFound 40 images belonging to 2 classes.\nFound 40 images belonging to 2 classes.\n\n\n\n# Function for plotting the predictions and writing to TensorBoard\ndef plot_predictions(title, model, log_dir, test_generator):\n    # Create a summary writer for TensorBoard\n    writer = tf.summary.create_file_writer(log_dir)\n\n    # Get the predicted classes for the test set\n    y_pred = model.predict(test_generator)\n    y_pred_classes = tf.round(y_pred).numpy().astype(int).flatten()\n\n    # Get the true classes for the test set\n    y_true = test_generator.classes.astype(int)\n\n    # Get the class labels for the dataset\n    class_labels = list(test_generator.class_indices.keys())\n\n    # Get all the images and their corresponding labels from the test set generator\n    images = []\n    labels = []\n    for i in range(len(test_generator)):\n        batch = test_generator[i]\n        images.extend(batch[0])\n        labels.extend(batch[1].astype(int))\n\n    for i in range(len(test_generator)):\n        # Write the image to TensorBoard\n        with tf.summary.create_file_writer(log_dir).as_default():\n            tf.summary.image(\"{}   Image {}   Predicted: {}   True: {}\".format(title, i+1, class_labels[y_pred_classes[i]], class_labels[labels[i]]), np.expand_dims(images[i], 0), step=0)\n\n\n\nVGG 1 Block\n\n# Define a function that creates a VGG block with one convolutional layer\ndef vgg_1_block():\n    # Create a Sequential model object with a name\n    model = Sequential(name = 'vgg_block_1')\n    \n    # Add a convolutional layer with 64 filters, a 3x3 kernel size, 'relu' activation, and 'same' padding\n    model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=input_img_size))\n    \n    # Add a max pooling layer with a 2x2 pool size\n    model.add(MaxPooling2D((2, 2)))\n    \n    # Add a flatten layer to convert the 2D feature maps to a 1D feature vector\n    model.add(Flatten())\n    \n    # Add a fully connected layer with 128 units and 'relu' activation\n    model.add(Dense(128, activation='relu'))\n    \n    # Add an output layer with 1 unit and 'sigmoid' activation (for binary classification)\n    model.add(Dense(1, activation='sigmoid'))\n    \n    # Return the model\n    return model\n\n# Create a VGG block with one convolutional layer\nmodel1 = vgg_1_block()\n\n# Print a summary of the model's architecture\nmodel1.summary()\n\n# Define a log directory for TensorBoard\nlog_dir = 'log_stats/vgg_1_block'\n\n# Define the TensorBoard callback with update_freq='batch'\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq='batch')\n\nModel: \"vgg_block_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 128, 128, 64)      1792      \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 64, 64, 64)       0         \n )                                                               \n                                                                 \n flatten (Flatten)           (None, 262144)            0         \n                                                                 \n dense (Dense)               (None, 128)               33554560  \n                                                                 \n dense_1 (Dense)             (None, 1)                 129       \n                                                                 \n=================================================================\nTotal params: 33,556,481\nTrainable params: 33,556,481\nNon-trainable params: 0\n_________________________________________________________________\n\n\n2023-04-19 14:38:27.877910: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\nSkipping registering GPU devices...\n\n\n\n# Compile the model with 'adam' optimizer, 'binary_crossentropy' loss function, and 'accuracy' metric\nmodel1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Start timing the training\nstart_time = time.time()\n\n# Train the model using the training generator for a specified number of epochs, and save the history\nhistory = model1.fit(train_generator, steps_per_epoch=len(train_generator), epochs=num_epochs, callbacks=[tensorboard_callback])\n\n# Stop timing the training\nend_time = time.time()\n\n# Calculate the training time by subtracting the start time from the end time\ntraining_time = end_time - start_time\n\n# Evaluate the model on the training set and get the training loss and accuracy\ntrain_loss, train_acc = model1.evaluate(train_generator)\n\n# Evaluate the model on the test set and get the test loss and accuracy\ntest_loss, test_acc = model1.evaluate(test_generator)\n\n# Count the number of parameters in the model\nnum_params = model1.count_params()\n\n# Open the results file in append mode and writing the results\nwith open('results.csv', mode='a', newline='') as results_file:\n    results_writer = csv.writer(results_file)\n    results_writer.writerow(['VGG 1 Block', training_time, train_loss, train_acc, test_acc, num_params])\n    \nmodel1.save('saved_models/vgg_1_block.h5')\n\nEpoch 1/20\n8/8 [==============================] - 4s 443ms/step - loss: 16.0090 - accuracy: 0.4938\nEpoch 2/20\n8/8 [==============================] - 4s 441ms/step - loss: 1.3641 - accuracy: 0.5063\nEpoch 3/20\n8/8 [==============================] - 4s 440ms/step - loss: 0.8025 - accuracy: 0.6000\nEpoch 4/20\n8/8 [==============================] - 4s 439ms/step - loss: 0.5900 - accuracy: 0.6875\nEpoch 5/20\n8/8 [==============================] - 4s 439ms/step - loss: 0.4382 - accuracy: 0.7750\nEpoch 6/20\n8/8 [==============================] - 4s 441ms/step - loss: 0.3841 - accuracy: 0.8687\nEpoch 7/20\n8/8 [==============================] - 4s 440ms/step - loss: 0.2853 - accuracy: 0.8875\nEpoch 8/20\n8/8 [==============================] - 4s 440ms/step - loss: 0.2322 - accuracy: 0.9438\nEpoch 9/20\n8/8 [==============================] - 4s 442ms/step - loss: 0.1883 - accuracy: 0.9563\nEpoch 10/20\n8/8 [==============================] - 4s 440ms/step - loss: 0.1287 - accuracy: 0.9812\nEpoch 11/20\n8/8 [==============================] - 4s 440ms/step - loss: 0.1186 - accuracy: 0.9875\nEpoch 12/20\n8/8 [==============================] - 4s 442ms/step - loss: 0.0991 - accuracy: 0.9625\nEpoch 13/20\n8/8 [==============================] - 4s 440ms/step - loss: 0.0737 - accuracy: 0.9812\nEpoch 14/20\n8/8 [==============================] - 4s 442ms/step - loss: 0.0589 - accuracy: 1.0000\nEpoch 15/20\n8/8 [==============================] - 4s 448ms/step - loss: 0.0466 - accuracy: 1.0000\nEpoch 16/20\n8/8 [==============================] - 4s 443ms/step - loss: 0.0412 - accuracy: 0.9937\nEpoch 17/20\n8/8 [==============================] - 4s 441ms/step - loss: 0.0302 - accuracy: 1.0000\nEpoch 18/20\n8/8 [==============================] - 4s 442ms/step - loss: 0.0279 - accuracy: 1.0000\nEpoch 19/20\n8/8 [==============================] - 4s 443ms/step - loss: 0.0257 - accuracy: 1.0000\nEpoch 20/20\n8/8 [==============================] - 4s 443ms/step - loss: 0.0187 - accuracy: 1.0000\n1/8 [==&gt;...........................] - ETA: 1s - loss: 0.0182 - accuracy: 1.00008/8 [==============================] - 1s 54ms/step - loss: 0.0206 - accuracy: 1.0000\n2/2 [==============================] - 0s 58ms/step - loss: 0.5984 - accuracy: 0.8250\n\n\n2023-04-19 14:38:31.364769: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-19 14:39:43.633980: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-19 14:39:44.234190: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n\nplot_predictions('Vgg1', model1, 'log_images/vgg_1_block', prediction_generator)\n\n 4/40 [==&gt;...........................] - ETA: 0s40/40 [==============================] - 1s 20ms/step\n\n\n2023-04-19 14:39:44.990430: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n\n\nVGG 3 Block\n\n# Define a function to create a VGG block with three convolutional layers\ndef vgg_3_block():\n    # Create a Sequential model object with the name 'vgg_block_3'\n    model = Sequential(name='vgg_block_3')\n    \n    # Add a convolutional layer with 64 filters, a kernel size of 3x3, 'same' padding, and ReLU activation,\n    # and specify the input shape as the desired image size and 3 color channels\n    model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=input_img_size))\n    \n    # Add a max pooling layer with a pool size of 2x2\n    model.add(MaxPooling2D((2, 2)))\n    \n    # Add another convolutional layer with 128 filters and 'same' padding\n    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n    \n    # Add another max pooling layer\n    model.add(MaxPooling2D((2, 2)))\n    \n    # Add a third convolutional layer with 256 filters and 'same' padding\n    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n    \n    # Add a third max pooling layer\n    model.add(MaxPooling2D((2, 2)))\n    \n    # Flatten the output of the convolutional layers\n    model.add(Flatten())\n    \n    # Add a fully connected layer with 128 units and ReLU activation\n    model.add(Dense(128, activation='relu'))\n    \n    # Add a final output layer with a single unit and sigmoid activation (for binary classification)\n    model.add(Dense(1, activation='sigmoid'))\n    \n    # Return the completed model object\n    return model\n\n# Create an instance of the VGG block using the vgg_3_block function\nmodel2 = vgg_3_block()\n\n# Print a summary of the model's architecture\nmodel2.summary()\n\n# Define a log directory for TensorBoard\nlog_dir = 'log_stats/vgg_3_block'\n\n# Define the TensorBoard callback with update_freq='batch'\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq='batch')\n\nModel: \"vgg_block_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_1 (Conv2D)           (None, 128, 128, 64)      1792      \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 64, 64, 64)       0         \n 2D)                                                             \n                                                                 \n conv2d_2 (Conv2D)           (None, 64, 64, 128)       73856     \n                                                                 \n max_pooling2d_2 (MaxPooling  (None, 32, 32, 128)      0         \n 2D)                                                             \n                                                                 \n conv2d_3 (Conv2D)           (None, 32, 32, 256)       295168    \n                                                                 \n max_pooling2d_3 (MaxPooling  (None, 16, 16, 256)      0         \n 2D)                                                             \n                                                                 \n flatten_1 (Flatten)         (None, 65536)             0         \n                                                                 \n dense_2 (Dense)             (None, 128)               8388736   \n                                                                 \n dense_3 (Dense)             (None, 1)                 129       \n                                                                 \n=================================================================\nTotal params: 8,759,681\nTrainable params: 8,759,681\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n# Compile the model with 'adam' optimizer, 'binary_crossentropy' loss function, and 'accuracy' metric\nmodel2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Start timing the training\nstart_time = time.time()\n\n# Train the model using the training generator for a specified number of epochs, and save the history\nhistory = model2.fit(train_generator, steps_per_epoch=len(train_generator), epochs=num_epochs, callbacks=[tensorboard_callback])\n\n# Stop timing the training\nend_time = time.time()\n\n# Calculate the training time by subtracting the start time from the end time\ntraining_time = end_time - start_time\n\n# Evaluate the model on the training set and get the training loss and accuracy\ntrain_loss, train_acc = model2.evaluate(train_generator)\n\n# Evaluate the model on the test set and get the test loss and accuracy\ntest_loss, test_acc = model2.evaluate(test_generator)\n\n# Count the number of parameters in the model\nnum_params = model2.count_params()\n\n# Open the results file in append mode and writing the results\nwith open('results.csv', mode='a', newline='') as results_file:\n    results_writer = csv.writer(results_file)\n    results_writer.writerow(['VGG 3 Block', training_time, train_loss, train_acc, test_acc, num_params])\n    \nmodel2.save('saved_models/vgg_3_block.h5')\n\nEpoch 1/20\n8/8 [==============================] - 4s 452ms/step - loss: 1.2802 - accuracy: 0.5250\nEpoch 2/20\n8/8 [==============================] - 4s 445ms/step - loss: 0.6984 - accuracy: 0.5437\nEpoch 3/20\n8/8 [==============================] - 4s 446ms/step - loss: 0.7064 - accuracy: 0.5000\nEpoch 4/20\n8/8 [==============================] - 4s 446ms/step - loss: 0.6933 - accuracy: 0.6125\nEpoch 5/20\n8/8 [==============================] - 4s 452ms/step - loss: 0.6818 - accuracy: 0.5437\nEpoch 6/20\n8/8 [==============================] - 4s 452ms/step - loss: 0.6226 - accuracy: 0.6562\nEpoch 7/20\n8/8 [==============================] - 4s 449ms/step - loss: 0.5584 - accuracy: 0.7250\nEpoch 8/20\n8/8 [==============================] - 4s 446ms/step - loss: 0.4817 - accuracy: 0.7625\nEpoch 9/20\n8/8 [==============================] - 4s 446ms/step - loss: 0.4709 - accuracy: 0.8188\nEpoch 10/20\n8/8 [==============================] - 4s 451ms/step - loss: 0.3938 - accuracy: 0.8000\nEpoch 11/20\n8/8 [==============================] - 4s 445ms/step - loss: 0.3187 - accuracy: 0.8438\nEpoch 12/20\n8/8 [==============================] - 4s 448ms/step - loss: 0.2332 - accuracy: 0.9062\nEpoch 13/20\n8/8 [==============================] - 4s 449ms/step - loss: 0.1594 - accuracy: 0.9438\nEpoch 14/20\n8/8 [==============================] - 4s 456ms/step - loss: 0.0972 - accuracy: 0.9688\nEpoch 15/20\n8/8 [==============================] - 4s 469ms/step - loss: 0.0486 - accuracy: 0.9812\nEpoch 16/20\n8/8 [==============================] - 4s 445ms/step - loss: 0.0302 - accuracy: 1.0000\nEpoch 17/20\n8/8 [==============================] - 4s 453ms/step - loss: 0.0142 - accuracy: 1.0000\nEpoch 18/20\n8/8 [==============================] - 4s 448ms/step - loss: 0.0089 - accuracy: 1.0000\nEpoch 19/20\n8/8 [==============================] - 4s 449ms/step - loss: 0.0033 - accuracy: 1.0000\nEpoch 20/20\n8/8 [==============================] - 4s 446ms/step - loss: 0.0023 - accuracy: 1.0000\n8/8 [==============================] - 1s 78ms/step - loss: 0.0012 - accuracy: 1.0000\n1/2 [==============&gt;...............] - ETA: 0s - loss: 0.9936 - accuracy: 0.85002/2 [==============================] - 0s 79ms/step - loss: 1.0520 - accuracy: 0.8250\n\n\n2023-04-19 14:39:46.379523: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-19 14:41:00.131167: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-19 14:41:00.947230: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n\nplot_predictions('Vgg3', model2, 'log_images/vgg_3_block', prediction_generator)\n\n 9/40 [=====&gt;........................] - ETA: 0s40/40 [==============================] - 1s 14ms/step\n\n\n2023-04-19 14:41:01.335855: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n\n\nVGG 3 Block with Data Argumentation\n\n# Define an ImageDataGenerator for data augmentation during training\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,                   # rescale pixel values to [0,1]\n    rotation_range=45,                # random rotation between 0-45 degrees\n    # width_shift_range=0.2,            # random shift horizontally up to 20% of the image width\n    # height_shift_range=0.2,           # random shift vertically up to 20% of the image height\n    shear_range=0.2,                  # random shear up to 20%\n    zoom_range=0.2,                   # random zoom up to 20%\n    # horizontal_flip=True,             # randomly flip images horizontally \n    fill_mode='nearest'               # fill any missing pixels with the nearest available pixel\n)\n\n# Create a flow of augmented training data from the training directory\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,                        # path to training data directory\n    target_size=img_size,             # size of input images\n    batch_size=batch_size,            # number of images per batch\n    class_mode='binary'               # type of classification problem (binary or categorical)\n)\n\n# Define an ImageDataGenerator for rescaling pixel values in the test set\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\n# Create a flow of test data from the test directory\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,                         # path to test data directory\n    target_size=img_size,             # size of input images\n    batch_size=batch_size,            # number of images per batch\n    class_mode='binary'               # type of classification problem (binary or categorical)\n)\n\nFound 160 images belonging to 2 classes.\nFound 40 images belonging to 2 classes.\n\n\n\n# Create an instance of the VGG block using the vgg_3_block function\nmodel3 = vgg_3_block()\n\n# Print a summary of the model's architecture\nmodel3.summary()\n\n# Define a log directory for TensorBoard\nlog_dir = 'log_stats/vgg_3_block_with_args'\n\n# Define the TensorBoard callback with update_freq='batch'\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq='batch')\n\nModel: \"vgg_block_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_4 (Conv2D)           (None, 128, 128, 64)      1792      \n                                                                 \n max_pooling2d_4 (MaxPooling  (None, 64, 64, 64)       0         \n 2D)                                                             \n                                                                 \n conv2d_5 (Conv2D)           (None, 64, 64, 128)       73856     \n                                                                 \n max_pooling2d_5 (MaxPooling  (None, 32, 32, 128)      0         \n 2D)                                                             \n                                                                 \n conv2d_6 (Conv2D)           (None, 32, 32, 256)       295168    \n                                                                 \n max_pooling2d_6 (MaxPooling  (None, 16, 16, 256)      0         \n 2D)                                                             \n                                                                 \n flatten_2 (Flatten)         (None, 65536)             0         \n                                                                 \n dense_4 (Dense)             (None, 128)               8388736   \n                                                                 \n dense_5 (Dense)             (None, 1)                 129       \n                                                                 \n=================================================================\nTotal params: 8,759,681\nTrainable params: 8,759,681\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n# Compile the model with 'adam' optimizer, 'binary_crossentropy' loss function, and 'accuracy' metric\nmodel3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Start timing the training\nstart_time = time.time()\n\n# Train the model using the training generator for a specified number of epochs, and save the history\nhistory = model3.fit(train_generator, steps_per_epoch=len(train_generator), epochs=num_epochs, callbacks=[tensorboard_callback])\n\n# Stop timing the training\nend_time = time.time()\n\n# Calculate the training time by subtracting the start time from the end time\ntraining_time = end_time - start_time\n\n# Evaluate the model on the training set and get the training loss and accuracy\ntrain_loss, train_acc = model3.evaluate(train_generator)\n\n# Evaluate the model on the test set and get the test loss and accuracy\ntest_loss, test_acc = model3.evaluate(test_generator)\n\n# Count the number of parameters in the model\nnum_params = model3.count_params()\n\n# Open the results file in append mode and writing the results\nwith open('results.csv', mode='a', newline='') as results_file:\n    results_writer = csv.writer(results_file)\n    results_writer.writerow(['VGG 3 Block with Argumentation', training_time, train_loss, train_acc, test_acc, num_params])\n    \nmodel3.save('saved_models/vgg_3_block_with_args.h5')\n\nEpoch 1/20\n8/8 [==============================] - 4s 463ms/step - loss: 1.2631 - accuracy: 0.4812\nEpoch 2/20\n8/8 [==============================] - 4s 456ms/step - loss: 0.6834 - accuracy: 0.5750\nEpoch 3/20\n8/8 [==============================] - 4s 453ms/step - loss: 0.6574 - accuracy: 0.5813\nEpoch 4/20\n8/8 [==============================] - 4s 453ms/step - loss: 0.6267 - accuracy: 0.7375\nEpoch 5/20\n8/8 [==============================] - 4s 452ms/step - loss: 0.5854 - accuracy: 0.7125\nEpoch 6/20\n8/8 [==============================] - 4s 449ms/step - loss: 0.5650 - accuracy: 0.7500\nEpoch 7/20\n8/8 [==============================] - 4s 450ms/step - loss: 0.5137 - accuracy: 0.7875\nEpoch 8/20\n8/8 [==============================] - 4s 452ms/step - loss: 0.4996 - accuracy: 0.7812\nEpoch 9/20\n8/8 [==============================] - 4s 452ms/step - loss: 0.4974 - accuracy: 0.7688\nEpoch 10/20\n8/8 [==============================] - 4s 449ms/step - loss: 0.4721 - accuracy: 0.7688\nEpoch 11/20\n8/8 [==============================] - 4s 451ms/step - loss: 0.4361 - accuracy: 0.8250\nEpoch 12/20\n8/8 [==============================] - 4s 455ms/step - loss: 0.4523 - accuracy: 0.8125\nEpoch 13/20\n8/8 [==============================] - 4s 448ms/step - loss: 0.4220 - accuracy: 0.8125\nEpoch 14/20\n8/8 [==============================] - 4s 447ms/step - loss: 0.4201 - accuracy: 0.8000\nEpoch 15/20\n8/8 [==============================] - 4s 450ms/step - loss: 0.4078 - accuracy: 0.8125\nEpoch 16/20\n8/8 [==============================] - 4s 452ms/step - loss: 0.4350 - accuracy: 0.8375\nEpoch 17/20\n8/8 [==============================] - 4s 453ms/step - loss: 0.4067 - accuracy: 0.8125\nEpoch 18/20\n8/8 [==============================] - 4s 453ms/step - loss: 0.3679 - accuracy: 0.8813\nEpoch 19/20\n8/8 [==============================] - 4s 452ms/step - loss: 0.3514 - accuracy: 0.8313\nEpoch 20/20\n8/8 [==============================] - 4s 448ms/step - loss: 0.3851 - accuracy: 0.8188\n8/8 [==============================] - 1s 109ms/step - loss: 0.3559 - accuracy: 0.8438\n1/2 [==============&gt;...............] - ETA: 0s - loss: 0.5282 - accuracy: 0.80002/2 [==============================] - 0s 80ms/step - loss: 0.5902 - accuracy: 0.7500\n\n\n2023-04-19 14:41:02.555902: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-19 14:42:17.701998: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-19 14:42:18.761244: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n\nplot_predictions('Vgg3_args', model3, 'log_images/vgg_3_block_with_args', prediction_generator)\n\n 9/40 [=====&gt;........................] - ETA: 0s40/40 [==============================] - 1s 14ms/step\n\n\n2023-04-19 14:42:19.125891: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n\n\nVGG 16 Transfer Learning\n\n# Define the preprocessing function for VGG16 model\npreprocess_input = tf.keras.applications.vgg16.preprocess_input\n\n# Create a train data generator with the preprocessing function\ntrain_datagen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)\n\n# Define the train generator by reading the images from the train directory\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,                        # path to training data directory\n    target_size=img_size,             # size of input images\n    batch_size=batch_size,            # number of images per batch\n    class_mode='binary'               # type of classification problem (binary or categorical)\n)\n\n# Create a test data generator with the same preprocessing function as train generator\ntest_datagen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)\n\n# Define the test generator by reading the images from the test directory\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,                         # path to test data directory\n    target_size=img_size,             # size of input images\n    batch_size=batch_size,            # number of images per batch\n    class_mode='binary'               # type of classification problem (binary or categorical)\n)\n\nFound 160 images belonging to 2 classes.\nFound 40 images belonging to 2 classes.\n\n\n\ndef vgg_16_transfer_learning():\n    # load model\n    model = tf.keras.applications.vgg16.VGG16(include_top=False, input_shape=input_img_size)\n    # mark loaded layers as not trainable\n    for layer in model.layers:\n        layer.trainable = False\n    # add new classifier layers\n    flat1 = Flatten()(model.layers[-1].output)\n    class1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n    output = Dense(1, activation='sigmoid')(class1)\n    # define new model\n    model = keras.models.Model(inputs=model.inputs, outputs=output, name='vgg_16')\n    return model\n\nmodel4 = vgg_16_transfer_learning()\n\n# Print a summary of the model's architecture\nmodel4.summary()\n\n# Define a log directory for TensorBoard\nlog_dir = 'log_stats/vgg_16'\n\n# Define the TensorBoard callback with update_freq='batch'\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq='batch')\n\nModel: \"vgg_16\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 128, 128, 3)]     0         \n                                                                 \n block1_conv1 (Conv2D)       (None, 128, 128, 64)      1792      \n                                                                 \n block1_conv2 (Conv2D)       (None, 128, 128, 64)      36928     \n                                                                 \n block1_pool (MaxPooling2D)  (None, 64, 64, 64)        0         \n                                                                 \n block2_conv1 (Conv2D)       (None, 64, 64, 128)       73856     \n                                                                 \n block2_conv2 (Conv2D)       (None, 64, 64, 128)       147584    \n                                                                 \n block2_pool (MaxPooling2D)  (None, 32, 32, 128)       0         \n                                                                 \n block3_conv1 (Conv2D)       (None, 32, 32, 256)       295168    \n                                                                 \n block3_conv2 (Conv2D)       (None, 32, 32, 256)       590080    \n                                                                 \n block3_conv3 (Conv2D)       (None, 32, 32, 256)       590080    \n                                                                 \n block3_pool (MaxPooling2D)  (None, 16, 16, 256)       0         \n                                                                 \n block4_conv1 (Conv2D)       (None, 16, 16, 512)       1180160   \n                                                                 \n block4_conv2 (Conv2D)       (None, 16, 16, 512)       2359808   \n                                                                 \n block4_conv3 (Conv2D)       (None, 16, 16, 512)       2359808   \n                                                                 \n block4_pool (MaxPooling2D)  (None, 8, 8, 512)         0         \n                                                                 \n block5_conv1 (Conv2D)       (None, 8, 8, 512)         2359808   \n                                                                 \n block5_conv2 (Conv2D)       (None, 8, 8, 512)         2359808   \n                                                                 \n block5_conv3 (Conv2D)       (None, 8, 8, 512)         2359808   \n                                                                 \n block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n                                                                 \n flatten_3 (Flatten)         (None, 8192)              0         \n                                                                 \n dense_6 (Dense)             (None, 128)               1048704   \n                                                                 \n dense_7 (Dense)             (None, 1)                 129       \n                                                                 \n=================================================================\nTotal params: 15,763,521\nTrainable params: 1,048,833\nNon-trainable params: 14,714,688\n_________________________________________________________________\n\n\n\n# Compile the model with 'adam' optimizer, 'binary_crossentropy' loss function, and 'accuracy' metric\nmodel4.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Start timing the training\nstart_time = time.time()\n\n# Train the model using the training generator for a specified number of epochs, and save the history\nhistory = model4.fit(train_generator, steps_per_epoch=len(train_generator), epochs=num_epochs, callbacks=[tensorboard_callback])\n\n# Stop timing the training\nend_time = time.time()\n\n# Calculate the training time by subtracting the start time from the end time\ntraining_time = end_time - start_time\n\n# Evaluate the model on the training set and get the training loss and accuracy\ntrain_loss, train_acc = model4.evaluate(train_generator)\n\n# Evaluate the model on the test set and get the test loss and accuracy\ntest_loss, test_acc = model4.evaluate(test_generator)\n\n# Count the number of parameters in the model\nnum_params = model4.count_params()\n\n# Open the results file in append mode and writing the results\nwith open('results.csv', mode='a', newline='') as results_file:\n    results_writer = csv.writer(results_file)\n    results_writer.writerow(['VGG 16', training_time, train_loss, train_acc, test_acc, num_params])\n    \nmodel4.save('saved_models/vgg_16_transfer_learning.h5')\n\nEpoch 1/20\n8/8 [==============================] - 3s 349ms/step - loss: 5.2201 - accuracy: 0.7875\nEpoch 2/20\n8/8 [==============================] - 3s 337ms/step - loss: 1.4520 - accuracy: 0.9250\nEpoch 3/20\n8/8 [==============================] - 3s 342ms/step - loss: 0.2833 - accuracy: 0.9625\nEpoch 4/20\n8/8 [==============================] - 3s 340ms/step - loss: 3.8766e-07 - accuracy: 1.0000\nEpoch 5/20\n8/8 [==============================] - 3s 335ms/step - loss: 2.5094e-04 - accuracy: 1.0000\nEpoch 6/20\n8/8 [==============================] - 3s 336ms/step - loss: 9.4699e-06 - accuracy: 1.0000\nEpoch 7/20\n8/8 [==============================] - 3s 335ms/step - loss: 5.3511e-07 - accuracy: 1.0000\nEpoch 8/20\n8/8 [==============================] - 3s 339ms/step - loss: 3.5025e-07 - accuracy: 1.0000\nEpoch 9/20\n8/8 [==============================] - 3s 336ms/step - loss: 2.1423e-07 - accuracy: 1.0000\nEpoch 10/20\n8/8 [==============================] - 3s 337ms/step - loss: 1.9476e-07 - accuracy: 1.0000\nEpoch 11/20\n8/8 [==============================] - 3s 337ms/step - loss: 1.4968e-07 - accuracy: 1.0000\nEpoch 12/20\n8/8 [==============================] - 3s 337ms/step - loss: 1.2451e-07 - accuracy: 1.0000\nEpoch 13/20\n8/8 [==============================] - 3s 335ms/step - loss: 1.1644e-07 - accuracy: 1.0000\nEpoch 14/20\n8/8 [==============================] - 3s 336ms/step - loss: 1.0194e-07 - accuracy: 1.0000\nEpoch 15/20\n8/8 [==============================] - 3s 337ms/step - loss: 9.6853e-08 - accuracy: 1.0000\nEpoch 16/20\n8/8 [==============================] - 3s 339ms/step - loss: 8.6054e-08 - accuracy: 1.0000\nEpoch 17/20\n8/8 [==============================] - 3s 338ms/step - loss: 7.8499e-08 - accuracy: 1.0000\nEpoch 18/20\n8/8 [==============================] - 3s 337ms/step - loss: 7.4105e-08 - accuracy: 1.0000\nEpoch 19/20\n8/8 [==============================] - 3s 337ms/step - loss: 6.9560e-08 - accuracy: 1.0000\nEpoch 20/20\n8/8 [==============================] - 3s 337ms/step - loss: 6.3877e-08 - accuracy: 1.0000\n8/8 [==============================] - 3s 331ms/step - loss: 6.1388e-08 - accuracy: 1.0000\n2/2 [==============================] - 1s 323ms/step - loss: 3.0647 - accuracy: 0.8500\n\n\n2023-04-19 14:42:20.544130: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-19 14:43:16.201525: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-19 14:43:19.074663: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n\ntest_loss, test_acc = model4.evaluate(test_generator)\n\n2023-04-19 14:43:19.915752: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n2/2 [==============================] - 1s 332ms/step - loss: 3.0647 - accuracy: 0.8500\n\n\n\nplot_predictions('Vgg16', model4, 'log_images/vgg_16', prediction_generator_vgg)\n\n 4/40 [==&gt;...........................] - ETA: 0s40/40 [==============================] - 1s 21ms/step\n\n\n2023-04-19 14:43:20.693366: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n\n\nMLP with 18 million trainable parameters\n\n# Define the MLP model\ndef create_mlp_18():\n    model = Sequential(name = 'MLP')\n    model.add(Flatten(input_shape=input_img_size))\n    model.add(Dense(256, activation='relu'))\n    model.add(Dense(4096, activation='relu'))\n    model.add(Dense(1024, activation='relu'))\n    # model.add(Dropout(0.2))\n    model.add(Dense(1, activation='sigmoid'))\n    return model\n\nmodel5 = create_mlp_18()\n# Print the model summary to see the number of parameters\nmodel5.summary()\n\n# Define a log directory for TensorBoard\nlog_dir = 'log_stats/mlp_18'\n\n# Define the TensorBoard callback with update_freq='batch'\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq='batch')\n\nModel: \"MLP\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n flatten_4 (Flatten)         (None, 49152)             0         \n                                                                 \n dense_8 (Dense)             (None, 256)               12583168  \n                                                                 \n dense_9 (Dense)             (None, 4096)              1052672   \n                                                                 \n dense_10 (Dense)            (None, 1024)              4195328   \n                                                                 \n dense_11 (Dense)            (None, 1)                 1025      \n                                                                 \n=================================================================\nTotal params: 17,832,193\nTrainable params: 17,832,193\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n# Compile the model with 'adam' optimizer, 'binary_crossentropy' loss function, and 'accuracy' metric\nmodel5.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Start timing the training\nstart_time = time.time()\n\n# Train the model using the training generator for a specified number of epochs, and save the history\nhistory = model5.fit(train_generator, steps_per_epoch=len(train_generator), epochs=num_epochs, callbacks=[tensorboard_callback])\n\n# Stop timing the training\nend_time = time.time()\n\n# Calculate the training time by subtracting the start time from the end time\ntraining_time = end_time - start_time\n\n# Evaluate the model on the training set and get the training loss and accuracy\ntrain_loss, train_acc = model5.evaluate(train_generator)\n\n# Evaluate the model on the test set and get the test loss and accuracy\ntest_loss, test_acc = model5.evaluate(test_generator)\n\n# Count the number of parameters in the model\nnum_params = model5.count_params()\n\n# Open the results file in append mode and writing the results\nwith open('results.csv', mode='a', newline='') as results_file:\n    results_writer = csv.writer(results_file)\n    results_writer.writerow(['MLP18', training_time, train_loss, train_acc, test_acc, num_params])\n    \nmodel5.save('saved_models/mlp18.h5')\n\nEpoch 1/20\n8/8 [==============================] - 2s 181ms/step - loss: 828.1496 - accuracy: 0.5188\nEpoch 2/20\n8/8 [==============================] - 1s 181ms/step - loss: 16.2150 - accuracy: 0.5375\nEpoch 3/20\n8/8 [==============================] - 1s 180ms/step - loss: 15.6720 - accuracy: 0.7688\nEpoch 4/20\n8/8 [==============================] - 1s 177ms/step - loss: 12.2998 - accuracy: 0.7250\nEpoch 5/20\n8/8 [==============================] - 1s 179ms/step - loss: 5.2125 - accuracy: 0.7750\nEpoch 6/20\n8/8 [==============================] - 1s 180ms/step - loss: 0.6979 - accuracy: 0.7625\nEpoch 7/20\n8/8 [==============================] - 1s 180ms/step - loss: 0.4396 - accuracy: 0.8125\nEpoch 8/20\n8/8 [==============================] - 1s 177ms/step - loss: 0.2875 - accuracy: 0.8375\nEpoch 9/20\n8/8 [==============================] - 1s 179ms/step - loss: 0.2572 - accuracy: 0.8562\nEpoch 10/20\n8/8 [==============================] - 1s 176ms/step - loss: 0.2298 - accuracy: 0.8813\nEpoch 11/20\n8/8 [==============================] - 1s 176ms/step - loss: 0.2363 - accuracy: 0.8625\nEpoch 12/20\n8/8 [==============================] - 1s 176ms/step - loss: 0.2186 - accuracy: 0.8813\nEpoch 13/20\n8/8 [==============================] - 1s 178ms/step - loss: 0.2041 - accuracy: 0.8875\nEpoch 14/20\n8/8 [==============================] - 1s 182ms/step - loss: 0.1776 - accuracy: 0.8875\nEpoch 15/20\n8/8 [==============================] - 1s 180ms/step - loss: 0.1936 - accuracy: 0.8813\nEpoch 16/20\n8/8 [==============================] - 1s 181ms/step - loss: 0.1938 - accuracy: 0.8750\nEpoch 17/20\n8/8 [==============================] - 1s 178ms/step - loss: 0.2023 - accuracy: 0.8562\nEpoch 18/20\n8/8 [==============================] - 1s 178ms/step - loss: 0.2071 - accuracy: 0.8562\nEpoch 19/20\n8/8 [==============================] - 1s 179ms/step - loss: 0.2038 - accuracy: 0.8562\nEpoch 20/20\n8/8 [==============================] - 1s 177ms/step - loss: 0.2014 - accuracy: 0.8562\n1/8 [==&gt;...........................] - ETA: 1s - loss: 0.1485 - accuracy: 0.95008/8 [==============================] - 0s 34ms/step - loss: 0.1929 - accuracy: 0.8562\n2/2 [==============================] - 0s 28ms/step - loss: 1.7975 - accuracy: 0.6750\n\n\n2023-04-19 14:43:22.001196: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-19 14:43:51.992711: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-19 14:43:52.421763: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n\nplot_predictions('MLP18', model5, 'log_images/mlp18', prediction_generator)\n\n 6/40 [===&gt;..........................] - ETA: 0s40/40 [==============================] - 1s 11ms/step\n\n\n2023-04-19 14:43:53.022685: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n\n\nMLP with 135 Million Trainable Parameters\n\n# Define the MLP model\ndef create_mlp_135():\n    model = Sequential(name = 'MLP')\n    model.add(Flatten(input_shape=input_img_size))\n    model.add(Dense(2500, activation='relu'))\n    model.add(Dense(2500, activation='relu'))\n    model.add(Dense(2500, activation='relu'))\n    # model.add(Dropout(0.2))\n    model.add(Dense(1, activation='sigmoid'))\n    return model\n\nmodel6 = create_mlp_135()\n# Print the model summary to see the number of parameters\nmodel6.summary()\n\n# Define a log directory for TensorBoard\nlog_dir = 'log_stats/mlp_135'\n\n# Define the TensorBoard callback with update_freq='batch'\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq='batch')\n\n2023-04-19 14:55:06.216229: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 491520000 exceeds 10% of free system memory.\n2023-04-19 14:55:06.543298: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 491520000 exceeds 10% of free system memory.\n2023-04-19 14:55:06.622387: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 491520000 exceeds 10% of free system memory.\n\n\nModel: \"MLP\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n flatten_5 (Flatten)         (None, 49152)             0         \n                                                                 \n dense_12 (Dense)            (None, 2500)              122882500 \n                                                                 \n dense_13 (Dense)            (None, 2500)              6252500   \n                                                                 \n dense_14 (Dense)            (None, 2500)              6252500   \n                                                                 \n dense_15 (Dense)            (None, 1)                 2501      \n                                                                 \n=================================================================\nTotal params: 135,390,001\nTrainable params: 135,390,001\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n# Compile the model with 'adam' optimizer, 'binary_crossentropy' loss function, and 'accuracy' metric\nmodel6.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Start timing the training\nstart_time = time.time()\n\n# Train the model using the training generator for a specified number of epochs, and save the history\nhistory = model6.fit(train_generator, steps_per_epoch=len(train_generator), epochs=num_epochs, callbacks=[tensorboard_callback])\n\n# Stop timing the training\nend_time = time.time()\n\n# Calculate the training time by subtracting the start time from the end time\ntraining_time = end_time - start_time\n\n# Evaluate the model on the training set and get the training loss and accuracy\ntrain_loss, train_acc = model6.evaluate(train_generator)\n\n# Evaluate the model on the test set and get the test loss and accuracy\ntest_loss, test_acc = model6.evaluate(test_generator)\n\n# Count the number of parameters in the model\nnum_params = model6.count_params()\n\n# Open the results file in append mode and writing the results\nwith open('results.csv', mode='a', newline='') as results_file:\n    results_writer = csv.writer(results_file)\n    results_writer.writerow(['MLP135', training_time, train_loss, train_acc, test_acc, num_params])\n    \nmodel5.save('saved_models/mlp135.h5')\n\nEpoch 1/20\n8/8 [==============================] - 11s 1s/step - loss: 8302.7051 - accuracy: 0.4750\nEpoch 2/20\n8/8 [==============================] - 10s 1s/step - loss: 871.6079 - accuracy: 0.6062\nEpoch 3/20\n8/8 [==============================] - 10s 1s/step - loss: 273.8789 - accuracy: 0.7188\nEpoch 4/20\n8/8 [==============================] - 10s 1s/step - loss: 134.2486 - accuracy: 0.7250\nEpoch 5/20\n8/8 [==============================] - 10s 1s/step - loss: 77.9153 - accuracy: 0.8062\nEpoch 6/20\n8/8 [==============================] - 10s 1s/step - loss: 115.3639 - accuracy: 0.8125\nEpoch 7/20\n8/8 [==============================] - 10s 1s/step - loss: 86.5102 - accuracy: 0.8813\nEpoch 8/20\n8/8 [==============================] - 10s 1s/step - loss: 5.3630 - accuracy: 0.9563\nEpoch 9/20\n8/8 [==============================] - 10s 1s/step - loss: 40.6399 - accuracy: 0.9187\nEpoch 10/20\n8/8 [==============================] - 10s 1s/step - loss: 8.1418 - accuracy: 0.9375\nEpoch 11/20\n8/8 [==============================] - 10s 1s/step - loss: 22.5314 - accuracy: 0.9375\nEpoch 12/20\n8/8 [==============================] - 10s 1s/step - loss: 50.7123 - accuracy: 0.9000\nEpoch 13/20\n8/8 [==============================] - 10s 1s/step - loss: 57.6497 - accuracy: 0.8813\nEpoch 14/20\n8/8 [==============================] - 10s 1s/step - loss: 34.0063 - accuracy: 0.9187\nEpoch 15/20\n8/8 [==============================] - 10s 1s/step - loss: 21.8980 - accuracy: 0.9187\nEpoch 16/20\n8/8 [==============================] - 10s 1s/step - loss: 0.7129 - accuracy: 0.9812\nEpoch 17/20\n8/8 [==============================] - 10s 1s/step - loss: 128.1349 - accuracy: 0.8875\nEpoch 18/20\n8/8 [==============================] - 10s 1s/step - loss: 12.1085 - accuracy: 0.9625\nEpoch 19/20\n8/8 [==============================] - 10s 1s/step - loss: 7.0070 - accuracy: 0.9750\nEpoch 20/20\n8/8 [==============================] - 10s 1s/step - loss: 36.5162 - accuracy: 0.9625\n1/8 [==&gt;...........................] - ETA: 1s - loss: 3.6694e-20 - accuracy: 1.00008/8 [==============================] - 0s 38ms/step - loss: 39.5354 - accuracy: 0.9812\n2/2 [==============================] - 0s 49ms/step - loss: 298.6696 - accuracy: 0.7000\n\n\n2023-04-19 14:55:09.682721: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-19 14:55:09.831306: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 491520000 exceeds 10% of free system memory.\n2023-04-19 14:55:09.874048: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 491520000 exceeds 10% of free system memory.\n2023-04-19 14:58:32.310921: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n2023-04-19 14:58:32.793472: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n\nplot_predictions('MLP135', model6, 'log_images/mlp135', prediction_generator)\n\n 2/40 [&gt;.............................] - ETA: 2s40/40 [==============================] - 2s 52ms/step\n\n\n2023-04-19 14:58:33.204956: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n     [[{{node Placeholder/_0}}]]\n\n\n\n\nComparisons\n\nimport pandas as pd\ndf = pd.read_csv('results.csv', index_col=0)\ndisplay(df)\n\n\n\n\n\n\n\n\nTraining Time\nTrain Loss\nTrain Acc\nTest Acc\nNum Params\n\n\nModel Name\n\n\n\n\n\n\n\n\n\nVGG 1 Block\n72.288201\n2.056565e-02\n1.00000\n0.825\n33556481\n\n\nVGG 3 Block\n73.744514\n1.194572e-03\n1.00000\n0.825\n8759681\n\n\nVGG 3 Block with Argumentation\n75.150163\n3.559158e-01\n0.84375\n0.750\n8759681\n\n\nVGG 16\n55.661988\n6.138818e-08\n1.00000\n0.850\n15763521\n\n\nMLP18\n29.990009\n1.929377e-01\n0.85625\n0.675\n17832193\n\n\nMLP135\n202.682255\n3.953538e+01\n0.98125\n0.700\n135390001\n\n\n\n\n\n\n\n\n\n\nAre the results as expected? Why or why not?\nVGG (1 Block)\nIn VGG (1 block) architecture, max-pooling and fully connected layers are placed after the single block of convolutional layers . Although this architecture has a limited number of parameters and trains quite quickly, it might not be able to recognise complicated aspects in the input.\nVGG (3 blocks)\nThis architecture made up of three blocks of convolutional layers, and each block is followed by fully connected and max-pooling layers. This architecture can capture more complicated aspects in the data because it has more parameters than VGG (1 block). However, because there are more parameters, it takes longer to train than VGG (1 block).\nVGG (3 blocks with data argumentation)\nBy adding different changes to the input images, such as rotation, scaling, and cropping, we inflate the size of the training set. By exposing the model to a larger range of input images when it is used with VGG (3 blocks), data augmentation can increase the model’s capacity to generalise to new data.\nVGG 16 Tranfer Learning\nWhen training a new model on a different dataset, transfer learning entails using a previously trained model as a starting point. Two pre-trained models that can be utilised for transfer learning are VGG16 and VGG19. These models have already learned to recognise a range of visual traits after being trained on massive datasets like ImageNet. For us, we used VGG16. We can reduce the amount of time and computational resources required to train a new model on a smaller dataset by starting with these pre-trained models.\nFurthermore, when the target dataset is modest, transfer learning using VGG16 outperforms training a model from scratch in terms of performance.\nMLP with 135 million parameters\nThe choice between using a MLP with 135 million parameters versus transfer learning with VGG16 depends on several factors, including the specific task, available resources, and the size of the dataset.\nIn general, using an MLP with 135 million parameters can potentially achieve higher accuracy compared to transfer learning with VGG16, especially for more complex tasks. However, training such a large MLP from scratch requires a significant amount of computational resources and time, making it impractical for many applications. Additionally, having such a large number of parameters increases the risk of overfitting, which can negatively impact model performance.\nOn the other hand, transfer learning with VGG16 can be a good choice for image classification tasks, especially if the dataset is small.\nMLP with 18 million parameters\nIn general, an MLP with 135 million parameters is likely to have a higher capacity to model complex relationships within the data compared to an MLP with 18 million parameters. However, having a larger number of parameters does not necessarily guarantee better performance, and can increase the risk of overfitting, especially if the dataset is small.\nOn the other hand, an MLP with 18 million parameters is likely to require fewer computational resources and training time compared to an MLP with 135 million parameters. This can be beneficial, especially if the available resources are limited.\nThe number of layers in both MLPs being 3, it’s important to note that the depth of the neural network is also an important factor to consider. Increasing the depth of the network can help to model more complex relationships within the data. However, it can also increase the risk of vanishing or exploding gradients, making it more difficult to train the network.\n\n\n\n\n\n\n\n\n\n\n\nModel Name\nTraining Time\nTrain Loss\nTrain Acc\nTest Acc\nNum Params\n\n\n\n\nVGG 1 Block\n72.28820062\n0.020565649\n1\n0.824999988\n33556481\n\n\nVGG 3 Block\n73.74451399\n0.001194572\n1\n0.824999988\n8759681\n\n\nVGG 3 Block with Argumentation\n75.15016341\n0.355915844\n0.84375\n0.75\n8759681\n\n\nVGG 16\n55.6619885\n6.14E-08\n1\n0.850000024\n15763521\n\n\nMLP18\n29.99000883\n0.192937687\n0.856249988\n0.675000012\n17832193\n\n\n\n\n\n\n\n\n\n\nDoes data augmentation help? Why or why not?\nData augmentation is a method of creating additional training data from the existing datase by using various transformations including rotation, scaling, flipping, cropping, and other picture modificationst.\nBy doing this, the model is exposed to a wider variety of training data and can improve its ability to identify and generalise to fresh, unexplored data.\nData augmentation can reduce overfitting, which occurs when the model memorises the training data rather than generalising to new data, by expanding the quantity and diversity of the training dataset. This may enable the model to perform more effectively on the test data.\nIn other words, data augmentation makes the model more reliable and capable of identifying objects or patterns.\nFor us, the argumented model is working worse than the original one. One reason could be, less number of epochs to train with, perhaps may with with higher epoch, we will get better results.\n\n\n\nDoes it matter how many epochs you fine tune the model? Why or why not?\nThe term “number of epochs” refers to how many times the complete dataset was run through the model during training while fine-tuning a pre-trained model.\nIf the number of epoch is low, the model will underfit.\nOn the other side, if the number of epochs is too high , then model might begin to overfit to the training data, which would mean that it would memorise the training data rather than generalising to new data.\nIt is crucial to select the right number of epochs for fine-tuning in accordance with the difficulty of the task, the size of the dataset, and the unique properties of the model and data. Normally, this value is calculated by keeping track of the model’s performance on a validation set during training and halting when the validation accuracy stops increasing or begins to drop.\nEven though we have not implemented it, but early stopping could be a good measure to utilize, when it comes to number of epochs.\nEarly stopping is a callback technique that can help prevent overfitting and save computational resources by stopping the training process before it completes all epochs. It can improve model performance, reduce training time, and help generalize the model better to unseen data.\n\n\n\nAre there any particular images that the model is confused about? Why or why not?\n\nThis was the image for which each model got confused."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "experience.html",
    "href": "experience.html",
    "title": "Experience",
    "section": "",
    "text": "Software Developer at Institute Management System (IMS)\n  August 2022 to Present\n  \n  Indian Institute of Technology Gandhinagar\n  \n  I am currently employed as a Project Fellow at IIT Gandhinagar, where my role revolves around the Institute Management System (IMS). My responsibilities encompass working with various technologies such as Angular and ASP.NET Core.\n  \n\n\n\n\nResearch Intern \nSept 2020 to Feb 2022\n  \n  Cooch Behar Government Engineering College \n\nLetter of Recommendation\n\n\n\nI dedicated one year and six months to the development and publication of three significant research papers and the acquisition of two copyright registrations. These achievements were realized in the field of computer vision, image processing, and soft computing."
  },
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Blogs",
    "section": "",
    "text": "Byte by Byte Vision\n\n\n\n\n\nWelcome to Byte-by-Byte Vision, your go-to platform for high-quality, curated resources in deep learning and computer vision. Learn at your own pace with structured paths, whether you’re a beginner or refining your expertise. Dive in and explore the transformative world of CV!\n\n\n\n\n\n\nSoumyaratna Debnath\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blogs/Byte by Byte Vision/index.html",
    "href": "blogs/Byte by Byte Vision/index.html",
    "title": "Byte by Byte Vision",
    "section": "",
    "text": "Generic\n\nML Was Hard Until I Learned These 5 Secrets!\nHow I’d learn ML (if I could start over)\nBasic probability: Joint, marginal and conditional probability | Independence\nWhat is Prior And Posterior\nMathematics for Deep Learning Playlist\nMathematics for Deep Learning with Codes\nNeural Networks Implemented From Scratch\n\n\n\nPyTorch and Visualizations\n\nPyTorch Tutorials - Complete Beginner Course\nPyTorch Lightning Tutorial\nComplete TensorBoard Guide\n\n\n\nGraph Neural Networks\n\nGraph Neural Networks - DeepFindr\nGraph Convolutional Networks using only NumPy\nFind the related codes here\n\n\n\nVision Transformers\n\nAttention in transformers, visually explained\nThe Illustrated Transformer\nTransformer Neural Networks Derived from Scratch\nVision Transformer from Scratch\nImplement and Train ViT From Scratch for Image Recognition - PyTorch\nVision Transformer in PyTorch\nFind the related codes here\n\n\n\nGenerative Adversarial Networks\n\nUnderstand the Math and Theory of GANs\nBuilding our first simple GAN\nPix2Pix Paper Walkthrough and implementation from scratch\nCycleGAN Paper Walkthrough and implementation from scratch\nProGAN Paper Walkthrough and implementation from scratch\nSRGAN Paper Walkthrough and implementation from scratch\nStyleGAN Paper Walkthorugh and implementation from scratch\nFind the related codes here\n\n\n\nVariational Autoencoder\n\nVariational Autoencoder Explained\nVariational AutoEncoder Paper Walkthrough and implementation from scratch\nFind the related codes here\n\n\n\nDiffusion Models\n\nDiffusion models explained in 4-difficulty levels\nWhat are Diffusion Models?\nDiffusion Models | Paper Explanation | Math Explained\nDDPM Explained and implementation from scratch\nWhat is Stable Diffusion?\nCoding Stable Diffusion from scratch in PyTorch\n\n\n\nNeural Radiance Field (NeRF) [Coming Soon]\n\nNeural Radiance Fields Paper Explained\n\n\n\nPyTorch Mobile [Coming Soon]"
  },
  {
    "objectID": "education.html",
    "href": "education.html",
    "title": "Education",
    "section": "",
    "text": "Master of Technology in Computer Science and Engineering\n  2022-2025\n  \n  Indian Institute of Technology Gandhinagar\n  CPI 8.86\n  \n  I am presently overseeing the following projects as integral components of my thesis work. \n  \n  \n    Primate Pose Estimation and Tracking for in-depth behavioral studies on primates using synthetic data.\n    Smart Farming with Aerial Imagery to enhance cotton crop yield using images taken from drones.\n  \n  \n  My projects endeavors heavily on the utilization of Computer Vision and Machine Learning techniques, exemplifying the intersection of technology and scientific exploration.\n  \n\n\n\n\n\n\nBachelor of Technology in Computer Science and Engineering \n2018-2022\n  \n  Cooch Behar Government Engineering College \n  CPI 9.34\n\n\n\nDuring my undergraduate studies, I had the privilege of working under the mentorship of Dr. Sourav De, focusing on the fascinating field of soft computing and evolutionary algorithms. In my final year project, I transitioned into the realm of deep learning, concentrating on underwater object detection and image enhancement using state-of-the-art models, thereby expanding my expertise and research interests."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Soumyaratna Debnath",
    "section": "",
    "text": "Bio\n\n\n\n\n\nI am an MTech student in Computer Science at IIT Gandhinagar, where I specialize in Computer Vision, focusing on rendering-based optimization and 3D shape analysis. Currently, I am a researcher in the CVIG Lab at IIT Gandhinagar, delving into cutting-edge projects. In addition to my academic journey, I served as a Project Fellow at IIT Gandhinagar for 19 Months, where I contributed to the Institute Management System using Angular and ASP.NET Core. With a passion for technology and innovation, I am committed to making a significant impact in the ever-evolving field of computer science."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "projects/Histogram Matching/index.html",
    "href": "projects/Histogram Matching/index.html",
    "title": "Histogram Matching from Scratch",
    "section": "",
    "text": "GitHub \n\nImporting the necessary libraries\n\n# Importing the libraries\nimport numpy as np\nimport os \nimport cv2 # used only for loading the image\nimport matplotlib.pyplot as plt # used only for displaying the image\n\n\n\nDefining the required functions\n\n# this function is responsible for calculating the histogram of an image\ndef calculate_histogram(image, num_bins=256):\n    histogram = np.zeros(num_bins, dtype=np.int32) # initialize the histogram\n    for pixel_value in image.ravel(): # ravel() returns a contiguous flattened array\n        histogram[pixel_value] += 1 # increment the count of the pixel value\n    return histogram # return the histogram\n\n# this function is responsible for calculating the normalized histogram of an image\ndef calculate_normalized_histogram(image, num_bins=256):\n    histogram = calculate_histogram(image, num_bins) # calculate the histogram\n    sum_of_histogram = np.sum(histogram) # sum of all the pixel values\n    histogram = histogram / sum_of_histogram # normalize the histogram\n    return histogram # return the normalized histogram\n\n# this function is responsible for calculating the cumulative histogram of an image\ndef calculate_cumulative_histogram(histogram):\n    sum_of_histogram = np.sum(histogram) # sum of all the pixel values\n    histogram = histogram / sum_of_histogram # normalize the histogram\n    cumulative_histogram = np.zeros(histogram.shape, dtype=np.float32) # initialize the cumulative histogram\n    cumulative_histogram[0] = histogram[0] \n    for i in range(1, histogram.shape[0]): # calculate the cumulative histogram\n        cumulative_histogram[i] = cumulative_histogram[i - 1] + histogram[i]\n    return cumulative_histogram # return the cumulative histogram\n\n\n# opeining the images in grayscale and storing them in a list\nimage_folder_path = os.path.join(os.getcwd(), 'Dataset', 'histogram_matching')\nimage_set = []\nfor image_name in os.listdir(image_folder_path): # iterate over all the images in the folder\n    img = cv2.imread(os.path.join(image_folder_path, image_name), cv2.IMREAD_GRAYSCALE) # read the image in grayscale\n    image_set.append(img) # append the image to the list\n\n\n# this function is responsible for displaying the images and their histograms\ndef visualize_histograms(image_set, figsize=(15, 9), image_titles=None):\n    plt.figure(figsize=figsize) # set the figure size\n\n    for i, image in enumerate(image_set): # iterate over all the images\n        histogram = calculate_histogram(image) # calculate the histogram of the image\n        normalized_histogram = calculate_normalized_histogram(image) # calculate the normalized histogram of the image\n        cumulative_histogram = calculate_cumulative_histogram(histogram) # calculate the cumulative histogram of the image\n        \n        plt.subplot(3, len(image_set), i + 1) # display the image\n        plt.imshow(image, cmap='gray')\n        plt.title(image_titles[i], fontsize=10)\n        plt.axis('off')\n\n        plt.subplot(3, len(image_set), i + 1 + len(image_set)) # display the histogram\n        plt.bar(range(256), normalized_histogram, width=1.0)\n        plt.title('Normalized Histogram ('+image_titles[i]+')', fontsize=8)\n        plt.xlabel('Pixel Value', fontsize=8)\n        plt.ylabel('Frequency', fontsize=8)\n\n        plt.subplot(3, len(image_set), i + 1 + 2 * len(image_set)) # display the cumulative histogram\n        plt.plot(cumulative_histogram)\n        plt.title('Cumulative Histogram ('+image_titles[i]+')', fontsize=8)\n        plt.xlabel('Pixel Value', fontsize=8)\n        plt.ylabel('Frequency', fontsize=8)\n\n    plt.tight_layout()\n    plt.show()\n\n# displaying the images and their histograms\nvisualize_histograms(image_set, image_titles=['Image 0', 'Image 1', 'Image 2', 'Image 3'])\n\n\n\n\n\n\nImplementing the algorithm\n\n# this function is responsible for matching the histogram of an image to the histogram of a reference image\ndef match_histograms(image, reference_image):\n    mapping = get_mapping(image, reference_image) # get the mapping\n    matched_image = np.zeros(image.shape, dtype=np.uint8) # initialize the matched image\n    for i in range(image.shape[0]): # match the image\n        for j in range(image.shape[1]):\n            matched_image[i, j] = mapping[image[i, j]]\n    return matched_image # return the matched image\n\n# this function is responsible for matching the histogram of an image to the histogram of a reference image\ndef get_mapping(image, reference_image, gray_levels=256):\n    histogram = calculate_histogram(image) # calculate the histogram of the image\n    cumulative_histogram = calculate_cumulative_histogram(histogram) # calculate the cumulative histogram of the image\n    reference_histogram = calculate_histogram(reference_image) # calculate the histogram of the reference image\n    reference_cumulative_histogram = calculate_cumulative_histogram(reference_histogram) # calculate the cumulative histogram of the reference image\n\n    mapping = np.zeros(gray_levels) # initialize the mapping\n    for pixel_value in range(gray_levels):\n        old_value = cumulative_histogram[pixel_value] # get the cumulative histogram of the image\n        temp = reference_cumulative_histogram - old_value # get the difference between the cumulative histogram of the reference image and the cumulative histogram of the image\n        new_value = np.argmin(np.abs(temp)) # get the index of the minimum value in the difference\n        mapping[pixel_value] = new_value # map the pixel value to the new value\n    return mapping # return the mapping\n\n\n# performing histogram matching and displaying the images and their histograms\ndef histogram_matching_and_visualization(image, reference_image, visualize=True): \n    matched_image = match_histograms(image, reference_image) # match the histogram of the image to the histogram of the reference image\n    image_set = [image, reference_image, matched_image] \n    image_titles = ['Source Image', 'Target Image', 'Matched Image']\n    if visualize:\n        visualize_histograms(image_set, image_titles=image_titles) # display the images and their histograms   \n    return matched_image # return the matched image  \n\n\nmatching = histogram_matching_and_visualization(image_set[0], image_set[1])  \n\n\n\n\n\nmatching = histogram_matching_and_visualization(image_set[3], image_set[2])\n\n\n\n\n\nmatching = histogram_matching_and_visualization(image_set[1], image_set[3])   \n\n\n\n\n\nmatching = histogram_matching_and_visualization(image_set[2], image_set[0])\n\n\n\n\n\n\nAnalysis of the obtained resutls\n\nimport numpy as np\nfrom scipy.stats import entropy\nfrom skimage.metrics import structural_similarity as ssim\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\n\ndef calculate_image_statistics(original_image, target_image, matched_image):\n    # Calculate Mean and Standard Deviation\n    mean_original = np.mean(original_image)\n    std_original = np.std(original_image)\n\n    mean_target = np.mean(target_image)\n    std_target = np.std(target_image)\n\n    mean_matched = np.mean(matched_image)\n    std_matched = np.std(matched_image)\n\n    # Calculate SSIM\n    ssim_score = ssim(original_image, matched_image)\n\n    # Create a dictionary to store the statistics\n    statistics = {\n        \"Original Mean\": mean_original,\n        \"Original Standard Deviation\": std_original,\n        \"Target Mean\": mean_target,\n        \"Target Standard Deviation\": std_target,\n        \"Matched Mean\": mean_matched,\n        \"Matched Standard Deviation\": std_matched,\n        \"SSIM Score (Source vs Matched)\": ssim_score,\n    }\n\n    return statistics\n\n\nmatching = histogram_matching_and_visualization(image_set[0], image_set[1], visualize=True)\nstatistics = calculate_image_statistics(image_set[0], image_set[1], matching)\nfor key, value in statistics.items():\n    print(f\"{key}: {value}\")\n\n\n\n\nOriginal Mean: 80.04150772094727\nOriginal Standard Deviation: 68.48801554947227\nTarget Mean: 126.48823547363281\nTarget Standard Deviation: 69.12917387622817\nMatched Mean: 130.7914924621582\nMatched Standard Deviation: 62.277325624710976\nSSIM Score (Source vs Matched): 0.6362629163593398\n\n\nMean and Standard Deviation\nThe original image has a mean of approximately 80.04 and a standard deviation of approximately 68.49. The target image has a mean of approximately 126.49 and a standard deviation of approximately 69.13. After the histogram matching process, the matched image has a mean of approximately 130.79 and a standard deviation of approximately 62.27. Interpretation: The means have shifted towards each other after histogram matching, but the standard deviations have not changed significantly.\nSSIM Score\nThe Structural Similarity Index (SSIM) score between the original and matched images is approximately 0.636. Interpretation: An SSIM score of 1 indicates a perfect match. A score of 0.636 suggests that the matched image is reasonably similar to the original but not a perfect match.\n\nmatching = histogram_matching_and_visualization(image_set[1], image_set[3], visualize=True)\nstatistics = calculate_image_statistics(image_set[1], image_set[3], matching)\nfor key, value in statistics.items():\n    print(f\"{key}: {value}\")\n\n\n\n\nOriginal Mean: 126.48823547363281\nOriginal Standard Deviation: 69.12917387622817\nTarget Mean: 102.53508758544922\nTarget Standard Deviation: 90.99119020648051\nMatched Mean: 102.81129837036133\nMatched Standard Deviation: 90.91224212665476\nSSIM Score (Source vs Matched): 0.6144812508253021\n\n\nMean and Standard Deviation\nThe original image has a mean of approximately 126.49 and a standard deviation of approximately 69.13. The target image has a mean of approximately 102.54 and a standard deviation of approximately 90.99. After the histogram matching process, the matched image has a mean of approximately 102.81 and a standard deviation of approximately 90.91. Interpretation: Both the means and standard deviations have shifted towards the target image each other after histogram matching.\nSSIM Score\nThe Structural Similarity Index (SSIM) score between the original and matched images is approximately 0.615. Interpretation: The matched image is reasonably similar to the original, but it may not be a perfect match.\n\n\nReferences\n\nHistogram matching Wikipedia\nStructural similarity index measure Wikipedia\nHistogram Matching, Google Earth Engine\nScikit-Image Histogram Matching"
  },
  {
    "objectID": "projects/Non Local Mean/index.html",
    "href": "projects/Non Local Mean/index.html",
    "title": "Non Local Means Denoising from Scratch",
    "section": "",
    "text": "GitHub \n\nImporting the libraries\n\nimport os\nimport numpy as np\nimport cv2 # used only for loading the image\nimport matplotlib.pyplot as plt # used only for displaying the image\nfrom tqdm import tqdm\n\n\n\nSetting up the images and required functions\n\n# Loading the images in grayscale and then storing them in a list for subsequent processing.\nimg_size = 256 # setting the size of the image\nimagefolder_path = os.path.join(os.getcwd(), 'Dataset', 'NLM') # path to the folder containing the images\nimage_set = []\n# create list of only jpg and png images\nimage_list = [f for f in os.listdir(imagefolder_path) if f.endswith('.jpg') or f.endswith('.png')]\nimage_list.sort()\nfor image_name in image_list: # iterate over all the images in the folder\n    img = cv2.imread(os.path.join(imagefolder_path, image_name), cv2.IMREAD_GRAYSCALE) # read the image in grayscale\n    img = cv2.resize(img, (img_size, img_size))\n    image_set.append(img) # append the image to the list\n\n\n# this function is used to add gaussian noise to the image\ndef add_gaussian_noise(image, sigma): \n    noisy_image = image.copy() # copy the image\n    row, col = noisy_image.shape # get the shape of the image\n    gauss = np.random.normal(0, sigma, (row, col)) # generate the gaussian noise\n    noisy_image = noisy_image + gauss # add the noise to the image\n    return noisy_image  # return the noisy image\n\n# this function is used to get the PSNR value of the image compared to the original image\ndef getPSNR(original_image, noisy_image):\n    mse = np.mean((original_image - noisy_image)**2) # calculate the mean square error\n    if mse == 0: # if mse is 0, then return 100\n        return 100\n    return 20 * np.log10(255.0 / np.sqrt(mse)) # return the PSNR value\n\n# this function is used to get the MSE value of the image compared to the original image\ndef getMSE(original_image, noisy_image):\n    mse = np.mean((original_image - noisy_image)**2) # calculate the mean square error\n    return mse\n\n# this function is used to get MSE and PSNR values of the image compared to the original image\ndef getStats(original, noisy, denoised, display=False):\n\n    if display: \n        figure, axes = plt.subplots(1, 4, figsize=(15, 5))\n        axes[0].imshow(original, cmap='gray') # display the original image\n        axes[0].axis('off')\n        axes[0].set_title('Original Image', fontsize=10)\n        axes[1].imshow(noisy, cmap='gray') # display the noisy image\n        axes[1].axis('off')\n        axes[1].set_title('Noisy Image', fontsize=10)\n        axes[2].imshow(denoised, cmap='gray') # display the denoised image\n        axes[2].axis('off')\n        axes[2].set_title('Denoised Image', fontsize=10)\n        axes[3].imshow(np.abs(original-denoised), cmap='gray') # display the difference image\n        axes[3].axis('off')\n        axes[3].set_title('Difference Image', fontsize=10)\n        plt.show()\n\n    mse_noisy = getMSE(original, noisy) # get the MSE value of the noisy image\n    mse_denoised = getMSE(original, denoised) # get the MSE value of the denoised image\n    psnr_noisy = getPSNR(original, noisy) # get the PSNR value of the noisy image\n    psnr_denoised = getPSNR(original, denoised) # get the PSNR value of the denoised image\n\n    if display:\n        print(f'MSE of Noisy Image: {mse_noisy:.2f}')\n        print(f'MSE of Denoised Image: {mse_denoised:.2f}\\n')\n        print(f'PSNR of Noisy Image: {psnr_noisy:.2f}')\n        print(f'PSNR of Denoised Image: {psnr_denoised:.2f}\\n')\n\n    return mse_noisy, mse_denoised, psnr_noisy, psnr_denoised\n \n\n\nSIGMA = [15, 45, 80] # list of sigma values\n\nnoisy15 = [] # list to store the noisy images with sigma = 15\nnoisy45 = [] # list to store the noisy images with sigma = 45\nnoisy80 = [] # list to store the noisy images with sigma = 80\n\n# iterate over all the images in the image set and add the gaussian noise to them\nfor idx in range(len(image_set)): \n    fig, axes = plt.subplots(1, len(SIGMA)+1, figsize=(15, 5))\n    axes[0].imshow(image_set[idx], cmap='gray')\n    axes[0].axis('off')\n    axes[0].set_title('Original Image '+str(idx), fontsize=10)\n\n    for i, s in enumerate(SIGMA):\n        noisy_image = add_gaussian_noise(image_set[idx], s)\n        axes[i+1].imshow(noisy_image, cmap='gray')\n        axes[i+1].set_title(f'Sigma = {s}', fontsize=10)\n        axes[i+1].axis('off')\n        # store the noisy image in the corresponding list\n        if s == 15: \n            noisy15.append(noisy_image)\n        elif s == 45:\n            noisy45.append(noisy_image)\n        else:\n            noisy80.append(noisy_image)\n    plt.show() # display the images\n\n# create a dictionary to store the noisy images\nimage_pool = {\n    0: image_set,\n    15: noisy15,\n    45: noisy45,\n    80: noisy80\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefining the NLM functionalities\n\n# funtion to add padding to the image\ndef getPaddedImage(image, padwidth):\n    paddedImage = np.zeros((image.shape[0] + 2 * padwidth, image.shape[1] + 2 * padwidth), dtype=np.uint8) \n    paddedImage[padwidth:padwidth+image.shape[0], padwidth:padwidth+image.shape[1]] = image\n    return paddedImage\n\n# function to get the comparison value\ndef getComparison(image, X, Y, r, f, B, h, sigma):\n    Iw, w = 0, 0\n    for i in range(X, X + r - f, 1):  # iterate over the window\n        for j in range(Y, Y + r - f, 1):   \n            B0 = image[j:j+f, i:i+f] # get the comparison patch\n            euclideanDistance = np.sum(np.square(B0 - B)) # calculate the euclidean distance\n            numerator = max(euclideanDistance - 2*sigma*sigma, 0) \n            w0 = np.exp(-1*(numerator/(h*h))) # calculate the weight\n            w += w0 # add the weight to the sum\n            f0 = int((f-1)/2)\n            Iw += w0*image[j + f0, i + f0] \n            \n    I_cap = Iw/w # calculate the comparison value\n    return I_cap # return the comparison value\n\n# function to apply the non local means\ndef NLM(image, r, f, h, sigma):\n    padwidth = int((r-1)/2) # calculate the padding width\n    image_padded = getPaddedImage(image, padwidth) # get the padded image\n    \n    num_iters = image.shape[1]*image.shape[0] # calculate the number of iterations\n    result = np.zeros(image_padded.shape, dtype=np.uint8) # create a denoised image\n\n    with tqdm(total=num_iters) as pbar: # create a progress bar\n        # iterate over the image\n        for x in range(padwidth, padwidth + image.shape[1]): \n            for y in range(padwidth, padwidth + image.shape[0]):\n                X = x - padwidth  # get the x coordinate of the comparison window\n                Y = y - padwidth  # get the y coordinate of the comparison window\n                f0 = int((f-1)/2) \n                B = image_padded[y-f0 : y+f0+1, x-f0 : x+f0+1] # get the center patch\n                I_cap = getComparison(image_padded, X, Y, r, f, B, h, sigma) # get the comparison value between the center patch and other patches in the research window\n                result[y, x] = np.floor(I_cap)  # update the denoised image\n                pbar.update(1) # update the progress bar\n\n    return result[padwidth:padwidth+image.shape[0],padwidth:padwidth+image.shape[1]] # return the denoised image\n\n\nf_list = [3, 5, 7, 9] # list of comparison window sizes\nr_list = [21, 35] # list of research window sizes\nk_list = [0.05, 0.2, 0.35, 0.6]  # list of k values\n\n\nwith open('results.csv', 'a') as file:\n    file.write('image,sigma,f,r,k,h,mse_noisy,mse_denoised,psnr_noisy,psnr_denoised\\n')\n\nfor sigma in list(image_pool.keys()):\n    if sigma == 0:\n        continue\n\n    for img_index in range(len(image_pool[sigma])):\n        noisy_image = image_pool[sigma][img_index]\n        original_image = image_pool[0][img_index]\n        \n        if sigma == 80: r = r_list[1]\n        else: r = r_list[0]\n\n        for f in f_list:\n            for k in k_list:\n\n                h = k*sigma\n                print(f'Image: {img_index}, sigma: {sigma}, f: {f}, r: {r}, k: {k}, h: {h}')\n\n                result = NLM(image=np.uint8(noisy_image), r=r, f=f, h=h, sigma=sigma)\n                stats = getStats(original_image, noisy_image, result, display=False)\n\n                # save the results in a csv file\n                with open('results.csv', 'a') as file:\n                    file.write(f'{img_index},{sigma},{f},{r},{k},{h},{stats[0]},{stats[1]},{stats[2]},{stats[3]}\\n')\n\n\n\nAnalysis of the results obtained\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nimport numpy as np\n\n\n# open the csv file\ndf = pd.read_csv('results.csv')\n\n\nrcParams['axes.titlepad'] = 20 # padding for the plot title\n\n# this funtion returns the anysis for the given image and sigma\ndef getAnanysis(image, sigma):\n    df0 = df[df['image'] == image]\n    df0_s = df0[df0['sigma'] == sigma]\n    df_max = df0_s[df0_s['psnr_denoised'] == df0_s['psnr_denoised'].max()]\n    df_max = df_max.rename(columns={'image': 'Image', 'sigma': 'Sigma', 'h': 'Optimum h', 'mse_noisy': 'MSE Noisy', \n                                      'f' : 'F', 'r' : 'R', 'k': 'K',\n                                      'mse_denoised': 'MSE Denoised', 'psnr_noisy': 'PSNR Noisy', 'psnr_denoised': 'PSNR Denoised'})\n    return df_max[0:1]\n\n# this function returns the best analysis for the given image\ndef getBest(image):\n    df_result = pd.DataFrame()\n    df_result = pd.concat([df_result, getAnanysis(image, 15)], ignore_index=True)\n    df_result = pd.concat([df_result, getAnanysis(image, 45)], ignore_index=True)\n    df_result = pd.concat([df_result, getAnanysis(image, 80)], ignore_index=True)\n    return df_result\n\n# this function plots the analysis for the given image, sigma and f values\ndef plotAnalysis(image, sigma, f_list):\n    fig, axes = plt.subplots(1, len(f_list), figsize=(15, 3))\n    for i, f in enumerate(f_list):\n        df0 = df[df['image'] == image]\n        df1 = df0[df0['f'] == f]\n        df2 = df1[df1['sigma'] == sigma]\n        \n        axes[i].plot(df2['h'], df2['psnr_denoised'], label='sigma = ' + str(sigma), alpha=0.7)\n        df_max = df2[df2['psnr_denoised'] == df2['psnr_denoised'].max()]\n        axes[i].scatter(df_max['h'], df_max['psnr_denoised'], marker='o',label='Optimum h', color='black', s=20)\n        \n        axes[i].set_xlabel('h')\n        axes[i].set_ylabel('PSNR')\n        axes[i].set_title('Image = ' + str(image) + ', sigma = ' + str(sigma) + ', f = ' + str(f), fontsize=10)\n        axes[i].legend()\n        axes[i].grid()\n    \n    plt.tight_layout()  \n    plt.show()\n\n# this function plots the compiled analysis for the given image\ndef getAnalysis(image):\n    print('Plotting for Image ' + str(image), '\\n')\n    sigmas = [15, 45, 80]\n    fs = [3, 5, 7, 9]\n    for s in sigmas:\n        plotAnalysis(image, s, fs) \n    plt.show()\n\n\nImage 0\n\ngetAnalysis(0)\nprint('\\nOptimum stats of Image 0 for different Sigma values\\n')\ngetBest(image = 0)\n\nPlotting for Image 0 \n\n\nOptimum stats of Image 0 for different Sigma values\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage\nSigma\nF\nR\nK\nOptimum h\nMSE Noisy\nMSE Denoised\nPSNR Noisy\nPSNR Denoised\n\n\n\n\n0\n0\n15\n5\n21\n0.60\n9.0\n225.750569\n77.435028\n24.594515\n29.241429\n\n\n1\n0\n45\n7\n21\n0.60\n27.0\n2033.781032\n90.043854\n15.047762\n28.586263\n\n\n2\n0\n80\n5\n35\n0.05\n4.0\n6362.757848\n96.997910\n10.094350\n28.263180\n\n\n\n\n\n\n\n\n# Denoising with the optimum values\nimage, sigma, f, r, h = 0, 15, 5, 21, 9 \nresult = NLM(image=np.uint8(image_pool[sigma][image]), r=r, f=f, h=h, sigma=sigma)\nstats = getStats(image_pool[0][image], image_pool[sigma][image], result, display=True)\n\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 65536/65536 [02:55&lt;00:00, 374.15it/s]\n\n\n\n\n\nMSE of Noisy Image: 224.86\nMSE of Denoised Image: 77.76\n\nPSNR of Noisy Image: 24.61\nPSNR of Denoised Image: 29.22\n\n\n\n\n\nImage 1\n\ngetAnalysis(1)\nprint('\\nOptimum stats of Image 1 for different Sigma values\\n')\ngetBest(image = 1)\n\nPlotting for Image 1 \n\n\nOptimum stats of Image 1 for different Sigma values\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage\nSigma\nF\nR\nK\nOptimum h\nMSE Noisy\nMSE Denoised\nPSNR Noisy\nPSNR Denoised\n\n\n\n\n0\n1\n15\n5\n21\n0.60\n9.00\n225.393005\n77.787003\n24.601399\n29.221733\n\n\n1\n1\n45\n7\n21\n0.35\n15.75\n2037.057186\n102.102417\n15.040771\n28.040443\n\n\n2\n1\n80\n9\n35\n0.05\n4.00\n6422.667925\n108.807724\n10.053649\n27.764206\n\n\n\n\n\n\n\n\n# Denoising with the optimum values\nimage, sigma, f, r, h = 1, 15, 5, 21, 9 \nresult = NLM(image=np.uint8(image_pool[sigma][image]), r=r, f=f, h=h, sigma=sigma)\nstats = getStats(image_pool[0][image], image_pool[sigma][image], result, display=True)\n\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 65536/65536 [02:57&lt;00:00, 369.16it/s]\n\n\n\n\n\nMSE of Noisy Image: 223.92\nMSE of Denoised Image: 77.42\n\nPSNR of Noisy Image: 24.63\nPSNR of Denoised Image: 29.24\n\n\n\n\n\nImage 2\n\ngetAnalysis(2)\nprint('\\nOptimum stats of Image 2 for different Sigma values\\n')\ngetBest(image = 2)\n\nPlotting for Image 2 \n\n\nOptimum stats of Image 2 for different Sigma values\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage\nSigma\nF\nR\nK\nOptimum h\nMSE Noisy\nMSE Denoised\nPSNR Noisy\nPSNR Denoised\n\n\n\n\n0\n2\n15\n3\n21\n0.60\n9.0\n224.381217\n30.602051\n24.620939\n33.273298\n\n\n1\n2\n45\n7\n21\n0.60\n27.0\n2026.872861\n82.821640\n15.062539\n28.949365\n\n\n2\n2\n80\n9\n35\n0.05\n4.0\n6411.461156\n102.665375\n10.061233\n28.016564\n\n\n\n\n\n\n\n\n# Denoising with the optimum values\nimage, sigma, f, r, h = 2, 15, 3, 21, 9 \nresult = NLM(image=np.uint8(image_pool[sigma][image]), r=r, f=f, h=h, sigma=sigma)\nstats = getStats(image_pool[0][image], image_pool[sigma][image], result, display=True)\n\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 65536/65536 [04:15&lt;00:00, 256.58it/s]\n\n\n\n\n\nMSE of Noisy Image: 225.78\nMSE of Denoised Image: 30.63\n\nPSNR of Noisy Image: 24.59\nPSNR of Denoised Image: 33.27\n\n\n\n\n\nImage 3\n\ngetAnalysis(3)\nprint('\\nOptimum stats of Image 3 for different Sigma values\\n')\ngetBest(image = 3)\n\nPlotting for Image 3 \n\n\nOptimum stats of Image 3 for different Sigma values\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage\nSigma\nF\nR\nK\nOptimum h\nMSE Noisy\nMSE Denoised\nPSNR Noisy\nPSNR Denoised\n\n\n\n\n0\n3\n15\n5\n21\n0.60\n9.00\n223.490709\n77.480606\n24.638209\n29.238874\n\n\n1\n3\n45\n9\n21\n0.35\n15.75\n2043.726862\n103.326492\n15.026575\n27.988687\n\n\n2\n3\n80\n3\n35\n0.05\n4.00\n6381.607095\n113.652237\n10.081503\n27.575024\n\n\n\n\n\n\n\n\n# Denoising with the optimum values\nimage, sigma, f, r, h = 3, 15, 5, 21, 9 \nresult = NLM(image=np.uint8(image_pool[sigma][image]), r=r, f=f, h=h, sigma=sigma)\nstats = getStats(image_pool[0][image], image_pool[sigma][image], result, display=True)\n\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 65536/65536 [03:07&lt;00:00, 350.23it/s]\n\n\n\n\n\nMSE of Noisy Image: 221.88\nMSE of Denoised Image: 77.22\n\nPSNR of Noisy Image: 24.67\nPSNR of Denoised Image: 29.25\n\n\n\n\n\n\nReferences\n\nSklern Implementation\nNon-Local Means Denoising\nPSNR\nPython Implementation of PSNR\nAn Improved Non-Local Means Algorithm for Image Denoising"
  },
  {
    "objectID": "projects/Wiener Filter/index.html",
    "href": "projects/Wiener Filter/index.html",
    "title": "Wiener Filter from Scratch",
    "section": "",
    "text": "GitHub \n\nPart 1 : Generating the Noisy Images using Inbuilt and Custom Functions\nImporting the Libraries\n\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport os\nfrom scipy.ndimage import convolve1d\nimport pandas as pd\nfrom skimage.metrics import peak_signal_noise_ratio\nfrom skimage.restoration import denoise_tv_chambolle\n\nFunctions to add noise to the image\n\n# add gaussian noise to image using inbuilt functions\ndef gaussian_noise_cv2(image, mean, std_dev):\n    noise = np.random.normal(mean, std_dev, image.shape).astype(np.uint8)\n    noisy_image = cv2.add(image, noise)\n    return noisy_image\n\n# add defocus blur to image using inbuilt functions\ndef defocus_blur_cv2(image, kernel_size):\n    if kernel_size % 2 == 0: kernel_size += 1\n    kernel = np.ones((kernel_size, kernel_size), dtype=np.float32) / (kernel_size ** 2)\n    blurred_image = cv2.filter2D(image, -1, kernel)\n    return blurred_image\n\n# function for performing addition to image in 32 bit and then clipping to 8 bit\ndef mimic_cv2_add(image, value):\n    result = image.astype(np.int32) + value\n    result[result &lt; 0] = 0\n    result[result &gt; 255] = 255\n    return result.astype(np.uint8)\n\n# add gaussian noise to image using custom functions\ndef gaussian_noise_custom(image, mean, std_dev):\n    noise = np.random.normal(mean, std_dev, image.shape).astype(np.uint8)\n    noisy_image = mimic_cv2_add(image, noise)\n    return noisy_image\n\n# add defocus blur to image using custom functions\ndef defocus_blur_custom(image, kernel_size):\n    if kernel_size % 2 == 0: kernel_size += 1\n    kernel = np.ones(kernel_size, dtype=np.float32) / kernel_size\n    blurred_image = np.apply_along_axis(convolve1d, axis=1, arr=image, weights=kernel, mode='constant', cval=0.0)\n    blurred_image = np.apply_along_axis(convolve1d, axis=0, arr=blurred_image, weights=kernel, mode='constant', cval=0.0)\n    return blurred_image\n\n\ndef int2str(num):\n    if num &lt; 10: return '0' + str(num)\n    else: return str(num)\n\n# helper function to corrupt the images\ndef corrupt_images(image_folder, save_folder, noice_func, blur_func):\n    images = os.listdir(image_folder)\n    for _image in images:\n        image = cv2.imread(os.path.join(image_folder, _image))\n        noise_levels = [3, 5, 7, 9, 11] # standard deviation of gaussian noise\n        blur_levels = [5, 11, 17, 23, 29] # kernel size for defocus blur\n\n        for noise_level, blur_level in zip(noise_levels, blur_levels):\n            blurred_image = blur_func(image, kernel_size=blur_level)\n            noisy_image = noice_func(blurred_image, mean=0, std_dev=noise_level)\n            image_name = _image.split('.')[0]\n            image_name = image_name + '_noise_' + int2str(noise_level) + '_blur_' + int2str(blur_level) + '.png'\n            plt.imsave(os.path.join(save_folder, image_name), cv2.cvtColor(noisy_image, cv2.COLOR_BGR2RGB))            \n\n\n# corrupting the images using inbuilt functions\ncorrupt_images('Dataset', 'Corrupted_CV2', gaussian_noise_cv2, defocus_blur_cv2)\n\n\n# corrupting the images using custom functions\ncorrupt_images('Dataset', 'Corrupted_Custom', gaussian_noise_custom, defocus_blur_custom)\n\n\n\nPart 2 : Compairing the Noisy Images obtained from Inbuilt and Custom Functions\n\n# plotting the images\ncorrupted_custom_path = 'Corrupted_Custom'\ncorrupted_cv2_path = 'Corrupted_CV2'\n\ncorrupted_images = os.listdir(corrupted_custom_path)\ncorrupted_images.sort()\n\n# plotiing the images into 4x5 grid\nfig, axes = plt.subplots(4, 5, figsize=(15, 10))\nfor i, ax in enumerate(axes.flat):\n    image = cv2.imread(os.path.join(corrupted_custom_path, corrupted_images[i]))\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    ax.imshow(image)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(corrupted_images[i].split('.')[0])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\ncorrupted_custom_path = 'Corrupted_Custom'\ncorrupted_cv2_path = 'Corrupted_CV2'\n\ncorrupted_images = os.listdir(corrupted_custom_path)\ncorrupted_images.sort()\n\n# get metrics\ndef get_metrics(image1, image2):\n    mse = np.mean((image1 - image2) ** 2)\n    psnr = peak_signal_noise_ratio(image1, image2)\n    return mse, psnr\n\n# get MSE, MAE, PSNR, SSIM between corrupted_custom_images and corrupted_cv2_images and print them\ncomparison_results = []\nfor image in corrupted_images:\n    image1 = cv2.imread(os.path.join(corrupted_custom_path, image))\n    image2 = cv2.imread(os.path.join(corrupted_cv2_path, image))\n    mse, psnr = get_metrics(image1, image2)\n    comparison_results.append([image.split('.')[0], mse, psnr])\ncomparison_results = pd.DataFrame(comparison_results, columns=['Image', 'MSE', 'PSNR'])\ncomparison_results.to_csv('comparison_results.csv', index=False) \n\n\nprint('Comparison Result')\ncomparison_results\n\nComparison Result\n\n\n\n\n\n\n\n\n\nImage\nMSE\nPSNR\n\n\n\n\n0\nimage1_noise_03_blur_05\n49.035501\n8.206387\n\n\n1\nimage1_noise_05_blur_11\n54.752382\n8.124839\n\n\n2\nimage1_noise_07_blur_17\n59.095725\n8.170110\n\n\n3\nimage1_noise_09_blur_23\n62.706012\n8.230098\n\n\n4\nimage1_noise_11_blur_29\n65.221655\n8.310742\n\n\n5\nimage2_noise_03_blur_05\n51.981909\n6.080089\n\n\n6\nimage2_noise_05_blur_11\n60.035568\n5.974780\n\n\n7\nimage2_noise_07_blur_17\n65.703902\n6.014880\n\n\n8\nimage2_noise_09_blur_23\n69.602320\n6.084250\n\n\n9\nimage2_noise_11_blur_29\n72.953488\n6.135030\n\n\n10\nimage3_noise_03_blur_05\n50.761306\n6.445932\n\n\n11\nimage3_noise_05_blur_11\n58.973741\n6.314150\n\n\n12\nimage3_noise_07_blur_17\n64.760429\n6.326103\n\n\n13\nimage3_noise_09_blur_23\n68.488619\n6.383005\n\n\n14\nimage3_noise_11_blur_29\n71.353838\n6.443379\n\n\n15\nimage4_noise_03_blur_05\n51.364747\n4.636014\n\n\n16\nimage4_noise_05_blur_11\n60.717343\n4.510094\n\n\n17\nimage4_noise_07_blur_17\n67.272995\n4.533867\n\n\n18\nimage4_noise_09_blur_23\n72.099014\n4.591977\n\n\n19\nimage4_noise_11_blur_29\n76.031730\n4.660811\n\n\n\n\n\n\n\n\n\nPart 3 : Implementing Custom Weiner Filter\n\ndef custom_wiener_filter(image, kernel, K, show_intermediate=False, show_result=False):\n    if show_result:\n        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n        plt.title(\"Original Image\")\n        plt.show()\n        \n    kernel /= np.sum(kernel) # normalize the kernel \n    if kernel.shape[2] != image.shape[2]: # check if the number of channels in the kernel matches the image\n        kernel = np.stack([kernel] * image.shape[2], axis=-1)\n    kernel_fft = np.fft.fft2(kernel, s=image.shape[:2], axes=(0, 1)) # fourier transform of the kernel\n\n    kernel_fft = np.conj(kernel_fft) / (np.abs(kernel_fft) ** 2 + K) # wiener filter in frequency domain\n    if show_intermediate:\n        plt.imshow(np.abs(kernel_fft))\n        plt.title(\"Wiener Filter in Frequency Domain\")\n        plt.show()\n\n    image_fft = np.fft.fft2(image, axes=(0, 1)) # fourier transform of the image\n    image_fft = np.multiply(image_fft, kernel_fft) # filtering in frequency domain\n    result = np.fft.ifft2(image_fft, axes=(0, 1)) # inverse fourier transform\n    result = np.abs(result).astype(np.uint8) # get the magnitude of the complex number\n\n    if show_result:\n        plt.imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))\n        plt.title(\"Filtered Image\")\n        plt.show()\n    return result\n\nExample Usage\n\ntest_img = cv2.imread('Corrupted_CV2/image3_noise_07_blur_17.png')\nkernel = np.ones((3, 3, test_img.shape[2]), dtype=np.float32)/9\nK = 0.3\n\nresult_img = custom_wiener_filter(image=test_img, kernel=kernel, K=K, show_intermediate=True, show_result=True)\n\ngt_image = cv2.imread('Dataset/image3.png')\nprint('PSNR of the noisy image: ', peak_signal_noise_ratio(test_img, gt_image))\nprint('PSNR of the filtered image: ', peak_signal_noise_ratio(gt_image, result_img))\n\n\n\n\n\n\n\n\n\n\nPSNR of the noisy image:  6.608012655397994\nPSNR of the filtered image:  12.224561559377236\n\n\n\ndenoised_folder_path = 'Denoised_Results'\ncorrupted_folder_path = 'Corrupted_CV2'\n\ncorrupted_images = os.listdir(corrupted_folder_path)\ncorrupted_images.sort()\n\nfor image in corrupted_images:\n    _image_ = cv2.imread(os.path.join(corrupted_folder_path, image))\n    _kernel_ = np.ones((3, 3, _image_.shape[2]), dtype=np.float32)/9\n    _K_ = 0.3\n    _result_ = custom_wiener_filter(image=_image_, kernel=_kernel_, K=_K_)\n    cv2.imwrite(os.path.join(denoised_folder_path, image), _result_)\n    \n\n\ndenoised_folder_path = 'Denoised_Results'\ncorrupted_folder_path = 'Corrupted_CV2'\ngt_folder_path = 'Dataset'\n\ncorrupted_images = os.listdir(corrupted_folder_path)\ncorrupted_images.sort()\n\n# create a grid of size 20 x 2 and show the images side by side\nfig, axes = plt.subplots(20, 2, figsize=(6, 50))\n\nfor i, ax in enumerate(axes.flat):\n    if i % 2 == 0:\n        image = cv2.imread(os.path.join(corrupted_folder_path, corrupted_images[i//2]))\n        ax.set_title('Corrupted Image')\n    else:\n        image = cv2.imread(os.path.join(denoised_folder_path, corrupted_images[i//2]))\n        ax.set_title('Denoised Image')\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    ax.imshow(image)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nMake a comparison report for the denoised images\n\ndenoised_folder_path = 'Denoised_Results'\ncorrupted_folder_path = 'Corrupted_CV2'\ngt_folder_path = 'Dataset'\n\ncorrupted_images = os.listdir(corrupted_folder_path)\ncorrupted_images.sort()\n\ndenoised_report = []\n\nfor image in corrupted_images:\n    _gt_ = cv2.imread(os.path.join(gt_folder_path, image.split('_')[0] + '.png'))\n    _image_ = cv2.imread(os.path.join(corrupted_folder_path, image))\n    _result_ = cv2.imread(os.path.join(denoised_folder_path, image))\n    psnr1 = peak_signal_noise_ratio(_gt_, _image_)\n    psnr2 = peak_signal_noise_ratio(_gt_, _result_)\n    denoised_report.append([image.split('.')[0], psnr1, psnr2])\n\ndenoised_report = pd.DataFrame(denoised_report, columns=['Image', 'PSNR of Corrupted Image', 'PSNR of Denoised Image'])\ndenoised_report.to_csv('denoised_report.csv', index=False)\n\n\ndenoised_report\n\n\n\n\n\n\n\n\nImage\nPSNR of Corrupted Image\nPSNR of Denoised Image\n\n\n\n\n0\nimage1_noise_03_blur_05\n9.040603\n15.162193\n\n\n1\nimage1_noise_05_blur_11\n8.433515\n14.113834\n\n\n2\nimage1_noise_07_blur_17\n8.185878\n13.622357\n\n\n3\nimage1_noise_09_blur_23\n8.032233\n13.287484\n\n\n4\nimage1_noise_11_blur_29\n7.935132\n13.053590\n\n\n5\nimage2_noise_03_blur_05\n6.956838\n12.656215\n\n\n6\nimage2_noise_05_blur_11\n6.361935\n11.571034\n\n\n7\nimage2_noise_07_blur_17\n6.121967\n11.077882\n\n\n8\nimage2_noise_09_blur_23\n5.987457\n10.777567\n\n\n9\nimage2_noise_11_blur_29\n5.892901\n10.545413\n\n\n10\nimage3_noise_03_blur_05\n7.400513\n13.778849\n\n\n11\nimage3_noise_05_blur_11\n6.834121\n12.699509\n\n\n12\nimage3_noise_07_blur_17\n6.608013\n12.224562\n\n\n13\nimage3_noise_09_blur_23\n6.497504\n11.959846\n\n\n14\nimage3_noise_11_blur_29\n6.410956\n11.738220\n\n\n15\nimage4_noise_03_blur_05\n5.536973\n10.995107\n\n\n16\nimage4_noise_05_blur_11\n4.948614\n9.929457\n\n\n17\nimage4_noise_07_blur_17\n4.731410\n9.492214\n\n\n18\nimage4_noise_09_blur_23\n4.617748\n9.242064\n\n\n19\nimage4_noise_11_blur_29\n4.558332\n9.078822\n\n\n\n\n\n\n\n\n\nPart 4 : Comparison with other image restoration algorithms\nImplementing the following algorithms for comparison\n\nNon Local Means Denoising : Unlike “local mean” filters, which take the mean value of a group of pixels surrounding a target pixel to smooth the image, non-local means filtering takes a mean of all pixels in the image, weighted by how similar these pixels are to the target pixel. This results in much greater post-filtering clarity, and less loss of detail in the image compared with local mean algorithms.\nMedian Filter : The main idea of the median filter is to run through the signal entry by entry, replacing each entry with the median of neighboring entries. The pattern of neighbors is called the “window”, which slides, entry by entry, over the entire signal.\nBilateral Filter : Bilateral Filter replaces the intensity of each pixel with a weighted average of intensity values from nearby pixels. This weight can be based on a Gaussian distribution. Crucially, the weights depend not only on Euclidean distance of pixels, but also on the radiometric differences (e.g., range differences, such as color intensity, depth distance, etc.). This preserves sharp edges.\nGaussian Filter : A Gaussian filter is a linear filter. It’s usually used to blur the image or to reduce noise. Gaussian filter is a low-pass filter, because it attenuates the high-frequency components and passes only the low-frequency components.\nTotal Variation Filter : Total variation denoising (TVD) is an image denoising algorithm which is based on the principle that signals with excessive and possibly spurious detail have high total variation, that is, the integral of the absolute gradient of the signal is high. According to this principle, reducing the total variation of the signal subject to it being a close match to the original signal, removes unwanted detail whilst preserving important details such as edges.\n\n\n# non local mean filter\ndef non_local_mean_rgb(image, sigma=0.08, patch_size=7, patch_distance=11):\n    denoised_image = cv2.fastNlMeansDenoisingColored(image, None, sigma, sigma, patch_size, patch_distance)\n    return denoised_image\n\n# median filter\ndef median_filter_rgb(image, kernel_size=3):\n    denoised_image = np.zeros_like(image)\n    for i in range(image.shape[2]):\n        denoised_image[:, :, i] = cv2.medianBlur(image[:, :, i], kernel_size)\n    return denoised_image\n\n# gaussian smoothing filter\ndef gaussian_smooting_filter_rgb(image, kernel_size=3):\n    denoised_image = np.zeros_like(image)\n    for i in range(image.shape[2]):\n        denoised_image[:, :, i] = cv2.GaussianBlur(image[:, :, i], (kernel_size, kernel_size), 0)\n    return denoised_image\n\n# bilateral filter\ndef bilateral_filter_rgb(image, kernel_size=3):\n    denoised_image = np.zeros_like(image)\n    for i in range(image.shape[2]):\n        denoised_image[:, :, i] = cv2.bilateralFilter(image[:, :, i], kernel_size, 75, 75)\n    return denoised_image\n\n# total variation regularization\ndef total_variation_regularization_rgb(image, weight=0.1, eps=0.0002):\n    image = image.astype(np.float32) / 255.0\n    denoised_image = np.zeros_like(image)\n    for i in range(image.shape[2]):\n        denoised_image[:, :, i] = denoise_tv_chambolle(image[:, :, i], weight=weight, eps=eps)\n    denoised_image = (denoised_image * 255).astype(np.uint8)\n    return denoised_image\n\n\n# function to compare the filters and plot the results\ndef compare_filters(original, noisy, denoised):\n    \n    original_ = cv2.imread(original)\n    noisy_ = cv2.imread(noisy)\n    denoised_0 = cv2.imread(denoised)\n    denoised_1 = non_local_mean_rgb(noisy_)\n    denoised_2 = median_filter_rgb(noisy_)\n    denoised_3 = gaussian_smooting_filter_rgb(noisy_)\n    denoised_4 = bilateral_filter_rgb(noisy_)\n    denoised_5 = total_variation_regularization_rgb(noisy_)\n\n    result = {}\n    result['Noisy'] = peak_signal_noise_ratio(original_, noisy_)\n    result['Wiener Filter'] = peak_signal_noise_ratio(original_, denoised_0)\n    result['Non Local Means'] = peak_signal_noise_ratio(original_, denoised_1)\n    result['Median Filter'] = peak_signal_noise_ratio(original_, denoised_2)\n    result['Gaussian Smoothing'] = peak_signal_noise_ratio(original_, denoised_3)\n    result['Bilateral Filter'] = peak_signal_noise_ratio(original_, denoised_4)\n    result['Total Variation Regularization'] = peak_signal_noise_ratio(original_, denoised_5)\n\n    fig, axes = plt.subplots(2, 4, figsize=(15, 6))\n\n    axes[0, 0].imshow(cv2.cvtColor(original_, cv2.COLOR_BGR2RGB))\n    axes[0, 0].set_title('Original Image')\n    axes[0, 0].axis('off')\n\n    axes[0, 1].imshow(cv2.cvtColor(noisy_, cv2.COLOR_BGR2RGB))\n    axes[0, 1].set_title('Noisy Image')\n    axes[0, 1].axis('off')\n\n    axes[0, 2].imshow(cv2.cvtColor(denoised_0, cv2.COLOR_BGR2RGB))\n    axes[0, 2].set_title('Wiener Filter')\n    axes[0, 2].axis('off')\n\n    axes[0, 3].imshow(cv2.cvtColor(denoised_1, cv2.COLOR_BGR2RGB))\n    axes[0, 3].set_title('Non Local Means')\n    axes[0, 3].axis('off')\n\n    axes[1, 0].imshow(cv2.cvtColor(denoised_2, cv2.COLOR_BGR2RGB))\n    axes[1, 0].set_title('Median Filter')\n    axes[1, 0].axis('off')\n\n    axes[1, 1].imshow(cv2.cvtColor(denoised_3, cv2.COLOR_BGR2RGB))\n    axes[1, 1].set_title('Gaussian Smoothing')\n    axes[1, 1].axis('off')\n\n    axes[1, 2].imshow(cv2.cvtColor(denoised_4, cv2.COLOR_BGR2RGB))\n    axes[1, 2].set_title('Bilateral Filter')\n    axes[1, 2].axis('off')\n\n    axes[1, 3].imshow(cv2.cvtColor(denoised_5, cv2.COLOR_BGR2RGB))\n    axes[1, 3].set_title('Total Variation Regularization')\n    axes[1, 3].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n    return result\n\nImage 1\n\nresults_1 = compare_filters('Dataset/image1.png', \n                            'Corrupted_CV2/image1_noise_11_blur_29.png', \n                            'Denoised_Results/image1_noise_11_blur_29.png')\nprint('Comparison Results')\ndf_1 = pd.DataFrame(results_1, index=['PSNR']).T\ndf_1\n\n\n\n\nComparison Results\n\n\n\n\n\n\n\n\n\nPSNR\n\n\n\n\nNoisy\n7.935132\n\n\nWiener Filter\n13.053590\n\n\nNon Local Means\n7.993497\n\n\nMedian Filter\n8.381787\n\n\nGaussian Smoothing\n10.024630\n\n\nBilateral Filter\n8.623257\n\n\nTotal Variation Regularization\n9.758347\n\n\n\n\n\n\n\nImage 2\n\nresults_2 = compare_filters('Dataset/image2.png', \n                            'Corrupted_CV2/image2_noise_11_blur_29.png', \n                            'Denoised_Results/image2_noise_11_blur_29.png')\nprint('Comparison Results')\ndf_2 = pd.DataFrame(results_2, index=['PSNR']).T\ndf_2\n\n\n\n\nComparison Results\n\n\n\n\n\n\n\n\n\nPSNR\n\n\n\n\nNoisy\n5.892901\n\n\nWiener Filter\n10.545413\n\n\nNon Local Means\n5.938721\n\n\nMedian Filter\n6.378402\n\n\nGaussian Smoothing\n8.095650\n\n\nBilateral Filter\n6.234894\n\n\nTotal Variation Regularization\n7.526716\n\n\n\n\n\n\n\nImage 3\n\nresults_3 = compare_filters('Dataset/image3.png',\n                            'Corrupted_CV2/image3_noise_11_blur_29.png',\n                            'Denoised_Results/image3_noise_11_blur_29.png')\nprint('Comparison Results')\ndf_3 = pd.DataFrame(results_3, index=['PSNR']).T\ndf_3\n\n\n\n\nComparison Results\n\n\n\n\n\n\n\n\n\nPSNR\n\n\n\n\nNoisy\n6.410956\n\n\nWiener Filter\n11.738220\n\n\nNon Local Means\n6.458596\n\n\nMedian Filter\n6.937527\n\n\nGaussian Smoothing\n8.760057\n\n\nBilateral Filter\n6.857594\n\n\nTotal Variation Regularization\n8.199067\n\n\n\n\n\n\n\nImage 4\n\nresults_4 = compare_filters('Dataset/image4.png',\n                            'Corrupted_CV2/image4_noise_11_blur_29.png',\n                            'Denoised_Results/image4_noise_11_blur_29.png')\nprint('Comparison Results')\ndf_4 = pd.DataFrame(results_4, index=['PSNR']).T\ndf_4\n\n\n\n\nComparison Results\n\n\n\n\n\n\n\n\n\nPSNR\n\n\n\n\nNoisy\n4.558332\n\n\nWiener Filter\n9.078822\n\n\nNon Local Means\n4.594948\n\n\nMedian Filter\n5.114527\n\n\nGaussian Smoothing\n6.892846\n\n\nBilateral Filter\n4.756240\n\n\nTotal Variation Regularization\n6.139856\n\n\n\n\n\n\n\nVisualizing the results\n\ncombined_df = pd.concat([df_1, df_2, df_3, df_4], axis=1)\nfig, ax = plt.subplots(figsize=(15, 7))\ncombined_df.plot.bar(ax=ax, rot=0, color=['magenta', 'lightgreen', 'coral', 'brown'])\nax.set_title('Comparison of Filters')\nax.legend(['Image 1', 'Image 2', 'Image 3', 'Image 4'])\nplt.tight_layout()\nplt.grid(axis='y', linestyle='dotted')\nplt.show()\n\n\n\n\nInference : Wiener Filter outperforms all the other filters in terms of PSNR.\n\n\nReferences\n\nWeiner Filter (Wikipedia) : https://en.wikipedia.org/wiki/Wiener_filter\nWeiner Filter (Mathworks) : https://www.mathworks.com/help/images/ref/wiener2.html\nWeiner Filter (Scipy) : https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.wiener.html\nNon Local Means Denoising (Wikipedia) : https://en.wikipedia.org/wiki/Non-local_means\nBilateral Filter (Wikipedia) : https://en.wikipedia.org/wiki/Bilateral_filter\nGaussian Filter (Wikipedia) : https://en.wikipedia.org/wiki/Gaussian_filter\nTotal Variation Filter (Wikipedia) : https://en.wikipedia.org/wiki/Total_variation_denoising\nMedian Filter (Wikipedia) : https://en.wikipedia.org/wiki/Median_filter\nImage Filtering with Wiener Filter and Median Filter, DOI-10.13140/RG.2.2.15700.65921"
  },
  {
    "objectID": "publications/cameraSHOT/index.html",
    "href": "publications/cameraSHOT/index.html",
    "title": "CameraSHOT",
    "section": "",
    "text": "Draft : Copyright Draft\n\n\n\n\n\n\nCameraSHOT is simple and effective application software that aims to work upon or alter the theme/color-scheme of an image. The application is inspired by Window’s adaptive theme feature that automatically picks an accent color from the desktop background for Start, taskbar, action center, and title bar. CameraSHOT, however, analyses the image’s pre-existing theme/color-scheme and alters it with almost zero assistance from the user. A detailed description of the software CameraSHOT has been presented in this work. The basic principle behind the software rests in analysing an image and altering its theme or clor scheme."
  },
  {
    "objectID": "publications/modifiedRDA-1/index.html",
    "href": "publications/modifiedRDA-1/index.html",
    "title": "A New Modified Red Deer Algorithm for Multi-level Image Thresholding",
    "section": "",
    "text": "DOI : 10.1109/ICRCICN50933.2020.9296166\n\n\n\n\nPublished in - 2020 Fifth International Conference on Research in Computational Intelligence and Communication Networks (ICRCICN)\n\nThis paper presents a modified evolution strategy based meta-heuristic, named Modified Red Deer Algorithm (MRDA), which can be effectively and methodically applied to solve single-objective optimization problems. Recently, the actions of red deers have been analysed during their breading time, that in turn inspired the researchers to develop a popular meta-heuristic, called Red Deer Algorithm (RDA). The RDA has been designed to deal with different combinatorial optimization problems in a variety of real-life applications. This paper introduces few adaptive approaches to modify the inherent operators and parameters of RDA to enhance its efficacy. As a comparative study, the performance of MRDA has been evaluated with RDA and Classical Genetic Algorithm (CGA) by utilizing some real-life gray-scale images. At the outset, the results of these competitive algorithms have been assessed with respect to optimum fitness, worst fitness, average fitness, standard deviation, convergence time at best case and average convergence time at three distinct level of thresholding for each test image. Finally, t-test and Friedman Test have been conducted among themselves to check out the superiority. This comparative analysis establishes that MRDA outperforms others in all facets and furnish exceedingly competitive results."
  },
  {
    "objectID": "publications/picrypt/index.html",
    "href": "publications/picrypt/index.html",
    "title": "Picrypt - Inscribe Images with Encrypted Texts",
    "section": "",
    "text": "Draft : Copyright Draft\n\n\n\n\n\n\nInspired by the idea “why to send less, when you can send more”, Picrypt is user-friendly, click-to-go-like software with a clean and simplistic user interface, which allows users to inscribe digital colour images with encrypted messages while preserving the original properties and qualities of the image. This image containing the encrypted message then can be downloaded and saved locally, and can even be shared to anyone across the globe digitally, while the data/message remains protected and preserved throughout. The passcode with which the messages are inscribed acts as the key for the successful decryption of the messages inscribed within. A detailed description of the software including the enciphering and deciphering of the messages has been presented in this work. The basic principle behind the software rests on using the state of pixel intensity values of a color image for inscribing textual data, encrypted with popular encryption methods like Vernam Cipher and Playfair Cipher, into the image pixel-by-pixel."
  }
]